#!/usr/bin/env python3
"""
Three-Layer Architecture Ablation Study
ä¸‰å±‚æ¶æ„æ¶ˆèå®éªŒ - éªŒè¯æ¯å±‚çš„è´¡çŒ®å’Œä¼˜åŒ–å‚æ•°
"""

import json
import time
import asyncio
import argparse
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from tabulate import tabulate
import os

# Set environment for consistent testing
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
# ç¡®ä¿LLMåŠŸèƒ½å¯ç”¨
os.environ['SKIP_LLM'] = 'false'


class AblationStudy:
    """ä¸‰å±‚æ¶æ„æ¶ˆèå®éªŒä¸»ç±»"""
    
    def __init__(self, debug_mode=False, dataset_size="subset"):
        """åˆå§‹åŒ–å®éªŒé…ç½®
        
        Args:
            debug_mode: æ˜¯å¦å¯ç”¨è°ƒè¯•è¾“å‡º
            dataset_size: æ•°æ®é›†å¤§å° ('subset' æˆ– 'complete')
        """
        self.results_dir = Path("experiment_results/ablation")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.debug_mode = debug_mode
        self.dataset_size = dataset_size
        
        # å¯¼å…¥æœ€ä½³å‚æ•°é…ç½®
        from optimal_parameters import JOIN_PARAMS, UNION_PARAMS
        
        # æ ¹æ®æ•°æ®é›†é€‰æ‹©å‚æ•°
        if dataset_size == "complete":
            # ä½¿ç”¨ä¼˜åŒ–åçš„å‚æ•°
            self.configs = {
                "L1": {
                    "name": "L1_only",
                    "metadata_filter": True,
                    "vector_search": False,
                    "llm_matching": False,
                    "max_metadata_candidates": JOIN_PARAMS["L1"]["max_metadata_candidates"],
                },
                "L2": {
                    "name": "L2_only", 
                    "metadata_filter": False,
                    "vector_search": True,
                    "llm_matching": False,
                    "max_vector_candidates": 200,  # å•ç‹¬L2éœ€è¦æ›´å¤šå€™é€‰
                    "vector_threshold": 0.4,
                },
                "L3": {
                    "name": "L3_only",
                    "metadata_filter": False,
                    "vector_search": False,
                    "llm_matching": True,
                    "max_llm_candidates": 30,  # ç›´æ¥LLM
                },
                "L1+L2": {
                    "name": "L1_L2",
                    "metadata_filter": True,
                    "vector_search": True,
                    "llm_matching": False,
                    **JOIN_PARAMS["L1+L2"],  # ä½¿ç”¨æœ€ä½³å‚æ•°
                },
                "L1+L3": {
                    "name": "L1_L3",
                    "metadata_filter": True,
                    "vector_search": False,
                    "llm_matching": True,
                    "max_metadata_candidates": 1000,
                    "max_llm_candidates": 30,
                },
                "L2+L3": {
                    "name": "L2_L3",
                    "metadata_filter": False,
                    "vector_search": True,
                    "llm_matching": True,
                    "max_vector_candidates": 150,
                    "max_llm_candidates": 20,
                    "vector_threshold": 0.5,
                },
                "L1+L2+L3": {
                    "name": "full_pipeline",
                    "metadata_filter": True,
                    "vector_search": True,
                    "llm_matching": True,
                    **JOIN_PARAMS["L1+L2+L3"],  # ä½¿ç”¨æœ€ä½³å‚æ•°
                }
            }
        else:
            # Subsetæ•°æ®é›†ä½¿ç”¨ç®€å•å‚æ•°
            self.configs = {
                "L1": {
                    "name": "L1_only",
                    "metadata_filter": True,
                    "vector_search": False,
                    "llm_matching": False,
                    "max_metadata_candidates": 200,
                },
                "L2": {
                    "name": "L2_only", 
                    "metadata_filter": False,
                    "vector_search": True,
                    "llm_matching": False,
                    "max_vector_candidates": 50,
                    "vector_threshold": 0.5,
                },
                "L3": {
                    "name": "L3_only",
                    "metadata_filter": False,
                    "vector_search": False,
                    "llm_matching": True,
                    "max_llm_candidates": 10,
                },
                "L1+L2": {
                    "name": "L1_L2",
                    "metadata_filter": True,
                    "vector_search": True,
                    "llm_matching": False,
                    "max_metadata_candidates": 100,
                    "max_vector_candidates": 30,
                    "vector_threshold": 0.5,
                },
                "L1+L3": {
                    "name": "L1_L3",
                    "metadata_filter": True,
                    "vector_search": False,
                    "llm_matching": True,
                    "max_metadata_candidates": 50,
                    "max_llm_candidates": 10,
                },
                "L2+L3": {
                    "name": "L2_L3",
                    "metadata_filter": False,
                    "vector_search": True,
                    "llm_matching": True,
                    "max_vector_candidates": 30,
                    "max_llm_candidates": 5,
                    "vector_threshold": 0.6,
                },
                "L1+L2+L3": {
                    "name": "full_pipeline",
                    "metadata_filter": True,
                    "vector_search": True,
                    "llm_matching": True,
                    "max_metadata_candidates": 100,
                    "max_vector_candidates": 30,
                    "max_llm_candidates": 5,
                    "early_stop_threshold": 0.90,
                }
            }
        
        # æ€§èƒ½ç»Ÿè®¡
        self.perf_stats = {}
        
    async def run_single_configuration(
        self,
        config_name: str,
        task_type: str,
        dataset_size: str,
        max_queries: int = 10
    ) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªé…ç½®çš„å®éªŒ"""
        
        print(f"\n{'='*80}")
        print(f"ğŸ§ª Running Ablation: {config_name}")
        print(f"ğŸ“Š Task: {task_type.upper()} | Dataset: {dataset_size}")
        print(f"ğŸ• Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}\n")
        
        config = self.configs[config_name]
        
        # åŠ è½½æ•°æ®
        tables, queries, ground_truth = await self.load_data(task_type, dataset_size)
        print(f"âœ… Loaded {len(tables)} tables, {len(queries)} queries")
        
        # é™åˆ¶æŸ¥è¯¢æ•°é‡
        if max_queries:
            queries = queries[:max_queries]
            print(f"ğŸ¯ Limited to {max_queries} queries")
        
        # åˆ›å»ºworkflow
        workflow = await self.create_ablation_workflow(config)
        
        # åˆå§‹åŒ–ï¼ˆæ„å»ºç´¢å¼•ï¼‰
        init_start = time.time()
        await workflow.initialize(tables)
        init_time = time.time() - init_start
        print(f"âœ… Workflow initialized in {init_time:.2f}s")
        
        # è¿è¡ŒæŸ¥è¯¢å¹¶æ”¶é›†æŒ‡æ ‡
        results = []
        layer_times = {"L1": [], "L2": [], "L3": [], "total": []}
        successful_queries = 0
        empty_predictions = 0  # ç»Ÿè®¡ç©ºé¢„æµ‹
        
        print(f"\nğŸ” Running {len(queries)} queries...")
        print("-" * 80)
        
        if self.debug_mode:
            print("\nğŸ› è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ - æ˜¾ç¤ºè¯¦ç»†åŒ¹é…ä¿¡æ¯")
        
        for i, query in enumerate(queries, 1):
            try:
                # è¿è¡Œå•ä¸ªæŸ¥è¯¢
                result, timing = await self.run_single_query(
                    workflow, query, config, ground_truth
                )
                
                if result:
                    results.append(result)
                    successful_queries += 1
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºé¢„æµ‹
                    if not result.get('predictions'):
                        empty_predictions += 1
                    
                    # æ”¶é›†æ—¶é—´ç»Ÿè®¡
                    for layer in ["L1", "L2", "L3", "total"]:
                        if layer in timing:
                            layer_times[layer].append(timing[layer])
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % max(1, len(queries) // 10) == 0 or i == len(queries):
                    success_rate = (successful_queries / i * 100) if i > 0 else 0
                    print(f"Progress: {i}/{len(queries)} | Success: {successful_queries}/{i} ({success_rate:.1f}%)")
                    if empty_predictions > 0:
                        print(f"         âš ï¸  ç©ºé¢„æµ‹: {empty_predictions}/{successful_queries}")
                    
            except Exception as e:
                print(f"âŒ Query {i} failed: {str(e)[:50]}")
        
        # è®¡ç®—èšåˆæŒ‡æ ‡
        metrics = self.calculate_metrics(results, ground_truth, config_name)
        
        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
        metrics["performance"] = {
            "init_time": init_time,
            "avg_times": {
                layer: np.mean(times) if times else 0
                for layer, times in layer_times.items()
            },
            "total_queries": len(queries),
            "successful_queries": successful_queries,
            "success_rate": successful_queries / len(queries) if queries else 0,
        }
        
        metrics["config"] = config
        metrics["task_type"] = task_type
        metrics["dataset_size"] = dataset_size
        
        # æ‰“å°ç»“æœæ‘˜è¦
        self.print_results_summary(metrics, config_name)
        
        # ä¿å­˜ç»“æœ
        self.save_results(metrics, config_name, task_type, dataset_size)
        
        return metrics
    
    async def load_data(
        self,
        task_type: str,
        dataset_size: str
    ) -> Tuple[List, List, Dict]:
        """åŠ è½½å®éªŒæ•°æ®"""
        from src.core.models import TableInfo, ColumnInfo
        
        dataset_dir = Path(f"examples/separated_datasets/{task_type}_{dataset_size}")
        
        # åŠ è½½è¡¨æ•°æ®
        with open(dataset_dir / "tables.json") as f:
            tables_data = json.load(f)
        
        # ä¼˜å…ˆä½¿ç”¨è¿‡æ»¤åçš„æŸ¥è¯¢ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        queries_filtered_path = dataset_dir / "queries_filtered.json"
        if queries_filtered_path.exists():
            print(f"ğŸ“Œ ä½¿ç”¨è¿‡æ»¤åçš„æŸ¥è¯¢: {queries_filtered_path}")
            with open(queries_filtered_path) as f:
                queries_data = json.load(f)
        else:
            with open(dataset_dir / "queries.json") as f:
                queries_data = json.load(f)
        
        # ä¼˜å…ˆä½¿ç”¨è½¬æ¢åçš„ground truthï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        gt_transformed_path = dataset_dir / "ground_truth_transformed.json"
        if gt_transformed_path.exists():
            print(f"ğŸ“Œ ä½¿ç”¨è½¬æ¢åçš„ground truth: {gt_transformed_path}")
            with open(gt_transformed_path) as f:
                ground_truth_data = json.load(f)
            # è½¬æ¢åçš„æ ¼å¼å·²ç»æ˜¯å­—å…¸å½¢å¼
            ground_truth = ground_truth_data
        else:
            with open(dataset_dir / "ground_truth.json") as f:
                ground_truth_data = json.load(f)
        
        # è½¬æ¢ä¸ºTableInfoå¯¹è±¡
        tables = []
        for table_data in tables_data:
            table = TableInfo(
                table_name=table_data['table_name'],
                columns=[
                    ColumnInfo(
                        table_name=table_data['table_name'],
                        column_name=col.get('column_name', col.get('name', '')),
                        data_type=col.get('data_type', col.get('type', 'unknown')),
                        sample_values=col.get('sample_values', [])[:5]
                    )
                    for col in table_data.get('columns', [])[:20]
                ]
            )
            tables.append(table)
        
        # å¦‚æœground truthè¿˜ä¸æ˜¯å­—å…¸æ ¼å¼ï¼Œè¿›è¡Œè½¬æ¢
        if isinstance(ground_truth_data, list):
            ground_truth = {}
            for gt in ground_truth_data:
                if task_type == "join":
                    key = f"{gt['query_table']}:{gt.get('query_column', '')}"
                else:
                    key = gt['query_table']
                
                if key not in ground_truth:
                    ground_truth[key] = []
                ground_truth[key].append(gt['candidate_table'])
        
        return tables, queries_data, ground_truth
    
    async def create_ablation_workflow(self, config: Dict):
        """åˆ›å»ºæ¶ˆèå®éªŒçš„workflow"""
        from src.core.ultra_optimized_workflow import UltraOptimizedWorkflow
        
        # ä½¿ç”¨UltraOptimizedWorkflowï¼Œå®ƒæœ‰æ›´å¥½çš„å±‚æ§åˆ¶
        workflow = UltraOptimizedWorkflow()
        
        # æ ¹æ®é…ç½®è°ƒæ•´å‚æ•°
        # L1: å…ƒæ•°æ®ç­›é€‰
        if config.get("metadata_filter"):
            workflow.enable_metadata_filter = True
            workflow.max_metadata_candidates = config.get("max_metadata_candidates", 100)
        else:
            workflow.enable_metadata_filter = False
            workflow.max_metadata_candidates = 10000  # ä¸ç­›é€‰ï¼Œè¿”å›æ‰€æœ‰
        
        # L2: å‘é‡æœç´¢
        if config.get("vector_search"):
            workflow.enable_vector_search = True
            workflow.max_vector_candidates = config.get("max_vector_candidates", 50)
            workflow.vector_threshold = config.get("vector_threshold", 0.6)
        else:
            workflow.enable_vector_search = False
            workflow.max_vector_candidates = 0
        
        # L3: LLMåŒ¹é…
        if config.get("llm_matching"):
            workflow.enable_llm_matching = True
            workflow.max_llm_candidates = config.get("max_llm_candidates", 3)
        else:
            workflow.enable_llm_matching = False
            workflow.max_llm_candidates = 0
        
        # è®¾ç½®æ—©åœé˜ˆå€¼
        workflow.early_stop_threshold = config.get("early_stop_threshold", 0.9)
        
        return workflow
    
    async def run_single_query(
        self,
        workflow,
        query: Dict,
        config: Dict,
        ground_truth: Dict
    ) -> Tuple[Dict, Dict]:
        """è¿è¡Œå•ä¸ªæŸ¥è¯¢å¹¶æ”¶é›†æŒ‡æ ‡"""
        from src.core.models import AgentState, TaskStrategy
        
        # å‡†å¤‡æŸ¥è¯¢çŠ¶æ€
        state = AgentState()
        query_table_name = query.get('query_table', '')
        query_column = query.get('query_column', '')
        
        # è°ƒè¯•è¾“å‡º
        if self.debug_mode:
            print(f"\n  ğŸ“ æŸ¥è¯¢: {query_table_name}:{query_column if query_column else 'ALL'}")
        
        # ç¡®ä¿table_metadata_cacheå­˜åœ¨å¹¶æ­£ç¡®
        if not hasattr(workflow, 'table_metadata_cache'):
            print(f"Warning: workflowæ²¡æœ‰table_metadata_cacheå±æ€§")
            return None, {}
        
        if not isinstance(workflow.table_metadata_cache, dict):
            print(f"Warning: table_metadata_cacheä¸æ˜¯dictï¼Œæ˜¯{type(workflow.table_metadata_cache)}")
            # å°è¯•ä»å…¨å±€ç¼“å­˜è·å–
            if hasattr(workflow, '_global_table_cache'):
                workflow.table_metadata_cache = workflow._global_table_cache
            else:
                return None, {}
        
        # è®¾ç½®æŸ¥è¯¢è¡¨
        query_table = workflow.table_metadata_cache.get(query_table_name)
        
        if not query_table:
            if self.debug_mode:
                print(f"  âŒ æŸ¥è¯¢è¡¨ {query_table_name} ä¸åœ¨ç¼“å­˜ä¸­")
            return None, {}
        
        state.query_tables = [query_table]
        state.strategy = TaskStrategy.TOP_DOWN
        
        # è®¾ç½®user_queryï¼ˆå¿…éœ€å­—æ®µï¼‰
        if query_column:
            state.user_query = f"Find tables with joinable column {query_column} for {query_table_name}"
        else:
            state.user_query = f"Find tables similar to {query_table_name}"
        
        # è®°å½•å„å±‚æ—¶é—´
        timing = {}
        total_start = time.time()
        
        # æ‰§è¡Œworkflowï¼ˆå¸¦è¶…æ—¶ï¼‰
        try:
            # è¿è¡Œä¼˜åŒ–çš„workflowï¼Œè¿”å›(state, metrics)
            result = await asyncio.wait_for(
                workflow.run_optimized(
                    state,
                    list(workflow.table_metadata_cache.keys()),
                    ground_truth  # ä¼ å…¥ground truthç”¨äºè¯„ä¼°
                ),
                timeout=10.0
            )
            
            # è§£åŒ…ç»“æœ
            if isinstance(result, tuple) and len(result) == 2:
                result_state, eval_metrics = result
            else:
                result_state = result
                eval_metrics = None
            
            timing["total"] = time.time() - total_start
            
            # æå–å„å±‚æ—¶é—´ï¼ˆå¦‚æœæœ‰ï¼‰
            if hasattr(workflow, 'performance_stats'):
                timing["L1"] = workflow.performance_stats.get("metadata_filter_time", 0)
                timing["L2"] = workflow.performance_stats.get("vector_search_time", 0)
                timing["L3"] = workflow.performance_stats.get("llm_match_time", 0)
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            predictions = []
            
            # å°è¯•å¤šç§æ–¹å¼è·å–é¢„æµ‹ç»“æœ
            if hasattr(result_state, 'table_matches') and result_state.table_matches:
                for match in result_state.table_matches[:20]:  # å¢åŠ åˆ°20ä¸ª
                    if hasattr(match, 'target_table'):
                        predictions.append(match.target_table)
                    elif isinstance(match, dict):
                        # å°è¯•å¤šä¸ªå­—æ®µå
                        for field in ['target_table', 'table_name', 'name', 'table']:
                            if field in match:
                                predictions.append(match[field])
                                break
                    elif isinstance(match, str):
                        predictions.append(match)
                    elif isinstance(match, tuple) and len(match) >= 1:
                        predictions.append(match[0])  # ç¬¬ä¸€ä¸ªå…ƒç´ é€šå¸¸æ˜¯è¡¨å
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰é¢„æµ‹ï¼Œå°è¯•ä»final_resultsè·å–
            if not predictions and hasattr(result_state, 'final_results'):
                if isinstance(result_state.final_results, list):
                    for item in result_state.final_results[:20]:
                        if isinstance(item, str):
                            predictions.append(item)
                        elif isinstance(item, dict):
                            for field in ['target_table', 'table_name', 'name']:
                                if field in item:
                                    predictions.append(item[field])
                                    break
                        elif isinstance(item, tuple) and len(item) >= 1:
                            predictions.append(item[0])
            
            # è·å–ground truth
            gt_key = f"{query_table_name}:{query_column}" if query_column else query_table_name
            true_matches = ground_truth.get(gt_key, [])
            
            # è°ƒè¯•è¾“å‡º
            if self.debug_mode:
                print(f"  ğŸ¯ Ground Truth: {true_matches[:5]}..." if len(true_matches) > 5 else f"  ğŸ¯ Ground Truth: {true_matches}")
                print(f"  ğŸ”® é¢„æµ‹æ•°é‡: {len(predictions)}")
                if predictions:
                    print(f"  ğŸ“Š å‰5ä¸ªé¢„æµ‹: {predictions[:5]}")
                    # æ£€æŸ¥å‘½ä¸­æƒ…å†µ
                    hits = [p for p in predictions if p in true_matches]
                    if hits:
                        print(f"  âœ… å‘½ä¸­: {hits[:3]}")
                    else:
                        print(f"  âŒ æ— å‘½ä¸­")
                else:
                    print(f"  âš ï¸  é¢„æµ‹ä¸ºç©ºï¼æ£€æŸ¥workflowè¿”å›æ ¼å¼")
            
            return {
                "query": query_table_name,
                "predictions": predictions,
                "ground_truth": true_matches,
                "timing": timing
            }, timing
            
        except asyncio.TimeoutError:
            timing["total"] = 10.0
            return None, timing
        except Exception as e:
            print(f"Query error: {e}")
            return None, {}
    
    def calculate_metrics(
        self,
        results: List[Dict],
        ground_truth: Dict,
        config_name: str
    ) -> Dict:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡ - åŒ…å«Precisionã€Recallã€F1"""
        if not results:
            return {
                "accuracy": {},
                "precision": {},
                "recall": {},
                "f1": {},
                "config_name": config_name
            }
        
        # è®¡ç®—Hit@KæŒ‡æ ‡
        hit_at_k = {1: [], 3: [], 5: [], 10: []}
        precision_at_k = {1: [], 3: [], 5: [], 10: []}
        recall_at_k = {1: [], 3: [], 5: [], 10: []}
        f1_at_k = {1: [], 3: [], 5: [], 10: []}
        
        for result in results:
            predictions = result.get("predictions", [])
            true_matches = result.get("ground_truth", [])
            
            if not true_matches:
                continue
            
            for k in [1, 3, 5, 10]:
                pred_at_k = set(predictions[:k])
                true_set = set(true_matches)
                
                # Hit@K
                hit = 1.0 if len(pred_at_k & true_set) > 0 else 0.0
                hit_at_k[k].append(hit)
                
                # Precision@K - åœ¨é¢„æµ‹é›†åˆä¸­æœ‰å¤šå°‘æ˜¯æ­£ç¡®çš„
                if pred_at_k:
                    precision = len(pred_at_k & true_set) / len(pred_at_k)
                else:
                    precision = 0.0
                precision_at_k[k].append(precision)
                
                # Recall@K - çœŸå®åŒ¹é…ä¸­æœ‰å¤šå°‘è¢«æ‰¾åˆ°
                recall = len(pred_at_k & true_set) / len(true_set) if true_set else 0.0
                recall_at_k[k].append(recall)
                
                # F1@K - Precisionå’ŒRecallçš„è°ƒå’Œå¹³å‡
                if precision + recall > 0:
                    f1 = 2 * precision * recall / (precision + recall)
                else:
                    f1 = 0.0
                f1_at_k[k].append(f1)
        
        # è®¡ç®—å¹³å‡å€¼
        metrics = {
            "accuracy": {
                f"hit@{k}": np.mean(hit_at_k[k]) * 100 if hit_at_k[k] else 0
                for k in [1, 3, 5, 10]
            },
            "precision": {
                f"p@{k}": np.mean(precision_at_k[k]) * 100 if precision_at_k[k] else 0
                for k in [1, 3, 5, 10]
            },
            "recall": {
                f"r@{k}": np.mean(recall_at_k[k]) * 100 if recall_at_k[k] else 0
                for k in [1, 3, 5, 10]
            },
            "f1": {
                f"f1@{k}": np.mean(f1_at_k[k]) * 100 if f1_at_k[k] else 0
                for k in [1, 3, 5, 10]
            },
            "evaluated_queries": len(results),
            "config_name": config_name
        }
        
        return metrics
    
    def print_results_summary(self, metrics: Dict, config_name: str):
        """æ‰“å°ç»“æœæ‘˜è¦ - åŒ…å«å®Œæ•´è¯„ä»·æŒ‡æ ‡"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š Ablation Results: {config_name}")
        print(f"{'='*80}")
        
        # ä¸»è¦æŒ‡æ ‡è¡¨æ ¼ (K=5)
        main_data = [
            ["Metric", "@1", "@5", "@10"],
            ["Hit Rate", 
             f"{metrics['accuracy'].get('hit@1', 0):.1f}%",
             f"{metrics['accuracy'].get('hit@5', 0):.1f}%",
             f"{metrics['accuracy'].get('hit@10', 0):.1f}%"],
            ["Precision",
             f"{metrics['precision'].get('p@1', 0):.1f}%",
             f"{metrics['precision'].get('p@5', 0):.1f}%",
             f"{metrics['precision'].get('p@10', 0):.1f}%"],
            ["Recall",
             f"{metrics['recall'].get('r@1', 0):.1f}%",
             f"{metrics['recall'].get('r@5', 0):.1f}%",
             f"{metrics['recall'].get('r@10', 0):.1f}%"],
            ["F1-Score",
             f"{metrics['f1'].get('f1@1', 0):.1f}%",
             f"{metrics['f1'].get('f1@5', 0):.1f}%",
             f"{metrics['f1'].get('f1@10', 0):.1f}%"],
        ]
        print("\nğŸ¯ Evaluation Metrics:")
        print(tabulate(main_data, headers="firstrow", tablefmt="grid"))
        
        # æ€§èƒ½è¡¨æ ¼
        perf = metrics.get("performance", {})
        avg_times = perf.get("avg_times", {})
        
        total_time = avg_times.get('total', 0.001) if avg_times.get('total', 0) > 0 else 0.001
        perf_data = [
            ["Layer", "Avg Time (s)", "Percentage"],
            ["L1 (Metadata)", f"{avg_times.get('L1', 0):.3f}", 
             f"{avg_times.get('L1', 0)/total_time*100:.1f}%"],
            ["L2 (Vector)", f"{avg_times.get('L2', 0):.3f}",
             f"{avg_times.get('L2', 0)/total_time*100:.1f}%"],
            ["L3 (LLM)", f"{avg_times.get('L3', 0):.3f}",
             f"{avg_times.get('L3', 0)/total_time*100:.1f}%"],
            ["Total", f"{avg_times.get('total', 0):.3f}", "100%"],
        ]
        print("\nâš¡ Performance Breakdown:")
        print(tabulate(perf_data, headers="firstrow", tablefmt="grid"))
        
        # æˆåŠŸç‡
        print(f"\nâœ… Success Rate: {perf.get('success_rate', 0)*100:.1f}%")
        qps = 1.0/avg_times.get('total', 1) if avg_times.get('total', 0) > 0 else 0
        print(f"ğŸ“ˆ QPS: {qps:.2f}" if qps > 0 else "ğŸ“ˆ QPS: N/A")
    
    def save_results(
        self,
        metrics: Dict,
        config_name: str,
        task_type: str,
        dataset_size: str
    ):
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ablation_{config_name}_{task_type}_{dataset_size}_{timestamp}.json"
        filepath = self.results_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Results saved to: {filepath}")
    
    async def run_all_configurations(
        self,
        task_type: str,
        dataset_size: str,
        max_queries: int = 10
    ):
        """è¿è¡Œæ‰€æœ‰é…ç½®çš„å®éªŒ"""
        all_results = {}
        
        for config_name in self.configs.keys():
            print(f"\n{'#'*80}")
            print(f"# Configuration {list(self.configs.keys()).index(config_name)+1}/{len(self.configs)}: {config_name}")
            print(f"{'#'*80}")
            
            try:
                result = await self.run_single_configuration(
                    config_name, task_type, dataset_size, max_queries
                )
                all_results[config_name] = result
            except Exception as e:
                print(f"âŒ Failed to run {config_name}: {e}")
                all_results[config_name] = {"error": str(e)}
        
        # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
        self.generate_comparison_report(all_results, task_type, dataset_size)
        
        return all_results
    
    def generate_comparison_report(
        self,
        all_results: Dict,
        task_type: str,
        dataset_size: str
    ):
        """ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š ABLATION STUDY COMPARISON REPORT")
        print(f"Task: {task_type.upper()} | Dataset: {dataset_size}")
        print(f"{'='*80}")
        
        # å‡†å¤‡æ¯”è¾ƒæ•°æ®
        comparison_data = []
        for config_name, result in all_results.items():
            if "error" in result:
                continue
            
            acc = result.get("accuracy", {})
            prec = result.get("precision", {})
            rec = result.get("recall", {})
            f1 = result.get("f1", {})
            perf = result.get("performance", {})
            avg_times = perf.get("avg_times", {})
            
            comparison_data.append([
                config_name,
                f"{acc.get('hit@1', 0):.1f}%",
                f"{acc.get('hit@5', 0):.1f}%",
                f"{prec.get('p@5', 0):.1f}%",
                f"{rec.get('r@5', 0):.1f}%",
                f"{f1.get('f1@5', 0):.1f}%",
                f"{avg_times.get('total', 0):.3f}s",
                f"{1.0/avg_times.get('total', 1):.1f}" if avg_times.get('total') else "N/A"
            ])
        
        # æ‰“å°æ¯”è¾ƒè¡¨
        headers = ["Config", "Hit@1", "Hit@5", "Precision@5", "Recall@5", "F1@5", "Latency", "QPS"]
        print("\nğŸ“ˆ Performance Comparison:")
        print(tabulate(comparison_data, headers=headers, tablefmt="grid"))
        
        # å±‚è´¡çŒ®åˆ†æ - æ˜¾ç¤ºé€å±‚æå‡
        if "L1" in all_results and "L1+L2" in all_results and "L1+L2+L3" in all_results:
            # è·å–å„å±‚æŒ‡æ ‡
            l1_metrics = all_results["L1"]
            l12_metrics = all_results["L1+L2"]
            l123_metrics = all_results["L1+L2+L3"]
            
            print("\nğŸ”¬ Layer-by-Layer Contribution Analysis:")
            print("\nğŸ“Š Progressive Performance Improvement:")
            
            # åˆ›å»ºé€å±‚æå‡è¡¨æ ¼
            layer_data = [
                ["Metric", "L1 Only", "L1+L2", "L1+L2+L3", "Total Gain"],
                ["Hit@5",
                 f"{l1_metrics.get('accuracy', {}).get('hit@5', 0):.1f}%",
                 f"{l12_metrics.get('accuracy', {}).get('hit@5', 0):.1f}%",
                 f"{l123_metrics.get('accuracy', {}).get('hit@5', 0):.1f}%",
                 f"+{l123_metrics.get('accuracy', {}).get('hit@5', 0) - l1_metrics.get('accuracy', {}).get('hit@5', 0):.1f}%"],
                ["Precision@5",
                 f"{l1_metrics.get('precision', {}).get('p@5', 0):.1f}%",
                 f"{l12_metrics.get('precision', {}).get('p@5', 0):.1f}%",
                 f"{l123_metrics.get('precision', {}).get('p@5', 0):.1f}%",
                 f"+{l123_metrics.get('precision', {}).get('p@5', 0) - l1_metrics.get('precision', {}).get('p@5', 0):.1f}%"],
                ["Recall@5",
                 f"{l1_metrics.get('recall', {}).get('r@5', 0):.1f}%",
                 f"{l12_metrics.get('recall', {}).get('r@5', 0):.1f}%",
                 f"{l123_metrics.get('recall', {}).get('r@5', 0):.1f}%",
                 f"+{l123_metrics.get('recall', {}).get('r@5', 0) - l1_metrics.get('recall', {}).get('r@5', 0):.1f}%"],
                ["F1@5",
                 f"{l1_metrics.get('f1', {}).get('f1@5', 0):.1f}%",
                 f"{l12_metrics.get('f1', {}).get('f1@5', 0):.1f}%",
                 f"{l123_metrics.get('f1', {}).get('f1@5', 0):.1f}%",
                 f"+{l123_metrics.get('f1', {}).get('f1@5', 0) - l1_metrics.get('f1', {}).get('f1@5', 0):.1f}%"]
            ]
            print(tabulate(layer_data, headers="firstrow", tablefmt="grid"))
            
            # è®¡ç®—å¹¶æ˜¾ç¤ºå¢é‡è´¡çŒ®
            l1_f1 = l1_metrics.get('f1', {}).get('f1@5', 0)
            l12_f1 = l12_metrics.get('f1', {}).get('f1@5', 0)
            l123_f1 = l123_metrics.get('f1', {}).get('f1@5', 0)
            
            print("\nğŸ“ˆ Incremental Contribution (F1@5):")
            print(f"  L1 Baseline: {l1_f1:.1f}%")
            print(f"  +L2 Contribution: {l12_f1 - l1_f1:+.1f}% â†’ Total: {l12_f1:.1f}%")
            print(f"  +L3 Contribution: {l123_f1 - l12_f1:+.1f}% â†’ Total: {l123_f1:.1f}%")
        
        # ä¿å­˜æ¯”è¾ƒæŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.results_dir / f"comparison_{task_type}_{dataset_size}_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Comparison report saved to: {report_file}")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Three-Layer Ablation Study")
    parser.add_argument("--config", type=str, default="progressive",
                        choices=["L1", "L2", "L3", "L1+L2", "L1+L3", "L2+L3", "L1+L2+L3", "all", "progressive"],
                        help="Configuration to test (progressive = L1, L1+L2, L1+L2+L3)")
    parser.add_argument("--task", type=str, default="join",
                        choices=["join", "union", "both"],
                        help="Task type")
    parser.add_argument("--dataset", type=str, default="subset",
                        choices=["subset", "complete"],
                        help="Dataset size")
    parser.add_argument("--max-queries", type=int, default=10,
                        help="Maximum queries to test")
    parser.add_argument("--debug", action="store_true",
                        help="Enable debug mode for detailed output")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå®éªŒå®ä¾‹ï¼Œä¼ é€’æ•°æ®é›†å¤§å°
    study = AblationStudy(debug_mode=args.debug, dataset_size=args.dataset)
    
    # è¿è¡Œå®éªŒ
    if args.config == "progressive":
        # ä¸“æ³¨äºé€’è¿›å®éªŒï¼šL1 â†’ L1+L2 â†’ L1+L2+L3
        print("\n" + "#"*80)
        print("# ğŸš€ Progressive Layer Ablation Study")
        print("# Testing: L1 â†’ L1+L2 â†’ L1+L2+L3")
        print("#"*80 + "\n")
        
        progressive_results = {}
        for config in ["L1", "L1+L2", "L1+L2+L3"]:
            print(f"\n{'='*80}")
            print(f"Testing Configuration: {config}")
            print(f"{'='*80}")
            
            if args.task == "both":
                for task in ["join", "union"]:
                    result = await study.run_single_configuration(
                        config, task, args.dataset, args.max_queries
                    )
                    progressive_results[f"{config}_{task}"] = result
            else:
                result = await study.run_single_configuration(
                    config, args.task, args.dataset, args.max_queries
                )
                progressive_results[config] = result
        
        # ç”Ÿæˆé€’è¿›æŠ¥å‘Š
        study.generate_comparison_report(progressive_results, args.task, args.dataset)
        
    elif args.config == "all":
        # è¿è¡Œæ‰€æœ‰é…ç½®
        if args.task == "both":
            # è¿è¡Œä¸¤ä¸ªä»»åŠ¡
            for task in ["join", "union"]:
                print(f"\n{'#'*80}")
                print(f"# Task: {task.upper()}")
                print(f"{'#'*80}")
                await study.run_all_configurations(task, args.dataset, args.max_queries)
        else:
            await study.run_all_configurations(args.task, args.dataset, args.max_queries)
    else:
        # è¿è¡Œå•ä¸ªé…ç½®
        if args.task == "both":
            for task in ["join", "union"]:
                await study.run_single_configuration(
                    args.config, task, args.dataset, args.max_queries
                )
        else:
            await study.run_single_configuration(
                args.config, args.task, args.dataset, args.max_queries
            )
    
    print("\nâœ… Ablation study completed!")


if __name__ == "__main__":
    asyncio.run(main())