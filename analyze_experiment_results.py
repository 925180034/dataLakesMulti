#!/usr/bin/env python
"""
åˆ†æå®éªŒç»“æœ
"""

import json
import os
import sys
from datetime import datetime
from typing import Dict, List

def load_experiment_result(file_path: str) -> Dict:
    """åŠ è½½å®éªŒç»“æœ"""
    with open(file_path, 'r') as f:
        return json.load(f)

def analyze_single_result(result: Dict, file_name: str):
    """åˆ†æå•ä¸ªå®éªŒç»“æœ"""
    print(f"\nğŸ“Š Analysis of: {file_name}")
    print("=" * 60)
    
    # é…ç½®ä¿¡æ¯
    config = result.get('config', {})
    print(f"Dataset: {config.get('dataset', 'N/A')}")
    print(f"Queries: {config.get('queries', 'N/A')}")
    print(f"Workers: {config.get('workers', 'N/A')}")
    
    # æ€§èƒ½æŒ‡æ ‡
    performance = result.get('performance', {})
    print(f"\nâ±ï¸  Performance:")
    print(f"  Total Time: {performance.get('total_time', 0):.2f}s")
    print(f"  Avg Response: {performance.get('avg_response_time', 0):.3f}s")
    print(f"  Throughput: {performance.get('throughput', 0):.2f} QPS")
    
    # å‡†ç¡®ç‡æŒ‡æ ‡
    metrics = result.get('metrics', {})
    if metrics:
        print(f"\nğŸ¯ Accuracy:")
        print(f"  Precision: {metrics.get('precision', 0):.3f}")
        print(f"  Recall: {metrics.get('recall', 0):.3f}")
        print(f"  F1-Score: {metrics.get('f1', 0):.3f}")
        print(f"  MRR: {metrics.get('mrr', 0):.3f}")
        
        # Hit@KæŒ‡æ ‡
        print(f"\nğŸ“ˆ Hit@K:")
        for k in [1, 3, 5, 10]:
            hit_k = metrics.get(f'hit@{k}', 0)
            print(f"  Hit@{k}: {hit_k:.3f}")

def compare_results(results: List[tuple]):
    """æ¯”è¾ƒå¤šä¸ªå®éªŒç»“æœ"""
    print("\n" + "=" * 80)
    print("ğŸ“Š COMPARATIVE ANALYSIS")
    print("=" * 80)
    
    # è¡¨å¤´
    print(f"\n{'Experiment':<30} {'Time(s)':<10} {'QPS':<10} {'Precision':<10} {'Recall':<10} {'F1':<10}")
    print("-" * 80)
    
    for file_name, result in results:
        config = result.get('config', {})
        performance = result.get('performance', {})
        metrics = result.get('metrics', {})
        
        exp_name = f"{config.get('dataset', 'N/A')}_{config.get('queries', 0)}q"
        total_time = performance.get('total_time', 0)
        throughput = performance.get('throughput', 0)
        precision = metrics.get('precision', 0) if metrics else 0
        recall = metrics.get('recall', 0) if metrics else 0
        f1 = metrics.get('f1', 0) if metrics else 0
        
        print(f"{exp_name:<30} {total_time:<10.2f} {throughput:<10.2f} {precision:<10.3f} {recall:<10.3f} {f1:<10.3f}")

def main():
    """ä¸»å‡½æ•°"""
    # å®éªŒç»“æœç›®å½•
    result_dir = "experiment_results/multi_agent_llm"
    
    if not os.path.exists(result_dir):
        print(f"âŒ Directory not found: {result_dir}")
        return
    
    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    json_files = [f for f in os.listdir(result_dir) if f.endswith('.json')]
    
    if not json_files:
        print(f"âŒ No experiment results found in {result_dir}")
        return
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    json_files.sort(reverse=True)
    
    print("=" * 80)
    print("ğŸ”¬ MULTI-AGENT SYSTEM EXPERIMENT RESULTS ANALYSIS")
    print("=" * 80)
    print(f"\nFound {len(json_files)} experiment results")
    
    # åŠ è½½æ‰€æœ‰ç»“æœ
    all_results = []
    for file_name in json_files[:10]:  # åªåˆ†ææœ€è¿‘10ä¸ª
        file_path = os.path.join(result_dir, file_name)
        try:
            result = load_experiment_result(file_path)
            all_results.append((file_name, result))
            
            # åˆ†æå•ä¸ªç»“æœ
            analyze_single_result(result, file_name)
        except Exception as e:
            print(f"âŒ Error loading {file_name}: {e}")
    
    # æ¯”è¾ƒç»“æœ
    if len(all_results) > 1:
        compare_results(all_results)
    
    # æœ€ä½³æ€§èƒ½
    if all_results:
        print("\n" + "=" * 80)
        print("ğŸ† BEST PERFORMANCE")
        print("=" * 80)
        
        # æ‰¾å‡ºæœ€ä½³QPS
        best_qps = max(all_results, key=lambda x: x[1].get('performance', {}).get('throughput', 0))
        print(f"\n Highest QPS: {best_qps[0]}")
        print(f"   QPS: {best_qps[1].get('performance', {}).get('throughput', 0):.2f}")
        
        # æ‰¾å‡ºæœ€ä½³F1åˆ†æ•°
        results_with_metrics = [(f, r) for f, r in all_results if r.get('metrics')]
        if results_with_metrics:
            best_f1 = max(results_with_metrics, key=lambda x: x[1].get('metrics', {}).get('f1', 0))
            print(f"\nHighest F1-Score: {best_f1[0]}")
            print(f"   F1: {best_f1[1].get('metrics', {}).get('f1', 0):.3f}")

if __name__ == "__main__":
    main()