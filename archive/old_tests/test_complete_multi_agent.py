#!/usr/bin/env python
"""
å®Œæ•´çš„å¤šæ™ºèƒ½ä½“æ•°æ®æ¹–å‘ç°ç³»ç»Ÿæµ‹è¯•
Complete Multi-Agent Data Lake Discovery System Test
"""

import json
import time
import numpy as np
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass, field, asdict
from pathlib import Path
import logging
import os
import sys

# è®¾ç½®Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===================== æ•°æ®ç»“æ„ =====================

@dataclass
class QueryTask:
    """æŸ¥è¯¢ä»»åŠ¡"""
    query_id: str
    query_table: str
    task_type: str  # 'join' or 'union'
    ground_truth: List[str]

@dataclass
class MatchResult:
    """åŒ¹é…ç»“æœ"""
    query_table: str
    matched_table: str
    score: float
    match_type: str
    method: str = ""

@dataclass
class EvaluationMetrics:
    """è¯„ä»·æŒ‡æ ‡"""
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    hit_at_1: float = 0.0
    hit_at_3: float = 0.0
    hit_at_5: float = 0.0
    hit_at_10: float = 0.0
    mrr: float = 0.0
    success_rate: float = 0.0
    avg_time: float = 0.0

# ===================== å¤šAgentç³»ç»Ÿï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰ =====================

class SimpleMultiAgentSystem:
    """ç®€åŒ–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        logger.info("Initializing Simple Multi-Agent System...")
        
        # åˆå§‹åŒ–å·¥å…·
        self._init_tools()
        
        # æ•°æ®ç¼“å­˜
        self.tables = {}
        self.embeddings = {}
        self.metadata_index = {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.query_count = 0
        self.success_count = 0
        self.query_times = []
        
        logger.info("System initialized")
        
    def _init_tools(self):
        """åˆå§‹åŒ–å·¥å…·å±‚"""
        try:
            # å¯¼å…¥å·¥å…·
            from src.core.ultra_optimized_workflow import UltraOptimizedWorkflow
            from src.tools.metadata_filter import MetadataFilter
            from src.tools.hnsw_search import HNSWVectorSearch
            from src.tools.smart_llm_matcher import SmartLLMMatcher
            from src.utils.llm_client import GeminiClient
            
            # åˆå§‹åŒ–å·¥ä½œæµ
            self.workflow = UltraOptimizedWorkflow()
            
            # åˆå§‹åŒ–å·¥å…·
            self.metadata_filter = MetadataFilter()
            self.vector_search = HNSWVectorSearch(dimension=384)
            
            # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
            llm_config = {
                "model_name": "gemini-1.5-flash",
                "temperature": 0.1,
                "max_tokens": 2000
            }
            self.llm_client = GeminiClient(llm_config)
            self.llm_matcher = SmartLLMMatcher(self.llm_client)
            
            logger.info("Tools initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize tools: {e}")
            raise
            
    def load_data(self, tables_path: str):
        """åŠ è½½æ•°æ®é›†"""
        logger.info(f"Loading data from {tables_path}")
        
        with open(tables_path, 'r') as f:
            tables_data = json.load(f)
        
        # å­˜å‚¨è¡¨ä¿¡æ¯
        for table in tables_data:
            self.tables[table['table_name']] = table
            
            # æ„å»ºå…ƒæ•°æ®ç´¢å¼•
            col_count = len(table['columns'])
            if col_count not in self.metadata_index:
                self.metadata_index[col_count] = []
            self.metadata_index[col_count].append(table['table_name'])
        
        logger.info(f"Loaded {len(self.tables)} tables")
        
        # åˆå§‹åŒ–å·¥ä½œæµ
        if hasattr(self.workflow, 'initialize'):
            self.workflow.initialize(tables_data)
            logger.info("Workflow initialized with table data")
            
    def process_query(self, query_task: QueryTask) -> List[MatchResult]:
        """å¤„ç†å•ä¸ªæŸ¥è¯¢ï¼ˆä½¿ç”¨ç°æœ‰çš„å·¥ä½œæµï¼‰"""
        start_time = time.time()
        self.query_count += 1
        
        try:
            query_table = self.tables.get(query_task.query_table)
            if not query_table:
                logger.error(f"Query table {query_task.query_table} not found")
                return []
            
            # ä½¿ç”¨ä¼˜åŒ–çš„å·¥ä½œæµ
            if hasattr(self.workflow, 'discover_tables'):
                results = self.workflow.discover_tables(
                    query_table=query_table,
                    task_type=query_task.task_type,
                    top_k=10
                )
                
                # è½¬æ¢ç»“æœæ ¼å¼
                matches = []
                for result in results:
                    if isinstance(result, dict):
                        matches.append(MatchResult(
                            query_table=query_task.query_table,
                            matched_table=result.get('table_name', ''),
                            score=result.get('score', 0.0),
                            match_type=query_task.task_type,
                            method=result.get('method', 'workflow')
                        ))
                    elif isinstance(result, tuple):
                        table_name, score = result
                        matches.append(MatchResult(
                            query_table=query_task.query_table,
                            matched_table=table_name,
                            score=score,
                            match_type=query_task.task_type,
                            method='workflow'
                        ))
                
                if matches:
                    self.success_count += 1
                    
            else:
                # ç®€å•çš„åŸºäºè§„åˆ™çš„åŒ¹é…ï¼ˆå¤‡ç”¨ï¼‰
                matches = self._simple_matching(query_table, query_task.task_type)
            
            query_time = time.time() - start_time
            self.query_times.append(query_time)
            
            logger.info(f"Processed {query_task.query_id} in {query_time:.3f}s, found {len(matches)} matches")
            
            return matches[:10]  # è¿”å›Top-10
            
        except Exception as e:
            logger.error(f"Error processing query {query_task.query_id}: {e}")
            return []
            
    def _simple_matching(self, query_table: Dict, task_type: str) -> List[MatchResult]:
        """ç®€å•çš„åŸºäºè§„åˆ™çš„åŒ¹é…ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        matches = []
        query_col_count = len(query_table['columns'])
        
        # åŸºäºåˆ—æ•°çš„ç®€å•åŒ¹é…
        for table_name, table in self.tables.items():
            if table_name == query_table['table_name']:
                continue
                
            col_count = len(table['columns'])
            
            # åˆ—æ•°ç›¸ä¼¼
            if abs(col_count - query_col_count) <= 2:
                score = 1.0 - abs(col_count - query_col_count) / 10.0
                
                # æ£€æŸ¥åˆ—åé‡å 
                query_col_names = {col['name'].lower() for col in query_table['columns']}
                table_col_names = {col['name'].lower() for col in table['columns']}
                overlap = len(query_col_names & table_col_names)
                
                if overlap > 0:
                    score += overlap / max(len(query_col_names), len(table_col_names))
                    
                if score > 0.3:
                    matches.append(MatchResult(
                        query_table=query_table['table_name'],
                        matched_table=table_name,
                        score=min(score, 1.0),
                        match_type=task_type,
                        method='simple_rule'
                    ))
        
        # æ’åºå¹¶è¿”å›
        matches.sort(key=lambda x: x.score, reverse=True)
        return matches
        
    def process_batch(self, query_tasks: List[QueryTask]) -> Dict[str, List[MatchResult]]:
        """æ‰¹é‡å¤„ç†æŸ¥è¯¢"""
        logger.info(f"Processing batch of {len(query_tasks)} queries")
        
        results = {}
        for query_task in query_tasks:
            results[query_task.query_id] = self.process_query(query_task)
            
        return results
        
    def calculate_metrics(self, results: Dict[str, List[MatchResult]], 
                         query_tasks: List[QueryTask]) -> EvaluationMetrics:
        """è®¡ç®—è¯„ä»·æŒ‡æ ‡"""
        metrics = EvaluationMetrics()
        
        total_precision = 0.0
        total_recall = 0.0
        hit_counts = {1: 0, 3: 0, 5: 0, 10: 0}
        mrr_sum = 0.0
        valid_queries = 0
        
        for query_task in query_tasks:
            query_results = results.get(query_task.query_id, [])
            
            if query_results:
                valid_queries += 1
                predicted = [r.matched_table for r in query_results]
                ground_truth = query_task.ground_truth
                
                if ground_truth:
                    # Precisionå’ŒRecall
                    true_positives = len(set(predicted[:10]) & set(ground_truth))
                    precision = true_positives / min(10, len(predicted)) if predicted else 0
                    recall = true_positives / len(ground_truth)
                    
                    total_precision += precision
                    total_recall += recall
                    
                    # Hit@K
                    for k in [1, 3, 5, 10]:
                        if len(predicted) >= k:
                            if any(p in ground_truth for p in predicted[:k]):
                                hit_counts[k] += 1
                    
                    # MRR
                    for i, p in enumerate(predicted):
                        if p in ground_truth:
                            mrr_sum += 1.0 / (i + 1)
                            break
        
        # è®¡ç®—å¹³å‡å€¼
        if valid_queries > 0:
            metrics.precision = total_precision / valid_queries
            metrics.recall = total_recall / valid_queries
            
            if metrics.precision + metrics.recall > 0:
                metrics.f1_score = 2 * metrics.precision * metrics.recall / (metrics.precision + metrics.recall)
            
            metrics.hit_at_1 = hit_counts[1] / valid_queries
            metrics.hit_at_3 = hit_counts[3] / valid_queries
            metrics.hit_at_5 = hit_counts[5] / valid_queries
            metrics.hit_at_10 = hit_counts[10] / valid_queries
            metrics.mrr = mrr_sum / valid_queries
        
        # å…¶ä»–æŒ‡æ ‡
        metrics.success_rate = self.success_count / max(self.query_count, 1)
        if self.query_times:
            metrics.avg_time = np.mean(self.query_times)
            
        return metrics

# ===================== ä¸»å‡½æ•° =====================

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Test Complete Multi-Agent System')
    parser.add_argument('--dataset', choices=['subset', 'complete'], 
                       default='subset', help='Dataset to use')
    parser.add_argument('--queries', type=int, default=100, 
                       help='Number of queries to test')
    parser.add_argument('--task', choices=['join', 'union', 'both'], 
                       default='both', help='Task type to test')
    
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ğŸš€ COMPLETE MULTI-AGENT DATA LAKE DISCOVERY TEST")
    print("="*70)
    
    # é€‰æ‹©æ•°æ®é›†
    if args.dataset == 'subset':
        tables_file = 'examples/final_subset_tables.json'
        ground_truth_file = 'examples/final_subset_ground_truth.json'
        print(f"ğŸ“Š Dataset: SUBSET (100 tables)")
    else:
        tables_file = 'examples/final_complete_tables.json'
        ground_truth_file = 'examples/final_complete_ground_truth.json'
        print(f"ğŸ“Š Dataset: COMPLETE (1534 tables)")
    
    print(f"ğŸ”§ Queries: {args.queries}")
    print(f"ğŸ“‹ Task: {args.task}")
    print()
    
    # åˆ›å»ºç³»ç»Ÿ
    system = SimpleMultiAgentSystem()
    
    # åŠ è½½æ•°æ®
    print("ğŸ“¥ Loading dataset...")
    system.load_data(tables_file)
    
    # åŠ è½½ground truth
    print("ğŸ“¥ Loading ground truth...")
    with open(ground_truth_file, 'r') as f:
        ground_truth_data = json.load(f)
    
    # åˆ›å»ºæŸ¥è¯¢ä»»åŠ¡
    query_tasks = []
    
    if isinstance(ground_truth_data, dict):
        # æ–°æ ¼å¼ï¼šåˆ†åˆ«å­˜å‚¨joinå’Œunion
        if args.task in ['join', 'both'] and 'join_ground_truth' in ground_truth_data:
            for i, item in enumerate(ground_truth_data['join_ground_truth'][:args.queries//2 if args.task == 'both' else args.queries]):
                query_tasks.append(QueryTask(
                    query_id=f"join_{i}",
                    query_table=item['table'],
                    task_type='join',
                    ground_truth=item.get('ground_truth', item.get('expected', []))
                ))
                
        if args.task in ['union', 'both'] and 'union_ground_truth' in ground_truth_data:
            for i, item in enumerate(ground_truth_data['union_ground_truth'][:args.queries//2 if args.task == 'both' else args.queries]):
                query_tasks.append(QueryTask(
                    query_id=f"union_{i}",
                    query_table=item['table'],
                    task_type='union',
                    ground_truth=item.get('ground_truth', item.get('expected', []))
                ))
    
    if not query_tasks:
        print("âŒ No query tasks created. Check ground truth format.")
        return
    
    print(f"ğŸ“‹ Created {len(query_tasks)} query tasks")
    
    # åˆ†åˆ«ç»Ÿè®¡
    join_count = sum(1 for t in query_tasks if t.task_type == 'join')
    union_count = sum(1 for t in query_tasks if t.task_type == 'union')
    if join_count > 0:
        print(f"   - JOIN: {join_count}")
    if union_count > 0:
        print(f"   - UNION: {union_count}")
    print()
    
    # è¿è¡Œæµ‹è¯•
    print("ğŸƒ Processing queries...")
    start_time = time.time()
    
    results = system.process_batch(query_tasks)
    
    total_time = time.time() - start_time
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = system.calculate_metrics(results, query_tasks)
    
    # è¾“å‡ºç»“æœ
    print("\n" + "="*70)
    print("ğŸ“Š EVALUATION RESULTS")
    print("="*70)
    
    print(f"\nâ±ï¸  Performance:")
    print(f"   Total Time: {total_time:.2f}s")
    print(f"   Queries Processed: {system.query_count}")
    print(f"   Successful: {system.success_count}")
    print(f"   Success Rate: {metrics.success_rate*100:.1f}%")
    print(f"   Avg Time/Query: {metrics.avg_time:.3f}s")
    print(f"   Throughput: {system.query_count/max(total_time, 0.001):.2f} QPS")
    
    print(f"\nğŸ¯ Accuracy Metrics:")
    print(f"   Precision: {metrics.precision:.3f}")
    print(f"   Recall: {metrics.recall:.3f}")
    print(f"   F1-Score: {metrics.f1_score:.3f}")
    print(f"   MRR: {metrics.mrr:.3f}")
    
    print(f"\nğŸ“ˆ Hit@K Metrics:")
    print(f"   Hit@1: {metrics.hit_at_1:.3f}")
    print(f"   Hit@3: {metrics.hit_at_3:.3f}")
    print(f"   Hit@5: {metrics.hit_at_5:.3f}")
    print(f"   Hit@10: {metrics.hit_at_10:.3f}")
    
    # åˆ†åˆ«ç»Ÿè®¡JOINå’ŒUNIONæ€§èƒ½
    if join_count > 0 and union_count > 0:
        print(f"\nğŸ“Š Task Breakdown:")
        
        # JOINæ€§èƒ½
        join_results = {k: v for k, v in results.items() if 'join' in k}
        join_success = sum(1 for v in join_results.values() if v)
        print(f"   JOIN: {join_success}/{join_count} ({join_success/max(join_count,1)*100:.1f}%)")
        
        # UNIONæ€§èƒ½
        union_results = {k: v for k, v in results.items() if 'union' in k}
        union_success = sum(1 for v in union_results.values() if v)
        print(f"   UNION: {union_success}/{union_count} ({union_success/max(union_count,1)*100:.1f}%)")
    
    # ä¿å­˜ç»“æœ
    timestamp = int(time.time())
    output_dir = Path('experiment_results/multi_agent')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = output_dir / f"complete_test_{args.dataset}_{args.queries}q_{timestamp}.json"
    
    save_data = {
        'config': {
            'dataset': args.dataset,
            'queries': args.queries,
            'task': args.task,
            'total_time': total_time
        },
        'metrics': asdict(metrics),
        'statistics': {
            'total_queries': system.query_count,
            'successful_queries': system.success_count,
            'join_queries': join_count,
            'union_queries': union_count
        }
    }
    
    with open(output_file, 'w') as f:
        json.dump(save_data, f, indent=2, default=str)
    
    print(f"\nğŸ’¾ Results saved to: {output_file}")
    print("\nâœ… TEST COMPLETED SUCCESSFULLY!")
    print("="*70 + "\n")

if __name__ == "__main__":
    main()