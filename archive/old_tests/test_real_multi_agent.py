#!/usr/bin/env python
"""
æµ‹è¯•çœŸå®çš„å¤šæ™ºèƒ½ä½“æ•°æ®æ¹–å‘ç°ç³»ç»Ÿ
Test Real Multi-Agent Data Lake Discovery System
"""

import asyncio
import json
import time
import logging
from pathlib import Path
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.core.real_multi_agent_system import (
    MultiAgentOrchestrator, 
    QueryTask,
    SystemMetrics
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def load_ground_truth(file_path: str, task_type: str = None):
    """åŠ è½½ground truthæ•°æ®"""
    with open(file_path, 'r') as f:
        data = json.load(f)
    
    # å¦‚æœæŒ‡å®šäº†ä»»åŠ¡ç±»å‹ï¼Œåªè¿”å›è¯¥ç±»å‹çš„æ•°æ®
    if task_type:
        if task_type == 'join':
            return data.get('join_ground_truth', [])
        elif task_type == 'union':
            return data.get('union_ground_truth', [])
    
    # è¿”å›æ‰€æœ‰æ•°æ®
    all_queries = []
    
    # å¤„ç†ä¸åŒæ ¼å¼çš„ground truth
    if isinstance(data, dict):
        # æ–°æ ¼å¼ï¼šåˆ†åˆ«å­˜å‚¨joinå’Œunion
        if 'join_ground_truth' in data:
            for item in data['join_ground_truth']:
                all_queries.append({
                    'query_table': item['table'],
                    'ground_truth': item.get('ground_truth', item.get('expected', [])),
                    'type': 'join'
                })
        if 'union_ground_truth' in data:
            for item in data['union_ground_truth']:
                all_queries.append({
                    'query_table': item['table'],
                    'ground_truth': item.get('ground_truth', item.get('expected', [])),
                    'type': 'union'
                })
    elif isinstance(data, list):
        # æ—§æ ¼å¼ï¼šåˆ—è¡¨å½¢å¼
        for item in data:
            query_type = item.get('type', 'join')
            all_queries.append({
                'query_table': item.get('query_table', item.get('table')),
                'ground_truth': item.get('ground_truth', item.get('expected', [])),
                'type': query_type
            })
    
    return all_queries

async def test_multi_agent_system(
    dataset: str = 'subset',
    max_queries: int = 100,
    parallel_workers: int = 4
):
    """æµ‹è¯•å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ"""
    
    print("\n" + "="*70)
    print("ğŸš€ REAL MULTI-AGENT DATA LAKE DISCOVERY SYSTEM TEST")
    print("="*70)
    
    # ç¡®å®šæ•°æ®æ–‡ä»¶è·¯å¾„
    if dataset == 'subset':
        tables_file = 'examples/final_subset_tables.json'
        ground_truth_file = 'examples/final_subset_ground_truth.json'
        print(f"ğŸ“Š Using SUBSET dataset (100 tables)")
    else:
        tables_file = 'examples/final_complete_tables.json'
        ground_truth_file = 'examples/final_complete_ground_truth.json'
        print(f"ğŸ“Š Using COMPLETE dataset (1534 tables)")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(tables_file).exists():
        logger.error(f"Tables file not found: {tables_file}")
        return
    if not Path(ground_truth_file).exists():
        logger.error(f"Ground truth file not found: {ground_truth_file}")
        return
    
    print(f"ğŸ“ Loading data from: {tables_file}")
    print(f"ğŸ“ Ground truth from: {ground_truth_file}")
    print(f"ğŸ”§ Max queries: {max_queries}")
    print(f"âš¡ Parallel workers: {parallel_workers}")
    print()
    
    # åˆ›å»ºåè°ƒå™¨
    print("ğŸ¤– Initializing Multi-Agent Orchestrator...")
    orchestrator = MultiAgentOrchestrator('config_optimized.yml')
    
    # åŠ è½½æ•°æ®
    print("ğŸ“¥ Loading dataset...")
    await orchestrator.load_data(tables_file)
    
    # åŠ è½½ground truth
    print("ğŸ“¥ Loading ground truth...")
    ground_truth_data = await load_ground_truth(ground_truth_file)
    
    if not ground_truth_data:
        logger.error("No ground truth data found")
        return
    
    print(f"âœ… Loaded {len(ground_truth_data)} ground truth queries")
    
    # åˆ›å»ºæŸ¥è¯¢ä»»åŠ¡
    query_tasks = []
    
    # åˆ†åˆ«ç»Ÿè®¡JOINå’ŒUNION
    join_count = 0
    union_count = 0
    
    for i, gt_item in enumerate(ground_truth_data[:max_queries]):
        task_type = gt_item.get('type', 'join')
        
        if task_type == 'join':
            query_id = f"join_{join_count}"
            join_count += 1
        else:
            query_id = f"union_{union_count}"
            union_count += 1
        
        task = QueryTask(
            query_id=query_id,
            query_table=gt_item['query_table'],
            task_type=task_type,
            ground_truth=gt_item['ground_truth']
        )
        query_tasks.append(task)
    
    print(f"\nğŸ“‹ Created {len(query_tasks)} query tasks:")
    print(f"   - JOIN queries: {join_count}")
    print(f"   - UNION queries: {union_count}")
    
    # è¿è¡Œæµ‹è¯•
    print(f"\nğŸƒ Running multi-agent processing with {parallel_workers} parallel workers...")
    print("   This may take a few minutes...\n")
    
    start_time = time.time()
    
    # æ‰¹é‡å¤„ç†
    results = await orchestrator.process_batch(query_tasks, parallel_workers=parallel_workers)
    
    total_time = time.time() - start_time
    
    # è·å–ç³»ç»ŸæŒ‡æ ‡
    metrics = orchestrator.get_metrics()
    agent_stats = orchestrator.get_agent_stats()
    
    # è¾“å‡ºç»“æœ
    print("\n" + "="*70)
    print("ğŸ“Š EVALUATION RESULTS")
    print("="*70)
    
    print(f"\nâ±ï¸  Performance Metrics:")
    print(f"   Total Time: {total_time:.2f} seconds")
    print(f"   Queries Processed: {metrics['total_queries']}")
    print(f"   Successful: {metrics['successful_queries']}")
    print(f"   Failed: {metrics['failed_queries']}")
    print(f"   Success Rate: {metrics['successful_queries']/max(metrics['total_queries'], 1)*100:.1f}%")
    print(f"   Avg Response Time: {metrics['avg_response_time']:.3f}s")
    print(f"   Throughput: {metrics['total_queries']/max(total_time, 0.001):.2f} QPS")
    
    print(f"\nğŸ¯ Accuracy Metrics:")
    print(f"   Precision: {metrics['precision']:.3f}")
    print(f"   Recall: {metrics['recall']:.3f}")
    print(f"   F1-Score: {metrics['f1_score']:.3f}")
    print(f"   MRR: {metrics['mrr']:.3f}")
    
    if metrics['hit_at_k']:
        print(f"\nğŸ“ˆ Hit@K Metrics:")
        for k in sorted(metrics['hit_at_k'].keys()):
            print(f"   Hit@{k}: {metrics['hit_at_k'][k]:.3f}")
    
    print(f"\nğŸ¤– Agent Performance:")
    for agent_name, stats in agent_stats.items():
        if stats['processed'] > 0:
            success_rate = stats['success'] / stats['processed'] * 100
            print(f"   {agent_name}:")
            print(f"      Processed: {stats['processed']}")
            print(f"      Success Rate: {success_rate:.1f}%")
            print(f"      Avg Time: {stats['avg_time']:.3f}s")
    
    # åˆ†æJOINå’ŒUNIONåˆ†åˆ«çš„æ€§èƒ½
    join_results = {k: v for k, v in results.items() if 'join' in k}
    union_results = {k: v for k, v in results.items() if 'union' in k}
    
    if join_results:
        print(f"\nğŸ”— JOIN Task Performance:")
        join_success = sum(1 for v in join_results.values() if v)
        print(f"   Total: {len(join_results)}")
        print(f"   Success: {join_success}")
        print(f"   Success Rate: {join_success/len(join_results)*100:.1f}%")
    
    if union_results:
        print(f"\nğŸ”€ UNION Task Performance:")
        union_success = sum(1 for v in union_results.values() if v)
        print(f"   Total: {len(union_results)}")
        print(f"   Success: {union_success}")
        print(f"   Success Rate: {union_success/len(union_results)*100:.1f}%")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    timestamp = int(time.time())
    output_dir = Path('experiment_results/multi_agent')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = output_dir / f"test_{dataset}_{max_queries}queries_{timestamp}.json"
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    save_data = {
        'config': {
            'dataset': dataset,
            'max_queries': max_queries,
            'parallel_workers': parallel_workers,
            'total_time': total_time
        },
        'metrics': metrics,
        'agent_stats': agent_stats,
        'task_breakdown': {
            'join': {
                'total': len(join_results),
                'success': sum(1 for v in join_results.values() if v)
            },
            'union': {
                'total': len(union_results),
                'success': sum(1 for v in union_results.values() if v)
            }
        },
        'sample_results': {}
    }
    
    # æ·»åŠ ä¸€äº›æ ·æœ¬ç»“æœ
    for query_id in list(results.keys())[:5]:
        if results[query_id]:
            save_data['sample_results'][query_id] = [
                {
                    'matched_table': r.matched_table,
                    'score': r.score,
                    'agent_used': r.agent_used
                }
                for r in results[query_id][:3]
            ]
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(output_file, 'w') as f:
        json.dump(save_data, f, indent=2, default=str)
    
    print(f"\nğŸ’¾ Detailed results saved to: {output_file}")
    print("\n" + "="*70)
    print("âœ… TEST COMPLETED SUCCESSFULLY!")
    print("="*70 + "\n")
    
    return metrics

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Test Real Multi-Agent System')
    parser.add_argument('--dataset', choices=['subset', 'complete'], 
                       default='subset', help='Dataset to use')
    parser.add_argument('--queries', type=int, default=100, 
                       help='Maximum number of queries to test')
    parser.add_argument('--workers', type=int, default=4,
                       help='Number of parallel workers')
    
    args = parser.parse_args()
    
    # è¿è¡Œæµ‹è¯•
    await test_multi_agent_system(
        dataset=args.dataset,
        max_queries=args.queries,
        parallel_workers=args.workers
    )

if __name__ == "__main__":
    asyncio.run(main())