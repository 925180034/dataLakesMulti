"""
Enhanced Adaptive Optimizer with Intra-Batch Dynamic Optimization
æ‰¹æ¬¡å†…åŠ¨æ€ä¼˜åŒ–å™¨ - æ ¹æ®å®æ—¶åé¦ˆåŠ¨æ€è°ƒæ•´å‚æ•°
"""
import logging
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
import numpy as np
from collections import deque

logger = logging.getLogger(__name__)


@dataclass
class DynamicOptimizationState:
    """åŠ¨æ€ä¼˜åŒ–çŠ¶æ€ï¼Œè·Ÿè¸ªå®æ—¶æ€§èƒ½"""
    # æ€§èƒ½æŒ‡æ ‡å†å²ï¼ˆä½¿ç”¨æ»‘åŠ¨çª—å£ï¼‰
    precision_history: deque = field(default_factory=lambda: deque(maxlen=10))
    recall_history: deque = field(default_factory=lambda: deque(maxlen=10))
    f1_history: deque = field(default_factory=lambda: deque(maxlen=10))
    query_time_history: deque = field(default_factory=lambda: deque(maxlen=10))
    
    # å½“å‰å‚æ•°
    llm_confidence_threshold: float = 0.3
    aggregator_min_score: float = 0.1
    aggregator_max_results: int = 100
    vector_top_k: int = 200
    
    # å‚æ•°è°ƒæ•´å†å²
    threshold_adjustments: List[float] = field(default_factory=list)
    
    # ç»Ÿè®¡ä¿¡æ¯
    queries_processed: int = 0
    last_adjustment_query: int = 0
    total_adjustments: int = 0


class IntraBatchOptimizer:
    """æ‰¹æ¬¡å†…åŠ¨æ€ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # ä¸ºJOINå’ŒUNIONç»´æŠ¤ç‹¬ç«‹çš„ä¼˜åŒ–çŠ¶æ€
        self.states = {
            'join': DynamicOptimizationState(),
            'union': DynamicOptimizationState()
        }
        
        # ä¼˜åŒ–ç­–ç•¥å‚æ•°
        self.adjustment_interval = 5  # æ¯5ä¸ªæŸ¥è¯¢è¯„ä¼°ä¸€æ¬¡
        self.min_samples = 3  # æœ€å°‘éœ€è¦3ä¸ªæ ·æœ¬æ‰å¼€å§‹è°ƒæ•´
        
        # â­ æ›´æ–°ï¼šåŸºäºå®é™…æµ‹è¯•ç»“æœçš„ç°å®ç›®æ ‡
        # JOINæœ€é«˜F1åªèƒ½åˆ°0.12ï¼ŒUNIONæœ€é«˜F1åªèƒ½åˆ°0.31
        self.targets = {
            'join': {'precision': 0.20, 'recall': 0.25, 'f1': 0.20},   # æ›´ç°å®çš„JOINç›®æ ‡
            'union': {'precision': 0.35, 'recall': 0.30, 'f1': 0.32}   # æ›´ç°å®çš„UNIONç›®æ ‡
        }
        
        # è°ƒæ•´æ­¥é•¿
        self.adjustment_steps = {
            'small': 0.02,   # å¾®è°ƒ
            'medium': 0.05,  # ä¸­ç­‰è°ƒæ•´
            'large': 0.10    # å¤§å¹…è°ƒæ•´
        }
        
        # â­ æ–°å¢ï¼šä»»åŠ¡ç‰¹å®šçš„ä¼˜åŒ–ç‰¹æ€§
        self.task_features = {
            'join': {
                'foreign_key_detection': True,
                'relationship_analysis': True,
                'ignore_table_name': True,
                'boost_factors': {
                    'foreign_key_match': 1.5,
                    'semantic_relationship': 1.3,
                    'llm_high_confidence': 1.4
                }
            },
            'union': {
                'prefix_matching': True,
                'pattern_recognition': True,
                'same_source_detection': True,
                'boost_factors': {
                    'same_prefix': 2.0,
                    'same_pattern': 1.6,
                    'exact_name_match': 1.8
                }
            }
        }
        
    def initialize_batch(self, task_type: str, data_size: int):
        """åˆå§‹åŒ–æ‰¹æ¬¡ï¼Œè®¾ç½®åˆå§‹å‚æ•°"""
        state = self.states[task_type]
        
        # â­ æ›´æ–°ï¼šåŸºäºå®éªŒç»“æœä¼˜åŒ– - L3ä¸åº”è¯¥è¿‡æ»¤è€Œåº”è¯¥é‡æ’åº
        if task_type == 'join':
            # JOIN: å…³ç³»æ¨ç†ä¼˜åŒ– - æä½é˜ˆå€¼ï¼Œè®©LLMé‡æ’åºè€Œéè¿‡æ»¤
            state.llm_confidence_threshold = 0.10  # é™ä½åˆ°0.10ï¼Œé¿å…è¿‡åº¦è¿‡æ»¤
            state.aggregator_min_score = 0.01      # æä½é˜ˆå€¼ï¼Œæœ€å¤§åŒ–å¬å›
            state.aggregator_max_results = 500     # å¢åŠ å€™é€‰æ•°é‡
            state.vector_top_k = 600               # å¢åŠ å‘é‡æœç´¢å€™é€‰
        else:  # union
            # UNION: æ¨¡å¼åŒ¹é…ä¼˜åŒ– - é€‚ä¸­é˜ˆå€¼å¹³è¡¡ç²¾åº¦å’Œå¬å›
            state.llm_confidence_threshold = 0.15  # é™ä½åˆ°0.15ï¼Œæé«˜å¬å›ç‡
            state.aggregator_min_score = 0.03      # é™ä½é˜ˆå€¼
            state.aggregator_max_results = 200     # å¢åŠ å€™é€‰æ•°é‡
            state.vector_top_k = 350                # å¢åŠ å‘é‡æœç´¢å€™é€‰
            
        # é‡ç½®ç»Ÿè®¡
        state.queries_processed = 0
        state.last_adjustment_query = 0
        state.total_adjustments = 0
        
        self.logger.info(f"Initialized {task_type.upper()} batch with dynamic optimization")
        self.logger.info(f"  Initial threshold: {state.llm_confidence_threshold}")
        self.logger.info(f"  Initial candidates: {state.aggregator_max_results}")
        
    def should_adjust(self, task_type: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒæ•´å‚æ•°"""
        state = self.states[task_type]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬
        if len(state.precision_history) < self.min_samples:
            return False
            
        # æ£€æŸ¥æ˜¯å¦åˆ°äº†è°ƒæ•´é—´éš”
        queries_since_last = state.queries_processed - state.last_adjustment_query
        if queries_since_last < self.adjustment_interval:
            return False
            
        return True
        
    def calculate_adjustment(self, task_type: str) -> Dict[str, float]:
        """è®¡ç®—å‚æ•°è°ƒæ•´é‡"""
        state = self.states[task_type]
        targets = self.targets[task_type]
        
        # è®¡ç®—æœ€è¿‘çš„å¹³å‡æ€§èƒ½
        avg_precision = np.mean(state.precision_history)
        avg_recall = np.mean(state.recall_history)
        avg_f1 = np.mean(state.f1_history)
        
        adjustments = {}
        
        # åˆ†ææ€§èƒ½å·®è·
        precision_gap = targets['precision'] - avg_precision
        recall_gap = targets['recall'] - avg_recall
        f1_gap = targets['f1'] - avg_f1
        
        self.logger.info(f"Performance gaps for {task_type}:")
        self.logger.info(f"  Precision: {avg_precision:.3f} (gap: {precision_gap:+.3f})")
        self.logger.info(f"  Recall: {avg_recall:.3f} (gap: {recall_gap:+.3f})")
        self.logger.info(f"  F1: {avg_f1:.3f} (gap: {f1_gap:+.3f})")
        
        # å†³å®šè°ƒæ•´ç­–ç•¥
        if recall_gap > 0.15:
            # å¬å›ç‡å¤ªä½ï¼Œéœ€è¦å¤§å¹…é™ä½é˜ˆå€¼
            adjustments['threshold_delta'] = -self.adjustment_steps['large']
            adjustments['candidates_delta'] = 100
            adjustments['top_k_delta'] = 150
            self.logger.info("  Strategy: AGGRESSIVE - Boost recall significantly")
            
        elif recall_gap > 0.05:
            # å¬å›ç‡åä½ï¼Œä¸­ç­‰è°ƒæ•´
            adjustments['threshold_delta'] = -self.adjustment_steps['medium']
            adjustments['candidates_delta'] = 50
            adjustments['top_k_delta'] = 75
            self.logger.info("  Strategy: MODERATE - Improve recall")
            
        elif precision_gap > 0.15 and recall_gap < -0.05:
            # ç²¾åº¦å¤ªä½ä½†å¬å›ç‡è¿‡é«˜ï¼Œéœ€è¦æé«˜é˜ˆå€¼
            adjustments['threshold_delta'] = self.adjustment_steps['medium']
            adjustments['candidates_delta'] = -50
            adjustments['top_k_delta'] = -75
            self.logger.info("  Strategy: TIGHTEN - Improve precision")
            
        elif abs(f1_gap) > 0.02:
            # F1éœ€è¦å¾®è°ƒ
            if f1_gap > 0:
                # F1åä½ï¼Œç¨å¾®é™ä½é˜ˆå€¼
                adjustments['threshold_delta'] = -self.adjustment_steps['small']
                adjustments['candidates_delta'] = 25
                adjustments['top_k_delta'] = 30
            else:
                # F1åé«˜ï¼ˆå°‘è§ï¼‰ï¼Œç¨å¾®æé«˜é˜ˆå€¼
                adjustments['threshold_delta'] = self.adjustment_steps['small']
                adjustments['candidates_delta'] = -10
                adjustments['top_k_delta'] = -15
            self.logger.info("  Strategy: FINE-TUNE - Optimize F1")
        else:
            # æ€§èƒ½æ¥è¿‘ç›®æ ‡ï¼Œä¸è°ƒæ•´
            self.logger.info("  Strategy: MAINTAIN - Performance near target")
            
        return adjustments
        
    def apply_adjustments(self, task_type: str, adjustments: Dict[str, float]):
        """åº”ç”¨å‚æ•°è°ƒæ•´"""
        if not adjustments:
            return
            
        state = self.states[task_type]
        
        # åº”ç”¨é˜ˆå€¼è°ƒæ•´
        if 'threshold_delta' in adjustments:
            old_threshold = state.llm_confidence_threshold
            state.llm_confidence_threshold += adjustments['threshold_delta']
            # é™åˆ¶èŒƒå›´
            state.llm_confidence_threshold = max(0.05, min(0.5, state.llm_confidence_threshold))
            
            # åŒæ­¥è°ƒæ•´æœ€å°åˆ†æ•°
            state.aggregator_min_score = state.llm_confidence_threshold * 0.2
            
            self.logger.info(f"  Adjusted threshold: {old_threshold:.3f} â†’ {state.llm_confidence_threshold:.3f}")
            
        # åº”ç”¨å€™é€‰æ•°è°ƒæ•´
        if 'candidates_delta' in adjustments:
            old_candidates = state.aggregator_max_results
            state.aggregator_max_results += int(adjustments['candidates_delta'])
            # é™åˆ¶èŒƒå›´
            state.aggregator_max_results = max(50, min(800, state.aggregator_max_results))
            
            self.logger.info(f"  Adjusted candidates: {old_candidates} â†’ {state.aggregator_max_results}")
            
        # åº”ç”¨TopKè°ƒæ•´
        if 'top_k_delta' in adjustments:
            old_top_k = state.vector_top_k
            state.vector_top_k += int(adjustments['top_k_delta'])
            # é™åˆ¶èŒƒå›´
            state.vector_top_k = max(100, min(1000, state.vector_top_k))
            
            self.logger.info(f"  Adjusted top_k: {old_top_k} â†’ {state.vector_top_k}")
            
        # æ›´æ–°ç»Ÿè®¡
        state.last_adjustment_query = state.queries_processed
        state.total_adjustments += 1
        state.threshold_adjustments.append(state.llm_confidence_threshold)
        
    def update_performance(self, task_type: str, precision: float, recall: float, f1: float, query_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        state = self.states[task_type]
        
        # æ·»åŠ åˆ°å†å²
        state.precision_history.append(precision)
        state.recall_history.append(recall)
        state.f1_history.append(f1)
        state.query_time_history.append(query_time)
        state.queries_processed += 1
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´
        if self.should_adjust(task_type):
            self.logger.info(f"\nğŸ”§ Dynamic adjustment triggered for {task_type} at query {state.queries_processed}")
            adjustments = self.calculate_adjustment(task_type)
            self.apply_adjustments(task_type, adjustments)
            
    def get_current_params(self, task_type: str) -> Dict[str, any]:
        """è·å–å½“å‰å‚æ•°"""
        state = self.states[task_type]
        return {
            'llm_confidence_threshold': state.llm_confidence_threshold,
            'aggregator_min_score': state.aggregator_min_score,
            'aggregator_max_results': state.aggregator_max_results,
            'vector_top_k': state.vector_top_k,
            'queries_processed': state.queries_processed,
            'total_adjustments': state.total_adjustments,
            'task_features': self.task_features.get(task_type, {})  # åŒ…å«ä»»åŠ¡ç‰¹å®šç‰¹æ€§
        }
    
    def apply_boost_factor(self, task_type: str, score: float, table1: str, table2: str) -> float:
        """åº”ç”¨ä»»åŠ¡ç‰¹å®šçš„boost factor
        
        Args:
            task_type: 'join' æˆ– 'union'
            score: åŸå§‹åˆ†æ•°
            table1: æŸ¥è¯¢è¡¨å
            table2: å€™é€‰è¡¨å
            
        Returns:
            è°ƒæ•´åçš„åˆ†æ•°
        """
        boost_factors = self.task_features[task_type].get('boost_factors', {})
        
        if task_type == 'join':
            # JOINç‰¹å®šboost
            if self._has_foreign_key_pattern(table1, table2):
                score *= boost_factors.get('foreign_key_match', 1.5)
                
        elif task_type == 'union':
            # UNIONç‰¹å®šboost
            if self._has_same_prefix(table1, table2):
                score *= boost_factors.get('same_prefix', 2.0)
            elif self._has_same_pattern(table1, table2):
                score *= boost_factors.get('same_pattern', 1.6)
        
        return score
    
    def _has_foreign_key_pattern(self, table1: str, table2: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ½œåœ¨çš„å¤–é”®å…³ç³»"""
        fk_patterns = ['_id', '_key', '_code', '_no']
        t1_lower = table1.lower()
        t2_lower = table2.lower()
        
        for pattern in fk_patterns:
            if pattern in t1_lower or pattern in t2_lower:
                base1 = t1_lower.replace(pattern, '')
                base2 = t2_lower.replace(pattern, '')
                if base1 in t2_lower or base2 in t1_lower:
                    return True
        return False
    
    def _has_same_prefix(self, table1: str, table2: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒå‰ç¼€"""
        def get_prefix(name):
            if '__' in name:
                return name.split('__')[0]
            elif '_' in name:
                parts = name.split('_')
                if len(parts) > 1:
                    return parts[0]
            return name[:min(10, len(name))]
        
        return get_prefix(table1) == get_prefix(table2)
    
    def _has_same_pattern(self, table1: str, table2: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒæ¨¡å¼"""
        import re
        
        # æå–æ¨¡å¼ï¼ˆå°†æ•°å­—æ›¿æ¢ä¸ºå ä½ç¬¦ï¼‰
        pattern1 = re.sub(r'\d+', '#', table1)
        pattern2 = re.sub(r'\d+', '#', table2)
        
        return pattern1 == pattern2
        
    def get_optimization_summary(self, task_type: str) -> str:
        """è·å–ä¼˜åŒ–æ€»ç»“"""
        state = self.states[task_type]
        
        if len(state.precision_history) == 0:
            return f"No data yet for {task_type}"
            
        summary = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  {task_type.upper()} Dynamic Optimization Summary              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Queries Processed: {state.queries_processed:>5}                              â•‘
â•‘  Total Adjustments: {state.total_adjustments:>5}                              â•‘
â•‘                                                               â•‘
â•‘  Current Parameters:                                          â•‘
â•‘    â€¢ Confidence Threshold: {state.llm_confidence_threshold:>6.3f}                     â•‘
â•‘    â€¢ Min Score: {state.aggregator_min_score:>6.3f}                               â•‘
â•‘    â€¢ Max Results: {state.aggregator_max_results:>4}                              â•‘
â•‘    â€¢ Vector TopK: {state.vector_top_k:>4}                              â•‘
â•‘                                                               â•‘
â•‘  Recent Performance (last {len(state.precision_history)} queries):                      â•‘
â•‘    â€¢ Avg Precision: {np.mean(state.precision_history):>6.3f}                       â•‘
â•‘    â€¢ Avg Recall: {np.mean(state.recall_history):>6.3f}                          â•‘
â•‘    â€¢ Avg F1: {np.mean(state.f1_history):>6.3f}                              â•‘
â•‘    â€¢ Avg Query Time: {np.mean(state.query_time_history):>5.2f}s                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        return summary


# ä½¿ç”¨ç¤ºä¾‹
def demo_intra_batch_optimization():
    """æ¼”ç¤ºæ‰¹æ¬¡å†…åŠ¨æ€ä¼˜åŒ–"""
    optimizer = IntraBatchOptimizer()
    
    # åˆå§‹åŒ–JOINæ‰¹æ¬¡
    optimizer.initialize_batch('join', 100)
    
    # æ¨¡æ‹ŸæŸ¥è¯¢å¤„ç†
    for i in range(20):
        # æ¨¡æ‹Ÿæ€§èƒ½ï¼ˆéšç€ä¼˜åŒ–é€æ¸æ”¹å–„ï¼‰
        base_precision = 0.15 + i * 0.01
        base_recall = 0.20 + i * 0.015
        precision = min(0.4, base_precision + np.random.uniform(-0.05, 0.05))
        recall = min(0.5, base_recall + np.random.uniform(-0.05, 0.05))
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        query_time = 2.5 + np.random.uniform(-0.5, 0.5)
        
        # æ›´æ–°æ€§èƒ½
        optimizer.update_performance('join', precision, recall, f1, query_time)
        
        # è·å–å½“å‰å‚æ•°
        params = optimizer.get_current_params('join')
        
        if i % 5 == 4:  # æ¯5ä¸ªæŸ¥è¯¢è¾“å‡ºä¸€æ¬¡
            print(f"\nQuery {i+1} - Current params:")
            print(f"  Threshold: {params['llm_confidence_threshold']:.3f}")
            print(f"  Candidates: {params['aggregator_max_results']}")
            print(f"  Adjustments: {params['total_adjustments']}")
    
    # è¾“å‡ºæ€»ç»“
    print(optimizer.get_optimization_summary('join'))


if __name__ == "__main__":
    demo_intra_batch_optimization()