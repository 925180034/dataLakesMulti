#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆLSH Ensembleæµ‹è¯•è„šæœ¬
æµ‹è¯•LSH Ensembleåœ¨è½¬æ¢åçš„æ•°æ®ä¸Šçš„åŸºæœ¬åŠŸèƒ½

ä½¿ç”¨ä½ çš„ä¸‰ä¸ªæ•°æ®é›†:
- NLCTables 
- WebTable
- OpenData
"""
import os
import sys
import json
import pandas as pd
from pathlib import Path
import time
import logging
import pickle
import farmhash

# ç¡®ä¿ä½¿ç”¨æœ¬åœ°datasketchç‰ˆæœ¬
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from datasketch import MinHashLSHEnsemble, MinHash

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def _hash_32(d):
    """ä½¿ç”¨farmhashçš„32ä½å“ˆå¸Œå‡½æ•°"""
    return farmhash.hash32(d)

class LSHEnsembleSimpleTest:
    """ç®€åŒ–ç‰ˆLSH Ensembleæµ‹è¯•å™¨"""
    
    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        
    def create_minhash(self, table_df: pd.DataFrame, num_perm: int = 128) -> MinHash:
        """ä¸ºè¡¨æ ¼åˆ›å»ºMinHash"""
        mh = MinHash(num_perm=num_perm, hashfunc=_hash_32)
        
        # å°†æ‰€æœ‰åˆ—çš„æ‰€æœ‰å€¼æ·»åŠ åˆ°MinHashä¸­
        for col in table_df.columns:
            for value in table_df[col].dropna().astype(str):
                if value.strip():  # å¿½ç•¥ç©ºå­—ç¬¦ä¸²
                    mh.update(value.strip().lower().encode('utf-8'))
        
        return mh
    
    def build_lsh_ensemble(self, dataset: str, task: str, threshold: float = 0.1, num_perm: int = 128, num_part: int = 8, m: int = 4) -> MinHashLSHEnsemble:
        """ä¸ºæŒ‡å®šæ•°æ®é›†æ„å»ºLSH Ensembleç´¢å¼•"""
        dataset_path = self.data_dir / dataset / task
        
        if not dataset_path.exists():
            logging.error(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
            return None
        
        logging.info(f"æ„å»ºLSH Ensembleç´¢å¼•: {dataset}-{task}")
        
        csv_files = list(dataset_path.glob("*.csv"))
        
        if len(csv_files) == 0:
            logging.warning(f"æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶: {dataset_path}")
            return None
        
        logging.info(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
        
        # ç¬¬ä¸€éï¼šè®¡ç®—æ‰€æœ‰åˆ—çš„å¤§å°
        logging.info("ç¬¬ä¸€éï¼šè®¡ç®—åˆ—å¤§å°...")
        sizes = []
        keys = []
        
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file, dtype=str).dropna()
                if len(df) == 0:
                    continue
                    
                # ä¸ºæ¯ä¸€åˆ—åˆ›å»ºé”®å€¼å’Œè®¡ç®—å¤§å°
                for column in df.columns:
                    vals = df[column].dropna().astype(str).tolist()
                    vals = list(set(vals))  # å»é‡
                    
                    key = f"{csv_file.name}.{column}"
                    keys.append(key)
                    sizes.append(len(vals))
                    
            except Exception as e:
                logging.error(f"å¤„ç†æ–‡ä»¶ {csv_file.name} æ—¶å‡ºé”™: {e}")
                continue
        
        if len(sizes) == 0:
            logging.error("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®")
            return None
        
        logging.info(f"æ€»å…± {len(sizes)} åˆ—")
        
        # åˆ›å»ºLSH Ensembleå¹¶åˆ†åŒº
        lsh = MinHashLSHEnsemble(
            threshold=threshold, 
            num_perm=num_perm,
            num_part=num_part, 
            m=m
        )
        
        # åˆ†åŒºè®¡æ•°
        lsh.count_partition(sizes)
        
        # ç¬¬äºŒéï¼šåˆ›å»ºMinHashå¹¶ç´¢å¼•
        logging.info("ç¬¬äºŒéï¼šåˆ›å»ºMinHashå¹¶æ„å»ºç´¢å¼•...")
        key_idx = 0
        
        for csv_file in csv_files:
            try:
                df = pd.read_csv(csv_file, dtype=str).dropna()
                if len(df) == 0:
                    continue
                    
                for column in df.columns:
                    vals = df[column].dropna().astype(str).tolist()
                    vals = list(set(vals))  # å»é‡
                    
                    # åˆ›å»ºMinHash
                    mh = MinHash(num_perm, hashfunc=_hash_32)
                    for val in vals:
                        if val.strip():
                            mh.update(val.strip().lower().encode('utf-8'))
                    
                    # æ·»åŠ åˆ°ç´¢å¼•
                    key = keys[key_idx]
                    size = sizes[key_idx]
                    lsh.index((key, mh, size))
                    
                    key_idx += 1
                    
                    if key_idx % 100 == 0:
                        logging.info(f"å·²ç´¢å¼• {key_idx} åˆ—")
                        
            except Exception as e:
                logging.error(f"ç´¢å¼•æ–‡ä»¶ {csv_file.name} æ—¶å‡ºé”™: {e}")
                continue
        
        logging.info(f"æˆåŠŸæ„å»ºLSH Ensembleç´¢å¼•ï¼ŒåŒ…å« {len(keys)} åˆ—")
        return lsh
    
    def query_similar_tables(self, query_table_name: str, query_column: str, 
                           lsh: MinHashLSHEnsemble, dataset: str, task: str,
                           threshold: float = 0.1, top_k: int = 10) -> list:
        """æŸ¥è¯¢ç›¸ä¼¼è¡¨æ ¼åˆ—"""
        dataset_path = self.data_dir / dataset / task
        csv_file = dataset_path / query_table_name
        
        if not csv_file.exists():
            logging.error(f"æŸ¥è¯¢è¡¨æ ¼æ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
            return []
        
        try:
            df = pd.read_csv(csv_file, dtype=str).dropna()
            if query_column not in df.columns:
                logging.error(f"æŸ¥è¯¢åˆ—ä¸å­˜åœ¨: {query_column}")
                return []
            
            # ä¸ºæŸ¥è¯¢åˆ—åˆ›å»ºMinHash
            vals = df[query_column].dropna().astype(str).tolist()
            vals = list(set(vals))  # å»é‡
            
            mh = MinHash(128, hashfunc=_hash_32)  # ä½¿ç”¨é»˜è®¤å‚æ•°
            for val in vals:
                if val.strip():
                    mh.update(val.strip().lower().encode('utf-8'))
            
            # æŸ¥è¯¢ç›¸ä¼¼åˆ—
            results = list(lsh.query(mh, len(vals)))
            
            # è§£æç»“æœå¹¶æ’åº
            similar_columns = []
            for result_key in results:
                if result_key == f"{query_table_name}.{query_column}":
                    continue  # è·³è¿‡è‡ªå·±
                
                parts = result_key.split('.')
                if len(parts) >= 2:
                    table_name = parts[0]
                    column_name = '.'.join(parts[1:])  # å¤„ç†åˆ—åä¸­å¯èƒ½åŒ…å«ç‚¹çš„æƒ…å†µ
                    
                    similar_columns.append({
                        'table_name': table_name,
                        'column_name': column_name,
                        'key': result_key
                    })
            
            return similar_columns[:top_k]
            
        except Exception as e:
            logging.error(f"æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
            return []
    
    def load_queries(self, dataset: str, task: str) -> list:
        """åŠ è½½æŸ¥è¯¢æ•°æ®"""
        queries_file = self.data_dir / dataset / task / "queries.json"
        
        if not queries_file.exists():
            logging.warning(f"æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {queries_file}")
            return []
        
        with open(queries_file, 'r', encoding='utf-8') as f:
            queries = json.load(f)
        
        return queries
    
    def evaluate_performance(self, dataset: str, task: str, max_queries: int = 5):
        """è¯„ä¼°æ€§èƒ½"""
        logging.info(f"\n=== è¯„ä¼° {dataset}-{task} ===")
        
        # æ„å»ºç´¢å¼•
        start_time = time.time()
        lsh = self.build_lsh_ensemble(dataset, task, threshold=0.1, num_perm=128, num_part=8, m=4)
        index_time = time.time() - start_time
        
        if lsh is None:
            logging.error("LSH Ensembleç´¢å¼•æ„å»ºå¤±è´¥")
            return
        
        logging.info(f"ç´¢å¼•æ„å»ºè€—æ—¶: {index_time:.2f}ç§’")
        
        # åŠ è½½æŸ¥è¯¢
        queries = self.load_queries(dataset, task)
        
        if len(queries) == 0:
            logging.warning("æ²¡æœ‰æŸ¥è¯¢æ•°æ®ï¼Œä½¿ç”¨éšæœºè¡¨æ ¼å’Œåˆ—è¿›è¡Œæµ‹è¯•")
            # ä½¿ç”¨ä¸€äº›ç¤ºä¾‹æŸ¥è¯¢
            dataset_path = self.data_dir / dataset / task
            csv_files = list(dataset_path.glob("*.csv"))[:max_queries]
            
            queries = []
            for csv_file in csv_files:
                try:
                    df = pd.read_csv(csv_file, dtype=str).dropna()
                    if len(df.columns) > 0:
                        queries.append({
                            "query_table": csv_file.name,
                            "query_column": df.columns[0]  # ä½¿ç”¨ç¬¬ä¸€åˆ—
                        })
                except:
                    continue
        
        logging.info(f"å¤„ç† {min(len(queries), max_queries)} ä¸ªæŸ¥è¯¢")
        
        # æ‰§è¡ŒæŸ¥è¯¢
        results = []
        total_query_time = 0
        
        for i, query in enumerate(queries[:max_queries]):
            query_table = query.get('query_table', query.get('seed_table'))
            
            if not query_table:
                logging.warning(f"æŸ¥è¯¢ {i} ç¼ºå°‘è¡¨æ ¼åç§°")
                continue
            
            # ç¡®ä¿æŸ¥è¯¢è¡¨æ ¼åç§°æ ¼å¼æ­£ç¡®
            if not query_table.endswith('.csv'):
                query_table += '.csv'
            
            # è·å–æŸ¥è¯¢åˆ—ï¼ˆå¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—ï¼‰
            query_column = query.get('query_column', query.get('column'))
            if not query_column:
                # å°è¯•è¯»å–è¡¨æ ¼è·å–ç¬¬ä¸€åˆ—
                try:
                    csv_file = self.data_dir / dataset / task / query_table
                    df = pd.read_csv(csv_file, dtype=str).dropna()
                    if len(df.columns) > 0:
                        query_column = df.columns[0]
                    else:
                        continue
                except:
                    continue
            
            logging.info(f"æŸ¥è¯¢ {i+1}: {query_table}.{query_column}")
            
            # æ‰§è¡ŒæŸ¥è¯¢
            start_time = time.time()
            similar_columns = self.query_similar_tables(
                query_table, query_column, lsh, dataset, task, threshold=0.1, top_k=10
            )
            query_time = time.time() - start_time
            total_query_time += query_time
            
            results.append({
                'query_table': query_table,
                'query_column': query_column,
                'similar_columns': similar_columns,
                'query_time': query_time
            })
            
            # æ˜¾ç¤ºç»“æœ
            if similar_columns:
                logging.info(f"  æ‰¾åˆ° {len(similar_columns)} ä¸ªç›¸ä¼¼åˆ—:")
                for j, sim_col in enumerate(similar_columns[:3]):
                    logging.info(f"    {j+1}. {sim_col['table_name']}.{sim_col['column_name']}")
            else:
                logging.info("  æœªæ‰¾åˆ°ç›¸ä¼¼åˆ—")
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        if results:
            avg_query_time = total_query_time / len(results)
            logging.info(f"\næ€§èƒ½ç»Ÿè®¡:")
            logging.info(f"  å¹³å‡æŸ¥è¯¢æ—¶é—´: {avg_query_time:.3f}ç§’")
            logging.info(f"  æ€»æŸ¥è¯¢æ—¶é—´: {total_query_time:.2f}ç§’")
            logging.info(f"  ç´¢å¼•æ„å»ºæ—¶é—´: {index_time:.2f}ç§’")
            logging.info(f"  å¤„ç†çš„æŸ¥è¯¢æ•°: {len(results)}")
        
        return results

def main():
    # LSHæ•°æ®ç›®å½•
    data_dir = "/root/dataLakesMulti/baselines/data/lsh"
    
    tester = LSHEnsembleSimpleTest(data_dir)
    
    # æµ‹è¯•æ•°æ®é›†
    datasets = [
        ('nlctables', 'join'),
        ('webtable', 'join'), 
        ('opendata', 'join')
    ]
    
    print("ğŸ” LSH Ensemble Baseline æµ‹è¯•")
    print("=" * 50)
    
    all_results = {}
    
    for dataset, task in datasets:
        try:
            results = tester.evaluate_performance(dataset, task, max_queries=3)
            all_results[f"{dataset}-{task}"] = results
        except Exception as e:
            logging.error(f"æµ‹è¯• {dataset}-{task} æ—¶å‡ºé”™: {e}")
            continue
    
    print("\nğŸ“Š æ€»ä½“ç»“æœæ‘˜è¦:")
    for key, results in all_results.items():
        if results:
            avg_time = sum(r['query_time'] for r in results) / len(results)
            avg_results = sum(len(r['similar_columns']) for r in results) / len(results)
            print(f"  {key}: {len(results)}ä¸ªæŸ¥è¯¢, å¹³å‡{avg_time:.3f}s/æŸ¥è¯¢, å¹³å‡{avg_results:.1f}ä¸ªç»“æœ")
    
    print("\nâœ… LSH Ensembleæµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    main()