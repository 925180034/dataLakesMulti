#!/usr/bin/env python3
"""
å‡†å¤‡åŸå§‹NLCTablesæ•°æ®ç”¨äºbaselineè¯„ä¼°
ç¡®ä¿ä½¿ç”¨ä¸ä¸»ç³»ç»Ÿä¸€è‡´çš„querieså’Œground truth
"""

import json
import pandas as pd
import os
from pathlib import Path
import shutil

class OriginalDataPreparer:
    def __init__(self):
        # åŸå§‹æ•°æ®è·¯å¾„
        self.original_base = Path("/root/autodl-tmp/datalakes")
        # æå–åçš„æ•°æ®è·¯å¾„ï¼ˆæœ‰æ­£ç¡®çš„querieså’Œground truthï¼‰
        self.extracted_base = Path("/root/dataLakesMulti/examples")
        # Baselineè¾“å‡ºè·¯å¾„
        self.output_base = Path("/root/dataLakesMulti/baselines/data")
        
    def convert_nlctables_table_to_csv(self, table_json_path: Path) -> pd.DataFrame:
        """å°†NLCTablesåŸå§‹JSONæ ¼å¼è½¬æ¢ä¸ºCSV DataFrame"""
        try:
            with open(table_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # NLCTablesä½¿ç”¨'table_array'å­˜å‚¨è¡¨æ ¼æ•°æ®
            if 'table_array' in data and data['table_array']:
                # ç¬¬ä¸€è¡Œé€šå¸¸æ˜¯åˆ—å
                table_array = data['table_array']
                if len(table_array) > 0:
                    columns = table_array[0] if len(table_array) > 0 else []
                    rows = table_array[1:] if len(table_array) > 1 else []
                    
                    # æ¸…ç†åˆ—åï¼ˆå¤„ç†éå­—ç¬¦ä¸²æˆ–ç‰¹æ®Šå­—ç¬¦ï¼‰
                    clean_columns = []
                    for i, col in enumerate(columns):
                        if col is None or col == '':
                            clean_columns.append(f'column_{i}')
                        else:
                            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…ç†ç‰¹æ®Šå­—ç¬¦
                            col_str = str(col).strip()
                            # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
                            col_str = col_str.replace(' ', '_').replace(':', '_').replace('-', '_')
                            col_str = col_str.replace('(', '_').replace(')', '_').replace('.', '_')
                            clean_columns.append(col_str if col_str else f'column_{i}')
                    
                    # åˆ›å»ºDataFrame
                    if rows:
                        # ç¡®ä¿è¡Œæ•°æ®é•¿åº¦ä¸åˆ—æ•°åŒ¹é…
                        aligned_rows = []
                        for row in rows:
                            if len(row) < len(clean_columns):
                                # å¡«å……ç¼ºå¤±å€¼
                                row = list(row) + [None] * (len(clean_columns) - len(row))
                            elif len(row) > len(clean_columns):
                                # æˆªæ–­å¤šä½™å€¼
                                row = row[:len(clean_columns)]
                            aligned_rows.append(row)
                        
                        df = pd.DataFrame(aligned_rows, columns=clean_columns)
                    else:
                        # åªæœ‰åˆ—åï¼Œæ²¡æœ‰æ•°æ®
                        df = pd.DataFrame(columns=clean_columns)
                    return df
            elif 'data' in data and data['data']:
                # å¤‡ç”¨æ ¼å¼
                return pd.DataFrame(data['data'])
            else:
                # ç©ºè¡¨
                return pd.DataFrame()
        except Exception as e:
            # è¿”å›ç©ºDataFrameè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return pd.DataFrame()
    
    def prepare_nlctables_data(self, task='join'):
        """å‡†å¤‡NLCTablesæ•°æ®ï¼Œä½¿ç”¨åŸå§‹è¡¨æ ¼ä½†ä¸ä¸»ç³»ç»Ÿä¸€è‡´çš„querieså’Œground truth"""
        print(f"\nğŸ“Š å‡†å¤‡NLCTables {task.upper()} æ•°æ®...")
        
        # 1. åŠ è½½æå–åçš„querieså’Œground truthï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
        extracted_path = self.extracted_base / 'nlctables' / f'{task}_subset'
        queries_file = extracted_path / 'queries.json'
        gt_file = extracted_path / 'ground_truth.json'
        
        if not queries_file.exists() or not gt_file.exists():
            print(f"âŒ æ‰¾ä¸åˆ°æå–åçš„queriesæˆ–ground truth: {extracted_path}")
            return
        
        with open(queries_file, 'r') as f:
            queries = json.load(f)
        with open(gt_file, 'r') as f:
            ground_truth = json.load(f)
        
        print(f"âœ… åŠ è½½äº† {len(queries)} ä¸ªæŸ¥è¯¢å’Œ {len(ground_truth)} ä¸ªground truth")
        
        # 2. è½¬æ¢åŸå§‹æ•°æ®è¡¨æ ¼
        if task == 'join':
            original_dir = self.original_base / 'nlcTables' / 'nlcTables-J'
        else:
            original_dir = self.original_base / 'nlcTables' / 'nlcTables-U'
        
        datalake_dir = original_dir / 'datalake-test'
        query_dir = original_dir / 'query-test'
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = self.output_base / 'aurum_original' / 'nlctables' / task
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡è½¬æ¢
        converted_count = 0
        failed_count = 0
        
        # è½¬æ¢datalakeè¡¨æ ¼
        print(f"ğŸ”„ è½¬æ¢datalakeè¡¨æ ¼...")
        for json_file in datalake_dir.glob('*.json'):
            try:
                df = self.convert_nlctables_table_to_csv(json_file)
                if not df.empty:
                    csv_path = output_dir / f"{json_file.stem}.csv"
                    df.to_csv(csv_path, index=False)
                    converted_count += 1
                else:
                    # ç©ºè¡¨ä¹Ÿä¿å­˜ï¼ˆå¯èƒ½æ˜¯ç‰¹å¾ï¼‰
                    csv_path = output_dir / f"{json_file.stem}.csv"
                    with open(csv_path, 'w') as f:
                        f.write("")  # ç©ºCSV
                    converted_count += 1
            except Exception as e:
                failed_count += 1
                if failed_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                    print(f"  âš ï¸ è½¬æ¢å¤±è´¥ {json_file.name}: {e}")
        
        # è½¬æ¢queryè¡¨æ ¼
        print(f"ğŸ”„ è½¬æ¢queryè¡¨æ ¼...")
        for json_file in query_dir.glob('*.json'):
            try:
                df = self.convert_nlctables_table_to_csv(json_file)
                if not df.empty:
                    csv_path = output_dir / f"{json_file.stem}.csv"
                    df.to_csv(csv_path, index=False)
                    converted_count += 1
                else:
                    csv_path = output_dir / f"{json_file.stem}.csv"
                    with open(csv_path, 'w') as f:
                        f.write("")
                    converted_count += 1
            except Exception as e:
                failed_count += 1
                if failed_count <= 3:
                    print(f"  âš ï¸ è½¬æ¢å¤±è´¥ {json_file.name}: {e}")
        
        print(f"âœ… æˆåŠŸè½¬æ¢ {converted_count} ä¸ªè¡¨æ ¼ï¼Œå¤±è´¥ {failed_count} ä¸ª")
        
        # 3. ä¿å­˜querieså’Œground truthçš„æ˜ å°„ï¼ˆç”¨äºè¯„ä¼°ï¼‰
        mapping = {
            'queries': queries,
            'ground_truth': ground_truth,
            'total_tables': converted_count,
            'dataset': 'nlctables',
            'task': task
        }
        
        mapping_file = output_dir / 'evaluation_mapping.json'
        with open(mapping_file, 'w') as f:
            json.dump(mapping, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ æ•°æ®å·²å‡†å¤‡å®Œæˆ: {output_dir}")
        print(f"   - è¡¨æ ¼æ•°: {converted_count}")
        print(f"   - æŸ¥è¯¢æ•°: {len(queries)}")
        print(f"   - æ˜ å°„æ–‡ä»¶: {mapping_file}")
        
        return output_dir
    
    def prepare_all_datasets(self):
        """å‡†å¤‡æ‰€æœ‰æ•°æ®é›†"""
        results = {}
        
        # NLCTables
        for task in ['join', 'union']:
            nlc_dir = self.original_base / 'nlcTables' / f'nlcTables-{task[0].upper()}'
            if nlc_dir.exists():
                result_dir = self.prepare_nlctables_data(task)
                results[f'nlctables_{task}'] = result_dir
            else:
                print(f"âš ï¸ æ‰¾ä¸åˆ°NLCTables {task} æ•°æ®: {nlc_dir}")
        
        # TODO: WebTableå’ŒOpenDataçš„å‡†å¤‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        
        return results


def main():
    print("ğŸš€ å¼€å§‹å‡†å¤‡åŸå§‹æ•°æ®ç”¨äºbaselineè¯„ä¼°")
    print("="*80)
    
    preparer = OriginalDataPreparer()
    
    # å‡†å¤‡æ•°æ®
    results = preparer.prepare_all_datasets()
    
    print("\n" + "="*80)
    print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
    print("\nå‡†å¤‡å¥½çš„æ•°æ®é›†:")
    for dataset, path in results.items():
        if path:
            print(f"  - {dataset}: {path}")
    
    print("\nä¸‹ä¸€æ­¥:")
    print("  1. è¿è¡Œ run_unified_evaluation_original.py ä½¿ç”¨åŸå§‹æ•°æ®è¯„ä¼°")
    print("  2. ç»“æœå°†ä¸ä¸»ç³»ç»Ÿä½¿ç”¨ç›¸åŒçš„querieså’Œground truth")


if __name__ == "__main__":
    main()