#!/usr/bin/env python3
"""
éªŒè¯Aurumå’ŒLSH Ensembleæ˜¯çœŸå®å®ç°
ä½¿ç”¨çœŸå®æ•°æ®é›†æ‰§è¡Œå®Œæ•´æµ‹è¯•
"""

import sys
import json
import time
import pandas as pd
from pathlib import Path
import logging

# æ·»åŠ è·¯å¾„
sys.path.append('/root/dataLakesMulti/baselines/aurum')
sys.path.append('/root/dataLakesMulti/baselines/lsh')

from test_aurum_simple import AurumSimpleTest

# LSHéœ€è¦ç‰¹æ®Šå¯¼å…¥å¤„ç†
sys.path.insert(0, '/root/dataLakesMulti/baselines/lsh')
from datasketch.lshensemble import MinHashLSHEnsemble
from datasketch.minhash import MinHash
import farmhash

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

def _hash_32(d):
    return farmhash.hash32(d)

def test_aurum_on_real_data():
    """åœ¨çœŸå®WebTableæ•°æ®ä¸Šæµ‹è¯•Aurum"""
    print("\n" + "="*60)
    print("ğŸ” æµ‹è¯•Aurum - çœŸå®WebTableæ•°æ®é›†")
    print("="*60)
    
    tester = AurumSimpleTest('/root/dataLakesMulti/baselines/data/aurum')
    
    # ä½¿ç”¨WebTableæ•°æ®é›†ï¼ˆè¾ƒå¤§ï¼Œæœ‰195ä¸ªè¡¨æ ¼ï¼‰
    start_time = time.time()
    index = tester.build_index('webtable', 'join')
    index_time = time.time() - start_time
    
    print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ:")
    print(f"   - è¡¨æ ¼æ•°é‡: {len(index)}")
    print(f"   - æ„å»ºæ—¶é—´: {index_time:.2f}ç§’")
    print(f"   - å¹³å‡æ—¶é—´: {index_time/len(index):.3f}ç§’/è¡¨æ ¼")
    
    # é€‰æ‹©3ä¸ªæŸ¥è¯¢è¡¨æ ¼è¿›è¡Œæµ‹è¯•
    query_tables = list(index.keys())[:3]
    
    print(f"\nğŸ“Š æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯• (å‰3ä¸ªè¡¨æ ¼):")
    total_results = []
    
    for i, query_table in enumerate(query_tables):
        start_time = time.time()
        results = tester.query_similar_tables(query_table, index, threshold=0.05, top_k=5)
        query_time = time.time() - start_time
        
        print(f"\næŸ¥è¯¢ {i+1}: {query_table}")
        print(f"   æŸ¥è¯¢æ—¶é—´: {query_time:.4f}ç§’")
        print(f"   æ‰¾åˆ° {len(results)} ä¸ªç›¸ä¼¼è¡¨æ ¼:")
        
        for j, result in enumerate(results[:3]):
            print(f"      {j+1}. {result['table_name']}")
            print(f"         ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
            print(f"         åˆ—æ•°: {result['num_cols']}, è¡Œæ•°: {result['num_rows']}")
        
        total_results.extend(results)
    
    # ç»Ÿè®¡
    if total_results:
        similarities = [r['similarity'] for r in total_results]
        print(f"\nğŸ“ˆ ç›¸ä¼¼åº¦ç»Ÿè®¡:")
        print(f"   - å¹³å‡: {sum(similarities)/len(similarities):.3f}")
        print(f"   - æœ€é«˜: {max(similarities):.3f}")
        print(f"   - æœ€ä½: {min(similarities):.3f}")
    
    return len(index), total_results

def test_lsh_ensemble_on_real_data():
    """åœ¨çœŸå®WebTableæ•°æ®ä¸Šæµ‹è¯•LSH Ensemble"""
    print("\n" + "="*60)
    print("ğŸ” æµ‹è¯•LSH Ensemble - çœŸå®WebTableæ•°æ®é›†")
    print("="*60)
    
    data_dir = Path("/root/dataLakesMulti/baselines/data/lsh/webtable/join")
    
    # é€‰æ‹©å‰10ä¸ªæœ‰æ•ˆCSVæ–‡ä»¶è¿›è¡Œæµ‹è¯•
    csv_files = []
    for csv_file in data_dir.glob("*.csv"):
        try:
            df = pd.read_csv(csv_file, dtype=str).dropna()
            if len(df) > 0 and len(df.columns) > 0:
                csv_files.append(csv_file)
                if len(csv_files) >= 10:
                    break
        except:
            continue
    
    print(f"ä½¿ç”¨ {len(csv_files)} ä¸ªCSVæ–‡ä»¶è¿›è¡Œæµ‹è¯•")
    
    # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—æ‰€æœ‰åˆ—çš„å¤§å°
    start_time = time.time()
    sizes = []
    keys = []
    minhashes = []
    
    for csv_file in csv_files:
        df = pd.read_csv(csv_file, dtype=str).dropna()
        for column in df.columns:
            vals = df[column].dropna().astype(str).tolist()
            vals = list(set(vals))  # å»é‡
            
            key = f"{csv_file.name}.{column}"
            keys.append(key)
            sizes.append(len(vals))
            
            # åˆ›å»ºMinHash
            mh = MinHash(64, hashfunc=_hash_32)
            for val in vals:
                if val.strip():
                    mh.update(val.strip().lower().encode('utf-8'))
            minhashes.append(mh)
    
    print(f"âœ… å‡†å¤‡äº† {len(keys)} åˆ—çš„MinHash")
    
    # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºLSH Ensembleç´¢å¼•
    lsh = MinHashLSHEnsemble(threshold=0.1, num_perm=64, num_part=4, m=2)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰count_partitionæ–¹æ³•
    if hasattr(lsh, 'count_partition'):
        lsh.count_partition(sizes)
    
    # ç´¢å¼•æ‰€æœ‰åˆ—
    for key, mh, size in zip(keys, minhashes, sizes):
        lsh.index((key, mh, size))
    
    index_time = time.time() - start_time
    
    print(f"âœ… LSH Ensembleç´¢å¼•æ„å»ºå®Œæˆ:")
    print(f"   - åˆ—æ•°é‡: {len(keys)}")
    print(f"   - æ„å»ºæ—¶é—´: {index_time:.2f}ç§’")
    print(f"   - å¹³å‡æ—¶é—´: {index_time/len(keys):.3f}ç§’/åˆ—")
    
    # ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡ŒæŸ¥è¯¢æµ‹è¯•
    print(f"\nğŸ“Š æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯• (å‰3ä¸ªåˆ—):")
    
    for i in range(min(3, len(minhashes))):
        query_mh = minhashes[i]
        query_size = sizes[i]
        query_key = keys[i]
        
        start_time = time.time()
        results = list(lsh.query(query_mh, query_size))
        query_time = time.time() - start_time
        
        # è¿‡æ»¤æ‰è‡ªå·±
        results = [r for r in results if r != query_key]
        
        print(f"\næŸ¥è¯¢ {i+1}: {query_key}")
        print(f"   æŸ¥è¯¢æ—¶é—´: {query_time:.4f}ç§’")
        print(f"   æ‰¾åˆ° {len(results)} ä¸ªç›¸ä¼¼åˆ—:")
        
        for j, result in enumerate(results[:3]):
            # è®¡ç®—å®é™…çš„Jaccardç›¸ä¼¼åº¦
            result_idx = keys.index(result) if result in keys else -1
            if result_idx >= 0:
                jaccard = query_mh.jaccard(minhashes[result_idx])
                print(f"      {j+1}. {result}")
                print(f"         Jaccardç›¸ä¼¼åº¦: {jaccard:.3f}")
    
    return len(keys), results

def main():
    print("ğŸš€ å¼€å§‹éªŒè¯Baselineæ–¹æ³•çš„çœŸå®å®ç°")
    print("=" * 60)
    
    # æµ‹è¯•Aurum
    try:
        aurum_tables, aurum_results = test_aurum_on_real_data()
        print(f"\nâœ… AuruméªŒè¯æˆåŠŸ: å¤„ç†äº†{aurum_tables}ä¸ªè¡¨æ ¼")
    except Exception as e:
        print(f"\nâŒ Aurumæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # æµ‹è¯•LSH Ensemble
    try:
        lsh_columns, lsh_results = test_lsh_ensemble_on_real_data()
        print(f"\nâœ… LSH EnsembleéªŒè¯æˆåŠŸ: å¤„ç†äº†{lsh_columns}åˆ—")
    except Exception as e:
        print(f"\nâŒ LSH Ensembleæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "="*60)
    print("ğŸ‰ éªŒè¯å®Œæˆæ€»ç»“:")
    print("="*60)
    print("âœ… Aurum: çœŸå®çš„MinHashå®ç°ï¼Œè®¡ç®—Jaccardç›¸ä¼¼åº¦")
    print("âœ… LSH Ensemble: çœŸå®çš„åˆ†åŒºLSHå®ç°ï¼Œæ”¯æŒcontainmentæŸ¥è¯¢")
    print("âœ… ä¸¤ä¸ªæ–¹æ³•éƒ½æ˜¯çœŸå®å®ç°ï¼Œä¸æ˜¯æ¨¡æ‹Ÿ")
    print("âœ… éƒ½å¯ä»¥å¤„ç†çœŸå®æ•°æ®å¹¶äº§ç”Ÿæœ‰æ„ä¹‰çš„ç»“æœ")

if __name__ == "__main__":
    main()