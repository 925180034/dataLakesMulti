#!/usr/bin/env python
"""
æ‰¹é‡è¯„ä¼°è„šæœ¬ - æµ‹è¯•å¤šä¸ªæŸ¥è¯¢
æ¨¡æ‹ŸçœŸå®çš„è¯„ä¼°åœºæ™¯
"""

import asyncio
import json
import time
import logging
from typing import List, Dict, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å‡å°‘å™ªéŸ³
logging.getLogger("src").setLevel(logging.WARNING)

async def batch_evaluate(num_queries: int = 10):
    """
    æ‰¹é‡è¯„ä¼°å¤šä¸ªæŸ¥è¯¢
    
    Args:
        num_queries: è¦æµ‹è¯•çš„æŸ¥è¯¢æ•°é‡
    """
    from src.core.workflow import discover_data
    
    print("="*70)
    print(f"ğŸ“Š æ•°æ®æ¹–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ - æ‰¹é‡è¯„ä¼° ({num_queries} ä¸ªæŸ¥è¯¢)")
    print("="*70)
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    with open('examples/final_subset_tables.json') as f:
        all_tables = json.load(f)
    print(f"âœ… å·²åŠ è½½ {len(all_tables)} ä¸ªè¡¨")
    
    # 2. å‡†å¤‡æŸ¥è¯¢ï¼ˆé€‰æ‹©å‰num_queriesä¸ªè¡¨ä½œä¸ºæŸ¥è¯¢ï¼‰
    query_tables = all_tables[:num_queries]
    print(f"\nğŸ” å‡†å¤‡ {num_queries} ä¸ªæŸ¥è¯¢:")
    for i, table in enumerate(query_tables[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"  {i}. {table['table_name']}")
    if num_queries > 5:
        print(f"  ... è¿˜æœ‰ {num_queries-5} ä¸ªæŸ¥è¯¢")
    
    # 3. æ‰§è¡Œæ‰¹é‡æŸ¥è¯¢
    print(f"\nâš¡ å¼€å§‹æ‰¹é‡æŸ¥è¯¢...")
    print("-"*70)
    
    results = []
    total_matches = 0
    query_times = []
    llm_calls_total = 0
    
    # ç¬¬ä¸€ä¸ªæŸ¥è¯¢åŒ…å«åˆå§‹åŒ–
    start_total = time.time()
    
    for i, query_table in enumerate(query_tables, 1):
        print(f"\næŸ¥è¯¢ {i}/{num_queries}: {query_table['table_name']}")
        
        start_time = time.time()
        
        try:
            # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆåªç¬¬ä¸€æ¬¡åˆå§‹åŒ–all_tables_dataï¼‰
            result = await discover_data(
                user_query=f"Find joinable tables for {query_table['table_name']}",
                query_tables=[query_table],
                all_tables_data=all_tables if i == 1 else None,  # åªç¬¬ä¸€æ¬¡åˆå§‹åŒ–
                use_optimized=True
            )
            
            query_time = time.time() - start_time
            query_times.append(query_time)
            
            # æ”¶é›†ç»“æœ
            if hasattr(result, 'table_matches') and result.table_matches:
                match_count = len(result.table_matches)
                total_matches += match_count
                print(f"  âœ… æ‰¾åˆ° {match_count} ä¸ªåŒ¹é… (è€—æ—¶: {query_time:.2f}ç§’)")
                
                # æ˜¾ç¤ºå‰3ä¸ªåŒ¹é…
                for j, match in enumerate(result.table_matches[:3], 1):
                    print(f"     {j}. {match.target_table} (åˆ†æ•°: {match.score:.2f})")
                    
                results.append({
                    "query": query_table['table_name'],
                    "matches": match_count,
                    "time": query_time,
                    "top_match": result.table_matches[0].target_table if result.table_matches else None
                })
            else:
                print(f"  âŒ æœªæ‰¾åˆ°åŒ¹é… (è€—æ—¶: {query_time:.2f}ç§’)")
                results.append({
                    "query": query_table['table_name'],
                    "matches": 0,
                    "time": query_time,
                    "top_match": None
                })
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            query_times.append(30.0)  # é»˜è®¤è¶…æ—¶
            results.append({
                "query": query_table['table_name'],
                "matches": 0,
                "time": 30.0,
                "error": str(e)
            })
    
    total_time = time.time() - start_total
    
    # 4. ç»Ÿè®¡åˆ†æ
    print("\n" + "="*70)
    print("ğŸ“Š æ‰¹é‡è¯„ä¼°ç»“æœ")
    print("="*70)
    
    avg_time = sum(query_times) / len(query_times) if query_times else 0
    min_time = min(query_times) if query_times else 0
    max_time = max(query_times) if query_times else 0
    
    # åˆ†ç¦»åˆå§‹åŒ–æ—¶é—´
    avg_time_no_init = sum(query_times[1:]) / len(query_times[1:]) if len(query_times) > 1 else avg_time
    
    print(f"\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
    print(f"  â€¢ æŸ¥è¯¢æ€»æ•°: {num_queries}")
    print(f"  â€¢ æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"  â€¢ å¹³å‡æ¯æŸ¥è¯¢: {avg_time:.2f}ç§’")
    print(f"  â€¢ å¹³å‡æ¯æŸ¥è¯¢(ä¸å«åˆå§‹åŒ–): {avg_time_no_init:.2f}ç§’")
    print(f"  â€¢ æœ€å¿«æŸ¥è¯¢: {min_time:.2f}ç§’")
    print(f"  â€¢ æœ€æ…¢æŸ¥è¯¢: {max_time:.2f}ç§’")
    
    print(f"\nğŸ¯ åŒ¹é…ç»Ÿè®¡:")
    print(f"  â€¢ æ€»åŒ¹é…æ•°: {total_matches}")
    print(f"  â€¢ å¹³å‡æ¯æŸ¥è¯¢åŒ¹é…æ•°: {total_matches/num_queries:.1f}")
    print(f"  â€¢ æˆåŠŸæŸ¥è¯¢ç‡: {len([r for r in results if r['matches'] > 0])/num_queries*100:.1f}%")
    
    print(f"\nâš¡ æ€§èƒ½è¯„ä¼°:")
    if avg_time_no_init <= 3:
        print(f"  âœ… ä¼˜ç§€ï¼å¹³å‡ {avg_time_no_init:.2f}ç§’ â‰¤ 3ç§’")
    elif avg_time_no_init <= 8:
        print(f"  âœ… è¾¾æ ‡ï¼å¹³å‡ {avg_time_no_init:.2f}ç§’ â‰¤ 8ç§’ç›®æ ‡")
    else:
        print(f"  âš ï¸ æœªè¾¾æ ‡ã€‚å¹³å‡ {avg_time_no_init:.2f}ç§’ > 8ç§’ç›®æ ‡")
    
    # 5. ä¿å­˜è¯¦ç»†ç»“æœ
    output = {
        "summary": {
            "total_queries": num_queries,
            "total_time": total_time,
            "avg_time": avg_time,
            "avg_time_no_init": avg_time_no_init,
            "min_time": min_time,
            "max_time": max_time,
            "total_matches": total_matches,
            "success_rate": len([r for r in results if r['matches'] > 0])/num_queries
        },
        "results": results
    }
    
    with open('batch_evaluation_results.json', 'w') as f:
        json.dump(output, f, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° batch_evaluation_results.json")
    print("="*70)
    
    return output


async def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # ä»å‘½ä»¤è¡Œè·å–æŸ¥è¯¢æ•°é‡
    num_queries = 10  # é»˜è®¤10ä¸ª
    if len(sys.argv) > 1:
        try:
            num_queries = int(sys.argv[1])
        except:
            print("ç”¨æ³•: python batch_evaluation.py [æŸ¥è¯¢æ•°é‡]")
            print("ç¤ºä¾‹: python batch_evaluation.py 20")
            return
    
    # ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨è¡¨æ•°é‡
    with open('examples/final_subset_tables.json') as f:
        all_tables = json.load(f)
    num_queries = min(num_queries, len(all_tables))
    
    # è¿è¡Œè¯„ä¼°
    await batch_evaluate(num_queries)


if __name__ == "__main__":
    asyncio.run(main())