#!/usr/bin/env python
"""
ä¸ºjoinå’Œunionä¸¤ç§ä»»åŠ¡ç±»å‹åˆ›å»ºé«˜è´¨é‡æ•°æ®é›†
"""
import csv
import json
import os
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Set, Tuple
import shutil

def load_csv_ground_truth(file_path: str) -> List[Dict]:
    """åŠ è½½CSVæ ¼å¼çš„ground truth"""
    ground_truth = []
    with open(file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            ground_truth.append(row)
    return ground_truth

def get_available_tables(tables_dir: str) -> Set[str]:
    """è·å–æ‰€æœ‰å¯ç”¨çš„è¡¨å"""
    tables = set()
    for file_name in os.listdir(tables_dir):
        if file_name.endswith('.csv'):
            tables.add(file_name)
    return tables

def extract_quality_ground_truth(
    ground_truth: List[Dict],
    available_tables: Set[str],
    task_type: str,
    min_candidates: int = 1,
    max_candidates: int = 20,
    max_queries: int = 100
) -> Tuple[Dict[str, List[str]], Dict]:
    """
    æå–é«˜è´¨é‡çš„ground truth
    """
    # æ„å»ºæŸ¥è¯¢åˆ°å€™é€‰è¡¨çš„æ˜ å°„
    query_to_candidates = defaultdict(set)
    
    for entry in ground_truth:
        query_table = entry.get('query_table', '')
        candidate_table = entry.get('candidate_table', '')
        
        # è¿‡æ»¤æ¡ä»¶
        if not query_table or not candidate_table:
            continue
        if query_table == candidate_table:  # è‡ªåŒ¹é…
            continue
        if query_table not in available_tables:
            continue
        if candidate_table not in available_tables:
            continue
            
        query_to_candidates[query_table].add(candidate_table)
    
    # åˆ†ææ•°æ®åˆ†å¸ƒ
    candidate_distribution = Counter()
    for candidates in query_to_candidates.values():
        candidate_distribution[len(candidates)] += 1
    
    print(f"  {task_type.upper()} - åŸå§‹åˆ†å¸ƒ:")
    for count in sorted(candidate_distribution.keys())[:10]:
        print(f"    {count}ä¸ªå€™é€‰: {candidate_distribution[count]}ä¸ªæŸ¥è¯¢")
    
    # é€‰æ‹©é«˜è´¨é‡çš„æŸ¥è¯¢
    quality_queries = {}
    
    # 1. æŒ‰å€™é€‰è¡¨æ•°é‡åˆ†ç»„
    queries_by_candidate_count = defaultdict(list)
    for q, c in query_to_candidates.items():
        count = len(c)
        if min_candidates <= count <= max_candidates:
            queries_by_candidate_count[count].append((q, list(c)))
    
    # 2. ä»æ¯ä¸ªç»„ä¸­é€‰æ‹©æŸ¥è¯¢ï¼Œç¡®ä¿å¤šæ ·æ€§
    queries_per_group = max(5, max_queries // len(queries_by_candidate_count)) if queries_by_candidate_count else 0
    
    for count in sorted(queries_by_candidate_count.keys()):
        queries_in_group = queries_by_candidate_count[count]
        for query, candidates in queries_in_group[:queries_per_group]:
            if len(quality_queries) < max_queries:
                quality_queries[query] = candidates
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_queries': len(quality_queries),
        'total_candidates': sum(len(c) for c in quality_queries.values()),
        'avg_candidates': sum(len(c) for c in quality_queries.values()) / len(quality_queries) if quality_queries else 0,
        'distribution': Counter(len(c) for c in quality_queries.values())
    }
    
    return quality_queries, stats

def load_table_data(table_path: str, max_rows: int = 100) -> Dict:
    """åŠ è½½å•ä¸ªè¡¨çš„æ•°æ®"""
    try:
        with open(table_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()[:max_rows]
        
        if not lines:
            return None
            
        # è§£æåˆ—ä¿¡æ¯
        header = lines[0].strip().split(',')
        columns = []
        
        for i, col_name in enumerate(header):
            sample_values = []
            for line in lines[1:6]:  # å‰5è¡Œä½œä¸ºæ ·æœ¬
                values = line.strip().split(',')
                if i < len(values):
                    sample_values.append(values[i])
            
            columns.append({
                'name': col_name,
                'type': 'string',
                'sample_values': sample_values[:3]
            })
        
        return {
            'table_name': Path(table_path).name,
            'columns': columns
        }
    except Exception as e:
        print(f"    è­¦å‘Š: æ— æ³•è¯»å–è¡¨ {table_path}: {e}")
        return None

def create_dataset_for_task(
    task_type: str,
    quality_ground_truth: Dict[str, List[str]],
    tables_dir: str,
    output_dir: str
) -> Dict:
    """ä¸ºç‰¹å®šä»»åŠ¡åˆ›å»ºæ•°æ®é›†æ–‡ä»¶"""
    task_dir = output_dir / task_type
    task_dir.mkdir(parents=True, exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰ç›¸å…³çš„è¡¨
    all_tables = set()
    for query in quality_ground_truth.keys():
        all_tables.add(query)
    for candidates in quality_ground_truth.values():
        all_tables.update(candidates)
    
    print(f"\n  å¤„ç† {len(all_tables)} ä¸ªè¡¨...")
    
    # 1. åˆ›å»ºtables.json
    tables_data = []
    for i, table_name in enumerate(all_tables, 1):
        if i % 50 == 0:
            print(f"    å·²å¤„ç† {i}/{len(all_tables)} ä¸ªè¡¨...")
        
        table_path = os.path.join(tables_dir, table_name)
        if os.path.exists(table_path):
            table_info = load_table_data(table_path)
            if table_info:
                tables_data.append(table_info)
    
    with open(task_dir / 'tables.json', 'w') as f:
        json.dump(tables_data, f, indent=2)
    print(f"  âœ… åˆ›å»º tables.json: {len(tables_data)} ä¸ªè¡¨")
    
    # 2. åˆ›å»ºqueries.json
    queries_data = []
    for query_table in quality_ground_truth.keys():
        queries_data.append({
            'query_table': query_table,
            'query_type': task_type
        })
    
    with open(task_dir / 'queries.json', 'w') as f:
        json.dump(queries_data, f, indent=2)
    print(f"  âœ… åˆ›å»º queries.json: {len(queries_data)} ä¸ªæŸ¥è¯¢")
    
    # 3. åˆ›å»ºground_truth.json
    ground_truth_data = []
    for query_table, candidates in quality_ground_truth.items():
        for candidate in candidates:
            ground_truth_data.append({
                'query_table': query_table,
                'candidate_table': candidate,
                'query_type': task_type
            })
    
    with open(task_dir / 'ground_truth.json', 'w') as f:
        json.dump(ground_truth_data, f, indent=2)
    print(f"  âœ… åˆ›å»º ground_truth.json: {len(ground_truth_data)} æ¡è®°å½•")
    
    return {
        'tables': len(tables_data),
        'queries': len(queries_data),
        'ground_truth_entries': len(ground_truth_data)
    }

def main():
    print("="*80)
    print("ğŸ”§ åˆ›å»ºå®Œæ•´çš„é«˜è´¨é‡æ•°æ®é›†ï¼ˆJOIN + UNIONï¼‰")
    print("="*80)
    
    base_dir = Path('/root/dataLakesMulti/Datasets/webtable')
    output_dir = Path('/root/dataLakesMulti/examples/separated_datasets')
    
    # å¤„ç†JOINå’ŒUNIONä¸¤ç§ä»»åŠ¡
    all_stats = {}
    
    for task_type in ['join', 'union']:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å¤„ç† {task_type.upper()} ä»»åŠ¡")
        print(f"{'='*60}")
        
        # è·¯å¾„
        task_base = base_dir / task_type
        gt_file = task_base / f'webtable_{task_type}_ground_truth.csv'
        tables_dir = task_base / 'tables'
        
        # 1. åŠ è½½æ•°æ®
        print(f"\nğŸ“š åŠ è½½ {task_type} æ•°æ®...")
        ground_truth = load_csv_ground_truth(gt_file)
        print(f"  åŸå§‹ground truth: {len(ground_truth)} æ¡")
        
        available_tables = get_available_tables(tables_dir)
        print(f"  å¯ç”¨è¡¨: {len(available_tables)} ä¸ª")
        
        # 2. æå–é«˜è´¨é‡æ•°æ®
        print(f"\nğŸ¯ æå–é«˜è´¨é‡ground truth...")
        quality_gt, stats = extract_quality_ground_truth(
            ground_truth,
            available_tables,
            task_type,
            min_candidates=1,
            max_candidates=20,
            max_queries=100
        )
        
        print(f"\n  é€‰æ‹©äº† {stats['total_queries']} ä¸ªæŸ¥è¯¢")
        print(f"  å¹³å‡å€™é€‰è¡¨æ•°: {stats['avg_candidates']:.2f}")
        
        # æ˜¾ç¤ºåˆ†å¸ƒ
        print(f"\n  é€‰ä¸­æŸ¥è¯¢çš„åˆ†å¸ƒ:")
        for count, freq in sorted(stats['distribution'].items())[:10]:
            print(f"    {count}ä¸ªå€™é€‰: {freq}ä¸ªæŸ¥è¯¢")
        
        # 3. åˆ›å»ºæ•°æ®é›†æ–‡ä»¶
        print(f"\nğŸ’¾ åˆ›å»º {task_type} æ•°æ®é›†...")
        task_stats = create_dataset_for_task(
            task_type,
            quality_gt,
            tables_dir,
            output_dir
        )
        
        # ä¿å­˜å®Œæ•´åç§°çš„æ•°æ®é›†ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
        full_name_dir = output_dir / f"{task_type}_subset"
        full_name_dir.mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶æ–‡ä»¶åˆ°å…¼å®¹ç›®å½•
        for file_name in ['tables.json', 'queries.json', 'ground_truth.json']:
            src = output_dir / task_type / file_name
            dst = full_name_dir / file_name
            shutil.copy2(src, dst)
        
        print(f"  âœ… åŒæ—¶åˆ›å»ºäº†å…¼å®¹ç›®å½•: {full_name_dir}")
        
        all_stats[task_type] = {
            **stats,
            **task_stats
        }
    
    # 4. åˆ›å»ºç»¼åˆç»Ÿè®¡
    print("\n" + "="*80)
    print("ğŸ“ˆ æ•°æ®é›†åˆ›å»ºå®Œæˆ - æ€»ä½“ç»Ÿè®¡")
    print("="*80)
    
    for task_type in ['join', 'union']:
        stats = all_stats[task_type]
        print(f"\n{task_type.upper()} ä»»åŠ¡:")
        print(f"  æŸ¥è¯¢æ•°: {stats['total_queries']}")
        print(f"  è¡¨æ•°: {stats['tables']}")
        print(f"  Ground Truthæ¡ç›®: {stats['ground_truth_entries']}")
        print(f"  å¹³å‡å€™é€‰è¡¨æ•°: {stats['avg_candidates']:.2f}")
    
    # 5. ä¿å­˜æ€»ä½“ç»Ÿè®¡
    with open(output_dir / 'dataset_statistics.json', 'w') as f:
        json.dump(all_stats, f, indent=2)
    
    # 6. åˆ›å»ºREADME
    readme_content = f"""# é«˜è´¨é‡æ•°æ®é›†

## æ•°æ®é›†ç»“æ„
```
separated_datasets/
â”œâ”€â”€ join/                    # JOINä»»åŠ¡æ•°æ®
â”‚   â”œâ”€â”€ tables.json          # {all_stats['join']['tables']}ä¸ªè¡¨
â”‚   â”œâ”€â”€ queries.json         # {all_stats['join']['total_queries']}ä¸ªæŸ¥è¯¢
â”‚   â””â”€â”€ ground_truth.json    # {all_stats['join']['ground_truth_entries']}æ¡è®°å½•
â”œâ”€â”€ union/                   # UNIONä»»åŠ¡æ•°æ®
â”‚   â”œâ”€â”€ tables.json          # {all_stats['union']['tables']}ä¸ªè¡¨
â”‚   â”œâ”€â”€ queries.json         # {all_stats['union']['total_queries']}ä¸ªæŸ¥è¯¢
â”‚   â””â”€â”€ ground_truth.json    # {all_stats['union']['ground_truth_entries']}æ¡è®°å½•
â”œâ”€â”€ join_subset/             # JOINå…¼å®¹ç›®å½•ï¼ˆåŒjoin/ï¼‰
â”œâ”€â”€ union_subset/            # UNIONå…¼å®¹ç›®å½•ï¼ˆåŒunion/ï¼‰
â””â”€â”€ dataset_statistics.json # ç»Ÿè®¡ä¿¡æ¯
```

## æ•°æ®è´¨é‡
- **JOIN**: å¹³å‡{all_stats['join']['avg_candidates']:.1f}ä¸ªå€™é€‰è¡¨/æŸ¥è¯¢
- **UNION**: å¹³å‡{all_stats['union']['avg_candidates']:.1f}ä¸ªå€™é€‰è¡¨/æŸ¥è¯¢
- **100%è¦†ç›–ç‡**: æ‰€æœ‰æŸ¥è¯¢éƒ½æœ‰ground truth
- **è¿‡æ»¤è‡ªåŒ¹é…**: ç§»é™¤äº†æ‰€æœ‰è‡ªåŒ¹é…çš„å€™é€‰
- **éªŒè¯è¡¨å­˜åœ¨**: ç¡®ä¿æ‰€æœ‰è¡¨éƒ½åœ¨æ•°æ®é›†ä¸­

## ä½¿ç”¨æ–¹æ³•

### æµ‹è¯•å•ä¸ªä»»åŠ¡
```bash
# æµ‹è¯•JOINä»»åŠ¡
python run_cached_experiments.py --task join --dataset subset --max-queries 10

# æµ‹è¯•UNIONä»»åŠ¡
python run_cached_experiments.py --task union --dataset subset --max-queries 10
```

### æµ‹è¯•ä¸¤ä¸ªä»»åŠ¡
```bash
python run_cached_experiments.py --task both --dataset subset --max-queries 10
```

## åˆ›å»ºæ—¶é—´
{Path(__file__).stat().st_mtime}
"""
    
    with open(output_dir / 'README.md', 'w') as f:
        f.write(readme_content)
    
    print("\nâœ… å®Œæˆï¼")
    print(f"\nè¾“å‡ºç›®å½•: {output_dir}")
    print(f"åŒ…å«:")
    print(f"  - join/ : JOINä»»åŠ¡æ•°æ®é›†")
    print(f"  - union/ : UNIONä»»åŠ¡æ•°æ®é›†")
    print(f"  - join_subset/ : JOINå…¼å®¹ç›®å½•")
    print(f"  - union_subset/ : UNIONå…¼å®¹ç›®å½•")
    print(f"  - dataset_statistics.json : ç»Ÿè®¡ä¿¡æ¯")
    print(f"  - README.md : ä½¿ç”¨è¯´æ˜")
    
    print(f"\nä½¿ç”¨ç¤ºä¾‹:")
    print(f"  python run_cached_experiments.py --task both --dataset subset --max-queries 20")


if __name__ == "__main__":
    main()