# NLCTables å®žéªŒæŒ‡å—ï¼ˆåŸºäºŽè®ºæ–‡ï¼‰

## ðŸ“š è®ºæ–‡æ ¸å¿ƒè¦ç‚¹

### æ•°æ®é›†åˆ›æ–°ç‚¹
**NLCTables** æ˜¯é¦–ä¸ªç»“åˆ**æŸ¥è¯¢è¡¨æ ¼**å’Œ**è‡ªç„¶è¯­è¨€æ¡ä»¶**çš„è¡¨æ ¼å‘çŽ°æ•°æ®é›†ï¼Œå®šä¹‰äº†æ–°çš„ä»»åŠ¡åœºæ™¯ï¼š
- **ä¼ ç»Ÿæ–¹æ³•**ï¼šä»…ä½¿ç”¨å…³é”®è¯æˆ–æŸ¥è¯¢è¡¨æ ¼
- **nlcTDåˆ›æ–°**ï¼šæŸ¥è¯¢è¡¨æ ¼ + è‡ªç„¶è¯­è¨€éœ€æ±‚ â†’ æ›´ç²¾ç¡®çš„è¡¨æ ¼å‘çŽ°

### æ•°æ®é›†è§„æ¨¡ï¼ˆè®ºæ–‡ Table 2ï¼‰
```
Dataset Type    Queries  Tables   GT      Pos:Neg Ratio
nlcTables_K     235      7,405    6,841   1:8.6
nlcTables-U     255      7,567    7,411   1:10.7
nlcTables-J     91       4,871    4,821   1:21.8
nlcTables-U-fz  39       1,620    1,560   1:11.7
nlcTables-J-fz  27       617      567     1:7.5
Total           647      22,080   21,200  -
```

## ðŸ”¬ å®žéªŒæ–¹æ³•è®ºï¼ˆåŸºäºŽè®ºæ–‡ Section 5ï¼‰

### 1. è¯„ä¼°æŒ‡æ ‡

è®ºæ–‡ä½¿ç”¨ä¸‰ä¸ªæ ‡å‡†æŒ‡æ ‡ï¼ˆSection 5.1ï¼‰ï¼š

```python
def calculate_metrics(retrieved_tables, ground_truth, k):
    """
    è®¡ç®— Precision@k, Recall@k, NDCG@k
    åŸºäºŽè®ºæ–‡å…¬å¼ï¼ˆç¬¬7é¡µï¼‰
    """
    # Precision@k = |T_g âˆ© T'| / |T'|
    precision = len(set(retrieved_tables[:k]) & set(ground_truth)) / k
    
    # Recall@k = |T_g âˆ© T'| / |T_g|
    recall = len(set(retrieved_tables[:k]) & set(ground_truth)) / len(ground_truth)
    
    # NDCG@k = 1/Z_k * Î£(Ï_i / log2(i+1))
    dcg = sum(relevance_score / np.log2(i+2) 
              for i, relevance_score in enumerate(relevance_scores[:k]))
    idcg = calculate_ideal_dcg(ground_truth_scores, k)
    ndcg = dcg / idcg if idcg > 0 else 0
    
    return precision, recall, ndcg
```

### 2. åŸºçº¿æ–¹æ³•å¯¹æ¯”

è®ºæ–‡æµ‹è¯•äº†6ä¸ªä»£è¡¨æ€§æ–¹æ³•ï¼ˆSection 5.1ï¼‰ï¼š

| ç±»åž‹ | æ–¹æ³• | é€‚ç”¨ä»»åŠ¡ | è®ºæ–‡ç»“æžœ |
|------|------|----------|----------|
| Keyword-based | GTR (SIGIR'21) | K, U, J | æ•´ä½“è¡¨çŽ°è¾ƒå¥½ |
| Keyword-based | StruBERT (WWW'22) | K, U, J | é•¿å¥æŸ¥è¯¢è¡¨çŽ°å·® |
| Union Search | Santos (SIGMOD'23) | U | å¬å›žé«˜ä½†ç²¾åº¦ä½Ž |
| Union Search | Starmie (VLDB'23) | U | è¯­ä¹‰æ•æ„Ÿæ€§å¥½ |
| Join Search | Josie (SIGMOD'19) | J | æžä½Žå¬å›žçŽ‡ |
| Join Search | DeepJoin (VLDB'23) | J | è¯­ä¹‰åŒ¹é…å·® |

### 3. å®žéªŒè®¾ç½®å»ºè®®

åŸºäºŽè®ºæ–‡ Section 5 çš„å®žéªŒè®¾è®¡ï¼š

```python
class NLCTablesExperiment:
    def __init__(self):
        self.metrics_k = [5, 10, 15, 20]  # è®ºæ–‡ä½¿ç”¨çš„kå€¼
        self.dataset_types = ['K', 'U', 'J', 'U-fz', 'J-fz']
        
    def run_experiments(self, method, dataset_type):
        """
        å¤çŽ°è®ºæ–‡å®žéªŒæµç¨‹
        """
        # 1. åŠ è½½æ•°æ®é›†
        queries, tables, ground_truth = self.load_nlctables(dataset_type)
        
        # 2. å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œæ£€ç´¢
        results = {}
        for query in queries:
            # æå–æŸ¥è¯¢è¡¨æ ¼å’ŒNLæ¡ä»¶
            query_table = query['query_table']
            nl_condition = query['nl_condition']
            
            # æ‰§è¡Œæ£€ç´¢
            retrieved = method.search(query_table, nl_condition, tables)
            
            # è®¡ç®—æŒ‡æ ‡
            for k in self.metrics_k:
                metrics = calculate_metrics(retrieved, ground_truth[query['id']], k)
                results[f"{query['id']}_k{k}"] = metrics
        
        return results
```

## ðŸŽ¯ å…³é”®å®žéªŒå‘çŽ°ï¼ˆè®ºæ–‡ Section 5.2-5.4ï¼‰

### RQ1: æ•´ä½“æ€§èƒ½åˆ†æž
- **å‘çŽ°**ï¼šçŽ°æœ‰æ–¹æ³•åœ¨nlcTDä»»åŠ¡ä¸Šè¡¨çŽ°å¾ˆå·®
- **åŽŸå› **ï¼šæ— æ³•åŒæ—¶å¤„ç†æŸ¥è¯¢è¡¨æ ¼å’Œè‡ªç„¶è¯­è¨€æ¡ä»¶
- **å»ºè®®**ï¼šéœ€è¦å¼€å‘æ–°çš„èžåˆæ–¹æ³•

### RQ2: æ¡ä»¶ç±»åž‹åˆ†æžï¼ˆTable 4ï¼‰
ä¸åŒNLæ¡ä»¶ç±»åž‹çš„æ€§èƒ½å·®å¼‚ï¼š

| æ¡ä»¶ç±»åž‹ | æ€§èƒ½ | åŽŸå›  |
|----------|------|------|
| Table Topic | æœ€å¥½ | ä¸Žå…³é”®è¯å¯¹é½ |
| Table Size | ä¸­ç­‰ | å®¹æ˜“é‡åŒ– |
| Categorical | ä¸­ç­‰ | ç±»åˆ«åŒ¹é…ç®€å• |
| String | è¾ƒå·® | è¯­ä¹‰ç†è§£éœ€æ±‚ |
| Numerical | å·® | é˜ˆå€¼åˆ¤æ–­å›°éš¾ |
| Date | æœ€å·® | æ—¶é—´æŽ¨ç†å¤æ‚ |
| Mixed-mode | æžå·® | å¤šæ¡ä»¶ç»„åˆ |

### RQ3: æ•°æ®é›†ç»„æˆå½±å“
1. **æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹**ï¼šè´Ÿæ ·æœ¬å¢žåŠ å¯¼è‡´æ€§èƒ½ä¸‹é™
2. **æ•°æ®è§„æ¨¡**ï¼šå¤§è§„æ¨¡æ—¶éœ€è¦ç´¢å¼•ç»“æž„
3. **æ¨¡ç³ŠæŸ¥è¯¢**ï¼šè¯­ä¹‰å¢žå¼ºåŽæ€§èƒ½ä¸‹é™æ˜Žæ˜¾

## ðŸš€ å®žéªŒæµç¨‹å»ºè®®

### Phase 1: åŸºçº¿è¯„ä¼°ï¼ˆ2-3å¤©ï¼‰

```bash
# 1. æ•°æ®å‡†å¤‡
python convert_nlctables.py \
    --input /root/autodl-tmp/datalakes/nlcTables \
    --output examples/nlctables

# 2. åŸºçº¿æµ‹è¯•ï¼ˆå¤çŽ°è®ºæ–‡ Figure 5ï¼‰
python run_baseline_experiment.py \
    --dataset nlctables \
    --methods all \
    --metrics P@k,R@k,NDCG@k \
    --k 5,10,15,20
```

### Phase 2: ç³»ç»Ÿé€‚é…ï¼ˆ3-5å¤©ï¼‰

é’ˆå¯¹ä½ çš„ä¸‰å±‚æž¶æž„ç³»ç»Ÿçš„é€‚é…ç­–ç•¥ï¼š

```python
class NLCTablesAdapter:
    """å°†nlcTablesé€‚é…åˆ°ä¸‰å±‚æž¶æž„"""
    
    def adapt_for_l1_metadata(self, nl_condition):
        """Layer 1: å…ƒæ•°æ®è¿‡æ»¤é€‚é…"""
        # æå–è¡¨çº§æ¡ä»¶
        table_conditions = extract_table_level_conditions(nl_condition)
        # è½¬æ¢ä¸ºå…ƒæ•°æ®è¿‡æ»¤è§„åˆ™
        return {
            'topic': table_conditions.get('topic'),
            'min_rows': table_conditions.get('size', {}).get('min_rows'),
            'min_cols': table_conditions.get('size', {}).get('min_cols')
        }
    
    def adapt_for_l2_vector(self, query_table, nl_condition):
        """Layer 2: å‘é‡æœç´¢é€‚é…"""
        # ç»„åˆæŸ¥è¯¢è¡¨æ ¼å’ŒNLæ¡ä»¶çš„åµŒå…¥
        table_embedding = self.encode_table(query_table)
        condition_embedding = self.encode_nl(nl_condition)
        # åŠ æƒç»„åˆ
        combined = 0.6 * table_embedding + 0.4 * condition_embedding
        return combined
    
    def adapt_for_l3_llm(self, query_table, nl_condition, candidates):
        """Layer 3: LLMéªŒè¯é€‚é…"""
        prompt = f"""
        Given query table: {query_table}
        NL condition: {nl_condition}
        
        Rank these candidate tables based on:
        1. Structural compatibility (union/join)
        2. Satisfaction of NL conditions
        3. Semantic relevance
        """
        return self.llm_rerank(prompt, candidates)
```

### Phase 3: æ¡ä»¶ç±»åž‹ä¼˜åŒ–ï¼ˆ5-7å¤©ï¼‰

åŸºäºŽè®ºæ–‡ Table 4 çš„å‘çŽ°ï¼Œé’ˆå¯¹æ€§ä¼˜åŒ–ï¼š

```python
class ConditionTypeOptimizer:
    """é’ˆå¯¹ä¸åŒæ¡ä»¶ç±»åž‹çš„ä¼˜åŒ–ç­–ç•¥"""
    
    def optimize_numerical_conditions(self, condition):
        """ä¼˜åŒ–æ•°å€¼æ¡ä»¶å¤„ç†"""
        # è®ºæ–‡å‘çŽ°ï¼šæ•°å€¼æ¡ä»¶æ€§èƒ½å·®
        # ç­–ç•¥ï¼šå¢žå¼ºé˜ˆå€¼åˆ¤æ–­é€»è¾‘
        threshold = extract_numerical_threshold(condition)
        return {
            'operator': threshold['op'],  # >, <, =
            'value': threshold['value'],
            'column': threshold['column']
        }
    
    def optimize_mixed_mode(self, conditions):
        """ä¼˜åŒ–æ··åˆæ¨¡å¼æ¡ä»¶"""
        # è®ºæ–‡å‘çŽ°ï¼šæ··åˆæ¡ä»¶æžå…·æŒ‘æˆ˜æ€§
        # ç­–ç•¥ï¼šåˆ†è§£ä¸ºå­æ¡ä»¶ï¼Œåˆ†åˆ«å¤„ç†åŽèšåˆ
        sub_conditions = decompose_conditions(conditions)
        sub_results = [self.process_single(c) for c in sub_conditions]
        return aggregate_results(sub_results)
```

### Phase 4: æ¶ˆèžå®žéªŒï¼ˆ3-4å¤©ï¼‰

å¤çŽ°è®ºæ–‡çš„æ¶ˆèžç ”ç©¶ï¼š

```python
# 1. æ¨¡ç³ŠæŸ¥è¯¢å½±å“ï¼ˆFigure 6ï¼‰
python ablation_fuzzy.py --compare original,fuzzy

# 2. æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹å½±å“ï¼ˆFigure 7ï¼‰
python ablation_ratio.py --ratios 1:3,1:6,1:12

# 3. æ•°æ®è§„æ¨¡å½±å“ï¼ˆTable 5ï¼‰
python ablation_scale.py --scales small,medium,large
```

## ðŸ“Š é¢„æœŸç»“æžœä¸Žæ”¹è¿›æ–¹å‘

### åŸºäºŽè®ºæ–‡çš„æ€§èƒ½é¢„æœŸ

| æ•°æ®é›† | æ–¹æ³•ç±»åž‹ | NDCG@10é¢„æœŸ | ä½ çš„ç³»ç»Ÿç›®æ ‡ |
|--------|----------|-------------|--------------|
| nlcTables-K | Keyword | 0.45-0.55 | 0.60+ |
| nlcTables-U | Union | 0.35-0.45 | 0.55+ |
| nlcTables-J | Join | 0.30-0.40 | 0.50+ |

### è®ºæ–‡æŒ‡å‡ºçš„æ”¹è¿›æ–¹å‘

1. **è¯­ä¹‰ç†è§£å¢žå¼º**ï¼š
   - çŽ°æœ‰æ–¹æ³•å¿½ç•¥NLæ¡ä»¶è¯­ä¹‰
   - å»ºè®®ï¼šä½¿ç”¨å¤§è¯­è¨€æ¨¡åž‹ç†è§£å¤æ‚æ¡ä»¶

2. **å¤šæ¨¡æ€èžåˆ**ï¼š
   - æŸ¥è¯¢è¡¨æ ¼å’ŒNLæ¡ä»¶çš„æœ‰æ•ˆç»“åˆ
   - å»ºè®®ï¼šè®¾è®¡è”åˆç¼–ç å™¨

3. **æ¡ä»¶åˆ†è§£ç­–ç•¥**ï¼š
   - æ··åˆæ¡ä»¶å¤„ç†å›°éš¾
   - å»ºè®®ï¼šå±‚æ¬¡åŒ–æ¡ä»¶å¤„ç†

4. **åˆ—ç±»åž‹æ„ŸçŸ¥**ï¼š
   - æ•°å€¼/æ—¥æœŸåˆ—è¢«å¿½ç•¥
   - å»ºè®®ï¼šç±»åž‹ç‰¹å®šçš„å¤„ç†æ¨¡å—

## ðŸ”§ å®žéªŒè„šæœ¬æ¨¡æ¿

```bash
#!/bin/bash
# nlctables_experiment.sh

# çŽ¯å¢ƒå‡†å¤‡
export DATASET_PATH=/root/autodl-tmp/datalakes/nlcTables
export OUTPUT_DIR=experiment_results/nlctables

# å®žéªŒ1: æ•´ä½“æ€§èƒ½è¯„ä¼°ï¼ˆå¤çŽ°Figure 5ï¼‰
python three_layer_ablation_optimized.py \
    --dataset $DATASET_PATH/nlcTables-U \
    --task union \
    --layers L1+L2+L3 \
    --metrics P@5,P@10,P@15,P@20,R@5,R@10,R@15,R@20,NDCG@5,NDCG@10,NDCG@15,NDCG@20 \
    --output $OUTPUT_DIR/overall_performance.json

# å®žéªŒ2: æ¡ä»¶ç±»åž‹åˆ†æžï¼ˆå¤çŽ°Table 4ï¼‰
for condition_type in topic size categorical string numerical date mixed; do
    python condition_type_analysis.py \
        --dataset $DATASET_PATH \
        --condition_type $condition_type \
        --output $OUTPUT_DIR/condition_${condition_type}.json
done

# å®žéªŒ3: æ•°æ®ç»„æˆç ”ç©¶ï¼ˆå¤çŽ°Section 5.4ï¼‰
python dataset_composition_study.py \
    --dataset $DATASET_PATH \
    --experiments fuzzy,ratio,scale \
    --output $OUTPUT_DIR/composition_study.json

# ç»“æžœæ±‡æ€»
python summarize_nlctables_results.py \
    --input $OUTPUT_DIR \
    --output nlctables_report.pdf
```

## ðŸ“ˆ è®ºæ–‡å…³é”®æ´žå¯Ÿæ€»ç»“

1. **nlcTDæ˜¯ä¸€ä¸ªå…¨æ–°ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡**
   - éœ€è¦åŒæ—¶ç†è§£è¡¨æ ¼ç»“æž„å’Œè‡ªç„¶è¯­è¨€è¯­ä¹‰
   - çŽ°æœ‰æ–¹æ³•éƒ½å­˜åœ¨æ˜¾è‘—ä¸è¶³

2. **æ•°æ®é›†è®¾è®¡åˆç†ä¸”å…¨é¢**
   - è¦†ç›–å¤šç§æŸ¥è¯¢ç±»åž‹å’Œæ¡ä»¶ç±»åˆ«
   - æä¾›äº†ä¸°å¯Œçš„è¯„ä¼°ç»´åº¦

3. **æœªæ¥ç ”ç©¶æ–¹å‘æ˜Žç¡®**
   - è¯­ä¹‰ç†è§£æ˜¯å…³é”®
   - éœ€è¦ä¸“é—¨ä¸ºnlcTDè®¾è®¡çš„æ–°æ–¹æ³•

4. **å®žéªŒè®¾ç½®ä¸¥è°¨**
   - å¤šç»´åº¦è¯„ä¼°ï¼ˆæ•´ä½“æ€§èƒ½ã€æ¡ä»¶ç±»åž‹ã€æ•°æ®ç»„æˆï¼‰
   - æä¾›äº†clear baselineå’Œæ”¹è¿›ç©ºé—´

## ðŸŽ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®

1. **ç«‹å³å¼€å§‹**ï¼šæ•°æ®è½¬æ¢å’ŒåŸºçº¿æµ‹è¯•ï¼ˆ1-2å¤©ï¼‰
2. **ç³»ç»Ÿé€‚é…**ï¼šå°†NLæ¡ä»¶é›†æˆåˆ°ä¸‰å±‚æž¶æž„ï¼ˆ3-5å¤©ï¼‰
3. **é’ˆå¯¹æ€§ä¼˜åŒ–**ï¼šåŸºäºŽè®ºæ–‡å‘çŽ°æ”¹è¿›å¼±ç‚¹ï¼ˆ5-7å¤©ï¼‰
4. **è®ºæ–‡æ’°å†™**ï¼šå¯¹æ¯”è®ºæ–‡ç»“æžœï¼Œå±•ç¤ºæ”¹è¿›ï¼ˆ2-3å¤©ï¼‰

è®ºæ–‡GitHub: https://github.com/SuDIS-ZJU/nlcTables