# æ•°æ®æ¹–ç³»ç»Ÿå¹¶è¡Œæ¶æ„ä¸å®éªŒæŒ‡å—
Data Lake System Parallelization Architecture & Experiment Guide

## ğŸš€ ç³»ç»Ÿå¹¶è¡Œå¤„ç†èƒ½åŠ›

### âœ… æ˜¯çš„ï¼Œæ‚¨çš„ç³»ç»Ÿæ”¯æŒå¤§è§„æ¨¡å¹¶è¡Œå¤„ç†ï¼

æ‚¨çš„å¤šæ™ºèƒ½ä½“æ•°æ®æ¹–ç³»ç»Ÿå…·å¤‡ä»¥ä¸‹å¹¶è¡Œå¤„ç†èƒ½åŠ›ï¼š

### 1. **å¤šå±‚æ¬¡å¹¶è¡Œæ¶æ„**

#### 1.1 Agentçº§å¹¶è¡Œ
- **6ä¸ªæ™ºèƒ½ä½“ååŒå·¥ä½œ**ï¼šOptimizerAgentã€PlannerAgentã€AnalyzerAgentã€SearcherAgentã€MatcherAgentã€AggregatorAgent
- **LangGraphçŠ¶æ€æœºç¼–æ’**ï¼šæ”¯æŒå¹¶å‘æ‰§è¡Œä¸å†²çªçš„Agentä»»åŠ¡

#### 1.2 æ•°æ®å¤„ç†å¹¶è¡Œ
```python
# OptimizerAgentåŠ¨æ€é…ç½®å¹¶è¡Œå‚æ•°
config.parallel_workers:
  - å°è§„æ¨¡ (<100è¡¨): 4 workers
  - ä¸­è§„æ¨¡ (100-500è¡¨): 8 workers  
  - å¤§è§„æ¨¡ (500-1000è¡¨): 12 workers
  - è¶…å¤§è§„æ¨¡ (>1000è¡¨): 16 workers

config.llm_concurrency: 3-5 å¹¶å‘LLMè°ƒç”¨
config.batch_size: 5-20 æ‰¹å¤„ç†å¤§å°
```

#### 1.3 æœç´¢å±‚å¹¶è¡Œ
- **Layer 1 (Metadata Filter)**: å¹¶è¡Œå…ƒæ•°æ®ç­›é€‰
- **Layer 2 (Vector Search)**: FAISSå‘é‡æœç´¢ï¼ˆGPUåŠ é€Ÿå¯é€‰ï¼‰
- **Layer 3 (LLM Matcher)**: æ‰¹é‡å¹¶è¡ŒLLMéªŒè¯

### 2. **æ€§èƒ½ä¼˜åŒ–è®¾è®¡**

#### 2.1 ç¼“å­˜ç­–ç•¥
- **L1ç¼“å­˜**: å†…å­˜ç¼“å­˜ï¼ˆ<100è¡¨ï¼‰
- **L2ç¼“å­˜**: Redisç¼“å­˜ï¼ˆ100-1000è¡¨ï¼‰
- **L3ç¼“å­˜**: æŒä¹…åŒ–ç¼“å­˜ï¼ˆ>1000è¡¨ï¼‰

#### 2.2 æ‰¹å¤„ç†ä¼˜åŒ–
```python
# LLMMatcherTool.batch_verify()
async def batch_verify(candidates, max_concurrent=10):
    # å¹¶è¡Œå¤„ç†å¤šä¸ªå€™é€‰è¡¨
    # æ”¯æŒ10ä¸ªå¹¶å‘LLMè°ƒç”¨
```

## ğŸ“Š å¦‚ä½•è¿è¡Œå®éªŒ

### 1. **å‡†å¤‡æ‚¨çš„æ•°æ®**

#### 1.1 è¡¨æ•°æ®æ ¼å¼ (`your_tables.json`)
```json
[
  {
    "table_name": "your_table_name",
    "columns": [
      {
        "column_name": "column1",
        "data_type": "string|numeric|date",
        "sample_values": ["value1", "value2", "value3"]
      },
      {
        "column_name": "column2",
        "data_type": "numeric",
        "sample_values": ["100", "200", "300"]
      }
    ],
    "row_count": 1000,
    "column_count": 10
  }
]
```

#### 1.2 æŸ¥è¯¢ä»»åŠ¡æ ¼å¼ (`your_queries.json`)
```json
[
  {
    "query_table": "table_to_match",
    "query_type": "join",  // æˆ– "union"
    "query_id": "q001"
  }
]
```

#### 1.3 Ground Truthæ ¼å¼ (`your_ground_truth.json`)
```json
[
  {
    "query_table": "table_to_match",
    "candidate_table": "matching_table_1",
    "label": 1
  }
]
```

### 2. **è¿è¡Œå®éªŒ**

#### æ–¹æ³•1: ä½¿ç”¨ä¸»å®éªŒè„šæœ¬
```bash
# å°†æ‚¨çš„æ•°æ®æ”¾åœ¨examplesæ–‡ä»¶å¤¹
cp your_tables.json examples/custom_tables.json
cp your_queries.json examples/custom_queries.json
cp your_ground_truth.json examples/custom_ground_truth.json

# è¿è¡Œå®éªŒ
python run_langgraph_system.py \
  --dataset custom \
  --task join \
  --max-queries 100 \
  --output results/my_experiment.json
```

#### æ–¹æ³•2: ç¼–ç¨‹è°ƒç”¨
```python
from src.core.langgraph_workflow import DataLakeDiscoveryWorkflow
import json

# åŠ è½½æ‚¨çš„æ•°æ®
with open('your_tables.json', 'r') as f:
    tables = json.load(f)

with open('your_queries.json', 'r') as f:
    queries = json.load(f)

# åˆ›å»ºå·¥ä½œæµ
workflow = DataLakeDiscoveryWorkflow()

# è¿è¡Œå®éªŒ
results = []
for query in queries:
    result = workflow.run(
        query=f"Find tables that can {query['query_type']} with {query['query_table']}",
        tables=tables,
        task_type=query['query_type'],
        query_table_name=query['query_table']
    )
    results.append(result)

# è¾“å‡ºè¯„ä»·æŒ‡æ ‡
print(f"æˆåŠŸç‡: {len([r for r in results if r['success']])/len(results)*100:.2f}%")
```

### 3. **è¯„ä»·æŒ‡æ ‡è¾“å‡º**

ç³»ç»Ÿè‡ªåŠ¨è®¡ç®—ä»¥ä¸‹æŒ‡æ ‡ï¼š

#### 3.1 åŸºç¡€æŒ‡æ ‡
- **Precision**: ç²¾ç¡®ç‡
- **Recall**: å¬å›ç‡  
- **F1-Score**: F1åˆ†æ•°
- **Success Rate**: æŸ¥è¯¢æˆåŠŸç‡

#### 3.2 æ’åæŒ‡æ ‡
- **Hit@1**: Top-1å‘½ä¸­ç‡
- **Hit@3**: Top-3å‘½ä¸­ç‡
- **Hit@5**: Top-5å‘½ä¸­ç‡
- **Hit@10**: Top-10å‘½ä¸­ç‡

#### 3.3 æ€§èƒ½æŒ‡æ ‡
- **Query Time**: å¹³å‡æŸ¥è¯¢æ—¶é—´
- **Throughput**: ååé‡(QPS)
- **Resource Usage**: èµ„æºä½¿ç”¨ç‡

### 4. **å¤§è§„æ¨¡æ•°æ®å¤„ç†å»ºè®®**

#### 4.1 æ•°æ®åˆ†ç‰‡
```python
# å¯¹äºè¶…å¤§è§„æ¨¡æ•°æ®ï¼ˆ>10000è¡¨ï¼‰ï¼Œå»ºè®®åˆ†ç‰‡å¤„ç†
def process_large_dataset(tables, chunk_size=1000):
    chunks = [tables[i:i+chunk_size] 
              for i in range(0, len(tables), chunk_size)]
    
    for chunk_id, chunk in enumerate(chunks):
        # å¤„ç†æ¯ä¸ªåˆ†ç‰‡
        process_chunk(chunk, chunk_id)
```

#### 4.2 æ€§èƒ½è°ƒä¼˜
```yaml
# config.yml - é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®çš„ä¼˜åŒ–é…ç½®
optimization:
  parallel_workers: 16        # æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹
  llm_concurrency: 5          # LLMå¹¶å‘æ•°
  batch_size: 20             # æ‰¹å¤„ç†å¤§å°
  cache_level: "L3"          # ä½¿ç”¨æŒä¹…åŒ–ç¼“å­˜
  vector_search:
    use_gpu: true            # å¯ç”¨GPUåŠ é€Ÿ
    index_type: "IVF"        # ä½¿ç”¨å€’æ’ç´¢å¼•
    nprobe: 32               # æœç´¢æ¢é’ˆæ•°
```

#### 4.3 åˆ†å¸ƒå¼æ‰©å±•ï¼ˆæœªæ¥æ”¯æŒï¼‰
```python
# è®¡åˆ’ä¸­çš„åˆ†å¸ƒå¼æ¶æ„
# - Ray/Daskåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶
# - Kuberneteså®¹å™¨ç¼–æ’
# - å¤šèŠ‚ç‚¹ååŒå¤„ç†
```

## ğŸ¯ æ€§èƒ½åŸºå‡†

### å½“å‰ç³»ç»Ÿæ€§èƒ½ï¼ˆå•æœºï¼‰
| æ•°æ®è§„æ¨¡ | æŸ¥è¯¢æ—¶é—´ | ååé‡ | å‡†ç¡®ç‡ |
|---------|---------|--------|--------|
| 100è¡¨ | 2-8ç§’ | 0.4-0.7 QPS | 85% |
| 1,000è¡¨ | 8-20ç§’ | 0.05-0.1 QPS | 80% |
| 10,000è¡¨ | 30-60ç§’ | 0.02 QPS | 75% |

### å¹¶è¡Œä¼˜åŒ–åç›®æ ‡
| æ•°æ®è§„æ¨¡ | æŸ¥è¯¢æ—¶é—´ | ååé‡ | å‡†ç¡®ç‡ |
|---------|---------|--------|--------|
| 100è¡¨ | 1-3ç§’ | 1-2 QPS | 90% |
| 1,000è¡¨ | 3-8ç§’ | 0.2-0.5 QPS | 90% |
| 10,000è¡¨ | 8-15ç§’ | 0.1 QPS | 85% |
| 100,000è¡¨ | 30-60ç§’ | 0.02 QPS | 80% |

## ğŸ“ å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

```bash
# 1. å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼‰
cd /root/dataLakesMulti

# 2. è¿è¡Œå°è§„æ¨¡æµ‹è¯•
python run_langgraph_system.py \
  --dataset subset \
  --task join \
  --max-queries 5

# 3. æŸ¥çœ‹ç»“æœ
cat experiment_results/langgraph_*.json | python -m json.tool

# 4. è¿è¡Œæ‚¨è‡ªå·±çš„æ•°æ®
# å°†æ‚¨çš„æ•°æ®è½¬æ¢ä¸ºæ‰€éœ€æ ¼å¼å
python run_langgraph_system.py \
  --dataset custom \
  --task both \
  --max-queries 100 \
  --output results/my_experiment.json
```

## ğŸ”§ ç›‘æ§ä¸è°ƒè¯•

### æŸ¥çœ‹å¹¶è¡Œæ‰§è¡ŒçŠ¶æ€
```bash
# å®æ—¶ç›‘æ§
watch -n 1 'ps aux | grep python | grep -E "worker|agent"'

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/langgraph_*.log

# æ€§èƒ½åˆ†æ
python -m cProfile -o profile.stats run_langgraph_system.py
```

## ğŸ’¡ æœ€ä½³å®è·µ

1. **æ•°æ®é¢„å¤„ç†**ï¼šå¯¹å¤§è§„æ¨¡æ•°æ®è¿›è¡Œé¢„ç´¢å¼•
2. **æ‰¹é‡æŸ¥è¯¢**ï¼šä½¿ç”¨æ‰¹å¤„ç†è€Œéå•ä¸ªæŸ¥è¯¢
3. **ç¼“å­˜åˆ©ç”¨**ï¼šå¯ç”¨å¤šçº§ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—
4. **èµ„æºç›‘æ§**ï¼šç›‘æ§å†…å­˜å’ŒCPUä½¿ç”¨ç‡
5. **å¢é‡å¤„ç†**ï¼šå¯¹æ–°å¢æ•°æ®ä½¿ç”¨å¢é‡ç´¢å¼•

## ğŸš§ æ³¨æ„äº‹é¡¹

1. **å†…å­˜éœ€æ±‚**ï¼šå»ºè®®è‡³å°‘16GB RAMï¼ˆå¤§è§„æ¨¡æ•°æ®éœ€32GB+ï¼‰
2. **APIé™åˆ¶**ï¼šæ³¨æ„LLM APIè°ƒç”¨é¢‘ç‡é™åˆ¶
3. **æ•°æ®è´¨é‡**ï¼šç¡®ä¿sample_valueså…·æœ‰ä»£è¡¨æ€§
4. **Ground Truth**ï¼šå‡†ç¡®çš„æ ‡æ³¨æ•°æ®å¯¹è¯„ä¼°è‡³å…³é‡è¦

---

**æ€»ç»“**ï¼šæ‚¨çš„ç³»ç»Ÿå·²ç»å…·å¤‡å¼ºå¤§çš„å¹¶è¡Œå¤„ç†èƒ½åŠ›ï¼Œå¯ä»¥å¤„ç†å¤§è§„æ¨¡æ•°æ®æ¹–åœºæ™¯ã€‚æŒ‰ç…§ä¸Šè¿°æŒ‡å—å‡†å¤‡æ•°æ®å¹¶è¿è¡Œå®éªŒï¼Œå³å¯è·å¾—å®Œæ•´çš„è¯„ä»·æŒ‡æ ‡ã€‚å¦‚éœ€è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½ï¼Œå¯ä»¥è°ƒæ•´å¹¶è¡Œå‚æ•°å’Œç¼“å­˜ç­–ç•¥ã€‚