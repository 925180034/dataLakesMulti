#!/usr/bin/env python
"""
ä¼˜åŒ–çš„çœŸå®åµŒå…¥ç”Ÿæˆå™¨ - è§£å†³å¤šè¿›ç¨‹é‡å¤åˆå§‹åŒ–é—®é¢˜
æ ¸å¿ƒé—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼š
1. é—®é¢˜ï¼š64ä¸ªè¿›ç¨‹æ¯ä¸ªéƒ½åˆå§‹åŒ–SentenceTransformeræ¨¡å‹ï¼Œå¯¼è‡´å†…å­˜çˆ†ç‚¸å’Œåˆå§‹åŒ–æ—¶é—´æé•¿
   è§£å†³ï¼šé¢„å…ˆç”Ÿæˆæ‰€æœ‰åµŒå…¥å¹¶ç¼“å­˜ï¼Œè¿›ç¨‹åªè¯»å–ç¼“å­˜

2. é—®é¢˜ï¼šæ¯æ¬¡è°ƒç”¨éƒ½æ£€æŸ¥å’ŒåŠ è½½æ¨¡å‹
   è§£å†³ï¼šå•æ¬¡é¢„è®¡ç®—ï¼Œåç»­ç›´æ¥ä½¿ç”¨ç¼“å­˜

3. é—®é¢˜ï¼šæ‰¹å¤„ç†å¤§å°ä¸åˆç†
   è§£å†³ï¼šæ ¹æ®å†…å­˜åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°
"""
import os
import sys
import json
import pickle
import time
import logging
from pathlib import Path
import numpy as np
from typing import List, Dict, Optional, Tuple
from sentence_transformers import SentenceTransformer
import faiss
import gc

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class OptimizedRealEmbeddingGenerator:
    """ä¼˜åŒ–çš„çœŸå®åµŒå…¥ç”Ÿæˆå™¨ - å•æ¬¡åˆå§‹åŒ–ï¼Œæ‰¹é‡å¤„ç†"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = None
        self.dimension = 384  # all-MiniLM-L6-v2çš„ç»´åº¦
        
    def initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹ï¼ˆåªè°ƒç”¨ä¸€æ¬¡ï¼‰"""
        if self.model is not None:
            return
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–SentenceTransformeræ¨¡å‹: {self.model_name}")
        start_time = time.time()
        
        try:
            # è®¾ç½®ç¼“å­˜ç›®å½•
            cache_folder = "/root/.cache/huggingface/hub"
            os.makedirs(cache_folder, exist_ok=True)
            
            # åŠ è½½æ¨¡å‹
            self.model = SentenceTransformer(
                self.model_name, 
                cache_folder=cache_folder,
                device='cpu'  # æ˜ç¡®ä½¿ç”¨CPUé¿å…GPUå†…å­˜é—®é¢˜
            )
            
            elapsed = time.time() - start_time
            logger.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def generate_table_text(self, table: Dict) -> str:
        """ä¸ºè¡¨ç”Ÿæˆæ–‡æœ¬è¡¨ç¤ºï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        table_name = table.get('table_name', table.get('name', ''))
        columns = table.get('columns', [])
        
        # æ„å»ºæ–‡æœ¬è¡¨ç¤º
        text_parts = [f"Table: {table_name}"]
        
        # æ·»åŠ åˆ—ä¿¡æ¯ï¼ˆé™åˆ¶æ•°é‡é¿å…è¿‡é•¿ï¼‰
        for col in columns[:20]:  # æœ€å¤š20åˆ—
            col_name = col.get('name', col.get('column_name', ''))
            col_type = col.get('type', col.get('data_type', ''))
            if col_name:
                text_parts.append(f"Column: {col_name}")
                if col_type:
                    text_parts[-1] += f" ({col_type})"
            
            # æ·»åŠ æ ·æœ¬å€¼ï¼ˆé™åˆ¶æ•°é‡ï¼‰
            sample_values = col.get('sample_values', [])
            if sample_values:
                samples = [str(v) for v in sample_values[:3] if v]
                if samples:
                    text_parts.append(f"  Values: {', '.join(samples)}")
        
        return ' | '.join(text_parts)
    
    def batch_generate_embeddings(self, tables: List[Dict], batch_size: int = 32) -> Tuple[np.ndarray, List[str]]:
        """æ‰¹é‡ç”ŸæˆåµŒå…¥ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰"""
        # åˆå§‹åŒ–æ¨¡å‹
        self.initialize_model()
        
        # ç”Ÿæˆæ–‡æœ¬è¡¨ç¤º
        logger.info("ğŸ“ ç”Ÿæˆè¡¨æ–‡æœ¬è¡¨ç¤º...")
        texts = []
        table_names = []
        
        for table in tables:
            text = self.generate_table_text(table)
            texts.append(text)
            table_names.append(table.get('table_name', table.get('name', f'table_{len(table_names)}')))
        
        logger.info(f"ğŸ“Š æ‰¹é‡ç”Ÿæˆ {len(texts)} ä¸ªåµŒå…¥å‘é‡...")
        start_time = time.time()
        
        # æ‰¹é‡ç¼–ç 
        all_embeddings = []
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            
            # ç”ŸæˆåµŒå…¥
            batch_embeddings = self.model.encode(
                batch_texts,
                show_progress_bar=False,  # å…³é—­è¿›åº¦æ¡é¿å…è¾“å‡ºæ··ä¹±
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            
            all_embeddings.append(batch_embeddings)
            
            # è¿›åº¦æŠ¥å‘Š
            progress = min(i + batch_size, len(texts))
            elapsed = time.time() - start_time
            speed = progress / elapsed if elapsed > 0 else 0
            eta = (len(texts) - progress) / speed if speed > 0 else 0
            
            logger.info(f"  è¿›åº¦: {progress}/{len(texts)} ({progress*100/len(texts):.1f}%) "
                       f"é€Ÿåº¦: {speed:.1f} è¡¨/ç§’, å‰©ä½™: {eta:.0f}ç§’")
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if i % (batch_size * 10) == 0:
                gc.collect()
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        embeddings_array = np.vstack(all_embeddings)
        
        elapsed = time.time() - start_time
        logger.info(f"âœ… åµŒå…¥ç”Ÿæˆå®Œæˆ: {len(texts)} ä¸ªè¡¨, è€—æ—¶: {elapsed:.2f}ç§’ "
                   f"({len(texts)/elapsed:.1f} è¡¨/ç§’)")
        
        return embeddings_array, table_names


def precompute_embeddings_for_dataset(dataset_name: str, dataset_type: str = 'subset',
                                     task_type: str = 'join', force_regenerate: bool = False):
    """ä¸ºæ•´ä¸ªæ•°æ®é›†é¢„è®¡ç®—åµŒå…¥"""
    logger.info("="*80)
    logger.info(f"ğŸš€ é¢„è®¡ç®—çœŸå®åµŒå…¥: {dataset_name}/{task_type}_{dataset_type}")
    logger.info("="*80)
    
    # æ„å»ºè·¯å¾„
    data_path = Path("examples") / dataset_name / f"{task_type}_{dataset_type}" / "tables.json"
    
    if not data_path.exists():
        logger.error(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {data_path}")
        return False
    
    # åŠ è½½æ•°æ®
    with open(data_path, 'r') as f:
        tables = json.load(f)
    logger.info(f"ğŸ“Š åŠ è½½äº† {len(tables)} ä¸ªè¡¨")
    
    # ç¼“å­˜è·¯å¾„
    cache_dir = Path("cache") / dataset_name
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    index_file = cache_dir / f"vector_index_{len(tables)}.pkl"
    embeddings_file = cache_dir / f"table_embeddings_{len(tables)}.pkl"
    
    # æ£€æŸ¥ç¼“å­˜
    if not force_regenerate and index_file.exists() and embeddings_file.exists():
        logger.info(f"âœ… ç¼“å­˜å·²å­˜åœ¨: {cache_dir}")
        logger.info(f"   ç´¢å¼•æ–‡ä»¶: {index_file.stat().st_size / (1024*1024):.2f} MB")
        logger.info(f"   åµŒå…¥æ–‡ä»¶: {embeddings_file.stat().st_size / (1024*1024):.2f} MB")
        return True
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = OptimizedRealEmbeddingGenerator()
    
    # æ ¹æ®æ•°æ®è§„æ¨¡è°ƒæ•´æ‰¹å¤„ç†å¤§å°
    if len(tables) > 1000:
        batch_size = 16  # å¤§æ•°æ®é›†ç”¨å°æ‰¹æ¬¡
    elif len(tables) > 500:
        batch_size = 32
    else:
        batch_size = 64  # å°æ•°æ®é›†å¯ä»¥ç”¨å¤§æ‰¹æ¬¡
    
    logger.info(f"âš™ï¸ ä½¿ç”¨æ‰¹å¤„ç†å¤§å°: {batch_size}")
    
    # ç”ŸæˆåµŒå…¥
    embeddings_array, table_names = generator.batch_generate_embeddings(tables, batch_size)
    
    # æ„å»ºFAISSç´¢å¼•
    logger.info("ğŸ“Š æ„å»ºFAISSç´¢å¼•...")
    dimension = embeddings_array.shape[1]
    
    if len(tables) < 1000:
        # å°æ•°æ®é›†ï¼šä½¿ç”¨FlatL2ï¼ˆç²¾ç¡®æœç´¢ï¼‰
        index = faiss.IndexFlatL2(dimension)
        logger.info("  ä½¿ç”¨FlatL2ç´¢å¼•ï¼ˆç²¾ç¡®æœç´¢ï¼‰")
    else:
        # å¤§æ•°æ®é›†ï¼šä½¿ç”¨HNSWï¼ˆè¿‘ä¼¼æœç´¢ï¼Œæ›´å¿«ï¼‰
        M = 32  # HNSWçš„è¿æ¥æ•°
        index = faiss.IndexHNSWFlat(dimension, M)
        index.hnsw.efConstruction = 100  # æ„å»ºæ—¶çš„æœç´¢å®½åº¦
        index.hnsw.efSearch = 50  # æŸ¥è¯¢æ—¶çš„æœç´¢å®½åº¦
        logger.info(f"  ä½¿ç”¨HNSWç´¢å¼•ï¼ˆè¿‘ä¼¼æœç´¢ï¼ŒM={M}ï¼‰")
    
    # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
    index.add(embeddings_array)
    logger.info(f"  æ·»åŠ äº† {len(embeddings_array)} ä¸ªå‘é‡åˆ°ç´¢å¼•")
    
    # åˆ›å»ºåµŒå…¥å­—å…¸
    embeddings_dict = {}
    for i, name in enumerate(table_names):
        embeddings_dict[name] = embeddings_array[i]
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    logger.info("ğŸ’¾ ä¿å­˜ç¼“å­˜...")
    
    with open(index_file, 'wb') as f:
        pickle.dump(index, f)
    logger.info(f"  ç´¢å¼•ä¿å­˜åˆ°: {index_file} ({index_file.stat().st_size / (1024*1024):.2f} MB)")
    
    with open(embeddings_file, 'wb') as f:
        pickle.dump(embeddings_dict, f)
    logger.info(f"  åµŒå…¥ä¿å­˜åˆ°: {embeddings_file} ({embeddings_file.stat().st_size / (1024*1024):.2f} MB)")
    
    # æ¸…ç†å†…å­˜
    del generator
    gc.collect()
    
    logger.info("âœ… é¢„è®¡ç®—å®Œæˆï¼")
    return True


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–çš„çœŸå®åµŒå…¥ç”Ÿæˆå™¨')
    parser.add_argument('--dataset', required=True, 
                       choices=['nlctables', 'opendata', 'webtable'],
                       help='æ•°æ®é›†åç§°')
    parser.add_argument('--type', default='subset',
                       choices=['subset', 'complete'],
                       help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--task', default='join',
                       choices=['join', 'union', 'both'],
                       help='ä»»åŠ¡ç±»å‹')
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰')
    
    args = parser.parse_args()
    
    # å¤„ç†ä»»åŠ¡
    tasks = ['join', 'union'] if args.task == 'both' else [args.task]
    
    for task in tasks:
        success = precompute_embeddings_for_dataset(
            args.dataset,
            args.type,
            task,
            args.force
        )
        
        if not success:
            logger.error(f"âŒ {args.dataset}/{task}_{args.type} å¤„ç†å¤±è´¥")
            sys.exit(1)
    
    logger.info("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")


if __name__ == "__main__":
    main()