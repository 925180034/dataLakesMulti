#!/usr/bin/env python
"""
é¢„è®¡ç®—å‘é‡åµŒå…¥
ä¸€æ¬¡æ€§ä¸ºæ‰€æœ‰è¡¨ç”Ÿæˆå‘é‡åµŒå…¥å¹¶ä¿å­˜
é¿å…æ¯æ¬¡æŸ¥è¯¢éƒ½é‡æ–°ç”Ÿæˆ
"""
import os
import sys
import json
import pickle
import time
import logging
from pathlib import Path
import numpy as np
import faiss

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def precompute_dataset_embeddings(dataset_type='complete', task_type='join'):
    """
    ä¸ºæŒ‡å®šæ•°æ®é›†é¢„è®¡ç®—æ‰€æœ‰å‘é‡åµŒå…¥
    """
    logger.info("="*80)
    logger.info(f"é¢„è®¡ç®—å‘é‡åµŒå…¥: {task_type.upper()} - {dataset_type.upper()}")
    logger.info("="*80)
    
    # åŠ è½½æ•°æ®
    base_dir = Path(f"examples/separated_datasets/{task_type}_{dataset_type}")
    
    with open(base_dir / "tables.json", 'r') as f:
        tables = json.load(f)
    
    logger.info(f"åŠ è½½ {len(tables)} ä¸ªè¡¨")
    
    # åˆ›å»ºç¼“å­˜ç›®å½•
    cache_dir = Path("cache") / dataset_type
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    # ç¼“å­˜æ–‡ä»¶è·¯å¾„
    index_file = cache_dir / f"vector_index_{len(tables)}.pkl"
    embeddings_file = cache_dir / f"table_embeddings_{len(tables)}.pkl"
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if index_file.exists() and embeddings_file.exists():
        logger.info(f"å‘é‡ç´¢å¼•å·²å­˜åœ¨: {index_file}")
        logger.info("å¦‚éœ€é‡æ–°ç”Ÿæˆï¼Œè¯·åˆ é™¤ç¼“å­˜æ–‡ä»¶")
        return
    
    # åˆå§‹åŒ–åµŒå…¥ç”Ÿæˆå™¨
    from src.tools.batch_embedding import BatchEmbeddingGenerator
    
    logger.info("åˆå§‹åŒ–åµŒå…¥ç”Ÿæˆå™¨...")
    generator = BatchEmbeddingGenerator()
    generator.initialize()
    
    # å‡†å¤‡æ–‡æœ¬
    logger.info("å‡†å¤‡è¡¨æ–‡æœ¬...")
    table_texts = []
    table_names = []
    
    for table in tables:
        # åˆ›å»ºè¡¨çš„æ–‡æœ¬è¡¨ç¤º
        table_name = table.get('table_name', table.get('name', ''))
        table_names.append(table_name)
        
        # ç»„åˆè¡¨åå’Œåˆ—ä¿¡æ¯
        text_parts = [f"Table: {table_name}"]
        
        columns = table.get('columns', [])
        for col in columns:
            col_name = col.get('name', '')
            col_type = col.get('type', '')
            text_parts.append(f"Column: {col_name} ({col_type})")
            
            # æ·»åŠ æ ·æœ¬å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
            sample_values = col.get('sample_values', [])
            if sample_values:
                values_str = ', '.join(str(v) for v in sample_values[:3])
                text_parts.append(f"  Samples: {values_str}")
        
        table_text = ' '.join(text_parts)
        table_texts.append(table_text)
    
    logger.info(f"å‡†å¤‡äº† {len(table_texts)} ä¸ªè¡¨æ–‡æœ¬")
    
    # æ‰¹é‡ç”ŸæˆåµŒå…¥
    logger.info("å¼€å§‹æ‰¹é‡ç”ŸæˆåµŒå…¥...")
    start_time = time.time()
    
    # æ‰¹é‡ç”Ÿæˆï¼Œæ¯æ‰¹32ä¸ª
    batch_size = 32
    all_embeddings = []
    
    for i in range(0, len(table_texts), batch_size):
        batch = table_texts[i:i+batch_size]
        batch_embeddings = generator.generate_batch(batch, batch_size=batch_size)
        all_embeddings.extend(batch_embeddings)
        
        progress = min(i + batch_size, len(table_texts))
        logger.info(f"  è¿›åº¦: {progress}/{len(table_texts)} ({progress*100/len(table_texts):.1f}%)")
    
    elapsed = time.time() - start_time
    logger.info(f"åµŒå…¥ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
    logger.info(f"å¹³å‡é€Ÿåº¦: {len(table_texts)/elapsed:.1f} ä¸ªè¡¨/ç§’")
    
    # åˆ›å»ºåµŒå…¥å­—å…¸
    logger.info("åˆ›å»ºåµŒå…¥å­—å…¸...")
    embeddings_dict = {}
    for i, name in enumerate(table_names):
        embeddings_dict[name] = all_embeddings[i]
    
    # æ„å»ºFAISSç´¢å¼•
    logger.info("æ„å»ºFAISS HNSWç´¢å¼•...")
    embeddings_array = np.array(all_embeddings).astype('float32')
    dimension = embeddings_array.shape[1]
    
    # ä½¿ç”¨HNSWç´¢å¼•ï¼ˆæ€§èƒ½æœ€ä¼˜ï¼‰
    index = faiss.IndexHNSWFlat(dimension, 32)
    index.hnsw.efConstruction = 200
    index.hnsw.efSearch = 160
    
    # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
    index.add(embeddings_array)
    
    logger.info(f"HNSWç´¢å¼•æ„å»ºå®Œæˆ: {len(tables)} ä¸ªå‘é‡ï¼Œç»´åº¦: {dimension}")
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    logger.info("ä¿å­˜ç´¢å¼•å’ŒåµŒå…¥...")
    
    with open(index_file, 'wb') as f:
        pickle.dump(index, f)
    logger.info(f"  ç´¢å¼•å·²ä¿å­˜: {index_file}")
    
    with open(embeddings_file, 'wb') as f:
        pickle.dump(embeddings_dict, f)
    logger.info(f"  åµŒå…¥å·²ä¿å­˜: {embeddings_file}")
    
    # éªŒè¯
    logger.info("éªŒè¯ä¿å­˜çš„æ–‡ä»¶...")
    
    with open(index_file, 'rb') as f:
        loaded_index = pickle.load(f)
    with open(embeddings_file, 'rb') as f:
        loaded_embeddings = pickle.load(f)
    
    logger.info(f"  éªŒè¯æˆåŠŸ: ç´¢å¼•åŒ…å« {loaded_index.ntotal} ä¸ªå‘é‡")
    logger.info(f"  éªŒè¯æˆåŠŸ: åµŒå…¥å­—å…¸åŒ…å« {len(loaded_embeddings)} ä¸ªè¡¨")
    
    logger.info("\nâœ… å‘é‡é¢„è®¡ç®—å®Œæˆï¼")
    logger.info(f"ç¼“å­˜ä½ç½®: {cache_dir}")
    
    return str(index_file), str(embeddings_file)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='é¢„è®¡ç®—å‘é‡åµŒå…¥')
    parser.add_argument('--dataset', 
                       choices=['subset', 'complete', 'both'],
                       default='both',
                       help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--task',
                       choices=['join', 'union', 'both'],
                       default='both',
                       help='ä»»åŠ¡ç±»å‹')
    
    args = parser.parse_args()
    
    # ç¡®å®šè¦å¤„ç†çš„æ•°æ®é›†å’Œä»»åŠ¡
    datasets = ['subset', 'complete'] if args.dataset == 'both' else [args.dataset]
    tasks = ['join', 'union'] if args.task == 'both' else [args.task]
    
    logger.info("="*80)
    logger.info("æ‰¹é‡é¢„è®¡ç®—å‘é‡åµŒå…¥")
    logger.info("="*80)
    
    for dataset in datasets:
        for task in tasks:
            try:
                logger.info(f"\nå¤„ç†: {task}_{dataset}")
                precompute_dataset_embeddings(dataset, task)
            except Exception as e:
                logger.error(f"å¤„ç† {task}_{dataset} å¤±è´¥: {e}")
                continue
    
    logger.info("\nâœ… æ‰€æœ‰æ•°æ®é›†å¤„ç†å®Œæˆï¼")


def precompute_all_embeddings(tables, dataset_type):
    """
    é¢„è®¡ç®—æ‰€æœ‰è¡¨çš„åµŒå…¥å‘é‡å¹¶ä¿å­˜
    ä¾›å…¶ä»–è„šæœ¬è°ƒç”¨çš„æ¥å£å‡½æ•°
    """
    cache_dir = Path("cache") / dataset_type
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶
    index_file = cache_dir / f"vector_index_{len(tables)}.pkl"
    embeddings_file = cache_dir / f"table_embeddings_{len(tables)}.pkl"
    
    # å¦‚æœç¼“å­˜å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
    if index_file.exists() and embeddings_file.exists():
        logger.info(f"âœ… ä½¿ç”¨å·²å­˜åœ¨çš„åµŒå…¥ç¼“å­˜: {cache_dir}")
        return
    
    logger.info(f"âš™ï¸ ç”Ÿæˆæ–°çš„åµŒå…¥å‘é‡ ({len(tables)} ä¸ªè¡¨)...")
    
    # é»˜è®¤ä½¿ç”¨çœŸå®åµŒå…¥ï¼ˆé™¤éæ˜ç¡®ç¦ç”¨ï¼‰
    use_real_embeddings = os.environ.get('USE_REAL_EMBEDDINGS', 'true').lower() != 'false'
    
    if use_real_embeddings:
        # ä½¿ç”¨ä¼˜åŒ–çš„çœŸå®åµŒå…¥ç”Ÿæˆå™¨
        logger.info("ğŸ“Š ä½¿ç”¨çœŸå®SentenceTransformeræ¨¡å‹...")
        from optimized_real_embedding import OptimizedRealEmbeddingGenerator
        
        generator = OptimizedRealEmbeddingGenerator()
        embeddings_array, table_names = generator.batch_generate_embeddings(tables, batch_size=32)
        
        # æ„å»ºåµŒå…¥å­—å…¸
        embeddings_data = {}
        for i, name in enumerate(table_names):
            embeddings_data[name] = embeddings_array[i]
    else:
        # ä½¿ç”¨è™šæ‹ŸåµŒå…¥ï¼ˆä»…ç”¨äºå¿«é€Ÿæµ‹è¯•ï¼Œä¸åº”ç”¨äºç”Ÿäº§ï¼‰
        logger.info("âš ï¸ è­¦å‘Šï¼šä½¿ç”¨è™šæ‹ŸåµŒå…¥ï¼ˆæ— è¯­ä¹‰æ„ä¹‰ï¼Œä»…ç”¨äºæµ‹è¯•ï¼‰...")
        embeddings_data = {}
        for table in tables:
            table_name = table.get('table_name', '')
            columns = table.get('columns', [])
            
            # åˆ›å»ºè¡¨çš„æ–‡æœ¬è¡¨ç¤º
            column_names = [col['name'] for col in columns]
            table_text = f"{table_name} {' '.join(column_names)}"
            
            # ä½¿ç”¨ç®€å•çš„è™šæ‹ŸåµŒå…¥
            import hashlib
            hash_obj = hashlib.md5(table_text.encode())
            hash_bytes = hash_obj.digest()
            # è½¬æ¢ä¸º384ç»´å‘é‡
            embedding = []
            for i in range(384):
                byte_idx = i % len(hash_bytes)
                embedding.append(float(hash_bytes[byte_idx]) / 255.0)
            
            embeddings_data[table_name] = embedding
    
    # ä¿å­˜åµŒå…¥
    import pickle
    embeddings_file = cache_dir / f"table_embeddings_{len(tables)}.pkl"
    with open(embeddings_file, 'wb') as f:
        pickle.dump(embeddings_data, f)
    
    # åˆ›å»ºFAISSç´¢å¼•
    import numpy as np
    try:
        import faiss
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        table_names = list(embeddings_data.keys())
        embeddings_array = np.array(list(embeddings_data.values()), dtype=np.float32)
        
        # åˆ›å»ºFAISSç´¢å¼•
        dimension = embeddings_array.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings_array)
        
        # ä¿å­˜ç´¢å¼•
        index_file = cache_dir / f"vector_index_{len(tables)}.pkl"
        with open(index_file, 'wb') as f:
            pickle.dump({'index': index, 'table_names': table_names}, f)
        
        print(f"âœ… åˆ›å»ºå‘é‡ç´¢å¼•: {index_file}")
    except ImportError:
        print("âš ï¸ FAISSæœªå®‰è£…ï¼Œè·³è¿‡ç´¢å¼•åˆ›å»º")
    
    return str(embeddings_file)

if __name__ == "__main__":
    main()