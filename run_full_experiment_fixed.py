#!/usr/bin/env python
"""
å®Œæ•´å®éªŒè¿è¡Œè„šæœ¬ï¼ˆå®Œå…¨ä¿®å¤ç‰ˆï¼‰
- ä½¿ç”¨æ­£ç¡®çš„queriesæ–‡ä»¶
- ç¡®ä¿LLMè¢«æ­£ç¡®è°ƒç”¨
- ä¿®å¤æ‰€æœ‰å·²çŸ¥bug
- ç”Ÿæˆå‡†ç¡®çš„è¯„ä»·æŒ‡æ ‡
"""

import json
import time
import asyncio
import numpy as np
from tabulate import tabulate
from typing import List, Dict, Any, Tuple
from collections import defaultdict
import os
import sys
import argparse

# è®¾ç½®ç¯å¢ƒå˜é‡ç¡®ä¿LLMå·¥ä½œ
os.environ['SKIP_LLM'] = 'false'  # ç¡®ä¿ä¸è·³è¿‡LLM
os.environ['LLM_TIMEOUT'] = '30'  # 30ç§’è¶…æ—¶ç»™LLMæ›´å¤šæ—¶é—´
os.environ['LLM_MAX_RETRIES'] = '3'  # 3æ¬¡é‡è¯•


def calculate_metrics(predictions: List[str], ground_truth: List[str]) -> Dict[str, float]:
    """è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°"""
    if not ground_truth:  # å¦‚æœæ²¡æœ‰ground truthï¼Œæ— æ³•è¯„ä¼°
        return {"precision": 0.0, "recall": 0.0, "f1": 0.0}
    
    if not predictions:  # å¦‚æœæ²¡æœ‰é¢„æµ‹ï¼Œå…¨éƒ¨ä¸º0
        return {"precision": 0.0, "recall": 0.0, "f1": 0.0}
    
    pred_set = set(predictions)
    truth_set = set(ground_truth)
    
    # è®¡ç®—äº¤é›†
    intersection = pred_set & truth_set
    
    # è®¡ç®—æŒ‡æ ‡
    precision = len(intersection) / len(pred_set) if pred_set else 0.0
    recall = len(intersection) / len(truth_set) if truth_set else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        "precision": precision,
        "recall": recall,
        "f1": f1
    }


async def run_full_experiment(dataset_type: str = "subset", max_queries: int = None, enable_llm: bool = True):
    """è¿è¡Œå®Œæ•´å®éªŒ - å®Œå…¨ä¿®å¤ç‰ˆ"""
    
    print("="*80)
    print(f"ğŸš€ æ•°æ®æ¹–å®Œæ•´å®éªŒç³»ç»Ÿï¼ˆå®Œå…¨ä¿®å¤ç‰ˆï¼‰")
    print(f"ğŸ“Š æ•°æ®é›†ç±»å‹: {dataset_type}")
    print(f"ğŸ“Š æœ€å¤§æŸ¥è¯¢æ•°: {max_queries if max_queries else 'å…¨éƒ¨'}")
    print(f"ğŸ¤– LLMçŠ¶æ€: {'å¯ç”¨' if enable_llm else 'ç¦ç”¨'}")
    print("="*80)
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    
    # é€‰æ‹©æ•°æ®é›†
    if dataset_type == "subset":
        tables_file = "examples/final_subset_tables.json"
        queries_file = "examples/final_subset_queries.json"
        ground_truth_file = "examples/final_subset_ground_truth_auto.json"
    else:
        tables_file = "examples/final_complete_tables.json"
        queries_file = "examples/final_complete_queries.json"
        ground_truth_file = "examples/final_complete_ground_truth_auto.json"
    
    # åŠ è½½è¡¨æ•°æ®
    with open(tables_file) as f:
        all_tables = json.load(f)
    
    # åŠ è½½æŸ¥è¯¢
    with open(queries_file) as f:
        all_queries = json.load(f)
    
    # åŠ è½½çœŸå®æ ‡ç­¾
    with open(ground_truth_file) as f:
        ground_truth_data = json.load(f)
    
    print(f"âœ… å·²åŠ è½½ {len(all_tables)} ä¸ªè¡¨")
    print(f"âœ… å·²åŠ è½½ {len(all_queries)} ä¸ªæŸ¥è¯¢")
    print(f"âœ… å·²åŠ è½½çœŸå®æ ‡ç­¾ ({len(ground_truth_data)}æ¡è®°å½•)")
    
    # é™åˆ¶æŸ¥è¯¢æ•°é‡
    if max_queries:
        all_queries = all_queries[:max_queries]
        print(f"ğŸ“Š é™åˆ¶æŸ¥è¯¢æ•°é‡ä¸º: {len(all_queries)}")
    
    # 2. åˆå§‹åŒ–å·¥ä½œæµ
    print("\nâš™ï¸ åˆå§‹åŒ–è¶…ä¼˜åŒ–å·¥ä½œæµ...")
    from src.core.models import TableInfo, ColumnInfo, AgentState
    
    # è½¬æ¢è¡¨æ•°æ®
    table_infos = []
    table_name_to_info = {}
    for table_data in all_tables:
        table_info = TableInfo(
            table_name=table_data['table_name'],
            columns=[
                ColumnInfo(
                    table_name=table_data['table_name'],
                    column_name=col.get('column_name', col.get('name', '')),
                    data_type=col.get('data_type', col.get('type', 'unknown'))
                )
                for col in table_data.get('columns', [])[:20]
            ]
        )
        table_infos.append(table_info)
        table_name_to_info[table_data['table_name']] = table_info
    
    # é€‰æ‹©å·¥ä½œæµ
    if enable_llm:
        print("  ğŸ“ ä½¿ç”¨ä¼˜åŒ–å·¥ä½œæµï¼ˆå¸¦LLMéªŒè¯ï¼‰")
        from src.core.optimized_workflow import OptimizedDataLakesWorkflow
        workflow = OptimizedDataLakesWorkflow()
        
        # å°è¯•å¯ç”¨LLMåŒ¹é…ï¼ˆå¦‚æœå­˜åœ¨è¯¥å±æ€§ï¼‰
        if hasattr(workflow, 'enable_llm_matching'):
            workflow.enable_llm_matching = True
            print("  âœ… LLMåŒ¹é…å·²å¯ç”¨")
        
        # é…ç½®å‚æ•°
        if hasattr(workflow, 'max_metadata_candidates'):
            workflow.max_metadata_candidates = 100
        if hasattr(workflow, 'max_vector_candidates'):
            workflow.max_vector_candidates = 50
        if hasattr(workflow, 'max_llm_candidates'):
            workflow.max_llm_candidates = 20
    else:
        print("  ğŸ“ ä½¿ç”¨è¶…å¿«é€Ÿå·¥ä½œæµï¼ˆæ— LLMï¼‰")
        from src.core.ultra_optimized_workflow import UltraOptimizedWorkflow
        workflow = UltraOptimizedWorkflow()
        workflow.enable_llm_matching = False
        workflow.max_metadata_candidates = 20
        workflow.max_vector_candidates = 5
        workflow.max_llm_candidates = 0
        workflow.early_stop_threshold = 0.75
    
    # åˆå§‹åŒ–ç´¢å¼•
    init_start = time.time()
    await workflow.initialize(table_infos)
    init_time = time.time() - init_start
    print(f"âœ… ç´¢å¼•åˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {init_time:.2f}ç§’)")
    
    # 3. æ‰§è¡ŒæŸ¥è¯¢è¯„ä¼°
    print(f"\nğŸ” å¼€å§‹è¯„ä¼° {len(all_queries)} ä¸ªæŸ¥è¯¢...")
    print("-"*80)
    
    all_results = []
    all_metrics = []
    query_times = []
    successful_queries = 0
    failed_queries = []
    
    # æ‰¹å¤„ç†è®¾ç½®
    batch_size = 5 if enable_llm else 10
    total_batches = (len(all_queries) + batch_size - 1) // batch_size
    
    for batch_idx in range(total_batches):
        batch_start = batch_idx * batch_size
        batch_end = min(batch_start + batch_size, len(all_queries))
        batch_queries = all_queries[batch_start:batch_end]
        
        print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx+1}/{total_batches} (æŸ¥è¯¢ {batch_start+1}-{batch_end})...")
        
        for query_idx, query_info in enumerate(batch_queries):
            actual_idx = batch_start + query_idx
            
            # è§£ææŸ¥è¯¢ä¿¡æ¯
            query_table_name = query_info.get('query_table', '')
            query_column = query_info.get('query_column', '')
            query_type = query_info.get('query_type', 'join')
            
            # è·å–æŸ¥è¯¢è¡¨çš„ä¿¡æ¯
            query_table_info = table_name_to_info.get(query_table_name)
            if not query_table_info:
                print(f"  âš ï¸ æŸ¥è¯¢ {actual_idx+1}: æœªæ‰¾åˆ°è¡¨ {query_table_name}")
                continue
            
            # è·å–çœŸå®æ ‡ç­¾
            expected = ground_truth_data.get(query_table_name, [])
            
            # å‡†å¤‡æŸ¥è¯¢çŠ¶æ€
            state = AgentState()
            state.user_query = f"Find tables that can {query_type} with {query_table_name}"
            if query_column:
                state.user_query += f" on column {query_column}"
            state.query_tables = [query_table_info]
            if query_column:
                for col in query_table_info.columns:
                    if col.column_name == query_column:
                        state.query_columns = [col]
                        break
            
            # æ‰§è¡ŒæŸ¥è¯¢
            start_time = time.time()
            try:
                # è®¾ç½®è¶…æ—¶æ—¶é—´
                timeout_seconds = 60.0 if enable_llm else 10.0
                
                # ä½¿ç”¨ä¸åŒçš„æ–¹æ³•è°ƒç”¨å·¥ä½œæµ
                if hasattr(workflow, 'run_optimized'):
                    # æ£€æŸ¥æ˜¯å“ªç§å·¥ä½œæµ
                    if 'UltraOptimized' in workflow.__class__.__name__:
                        # UltraOptimizedWorkflow æ¥å—3ä¸ªå‚æ•°
                        result_state, metrics = await asyncio.wait_for(
                            workflow.run_optimized(
                                state,
                                [t.table_name for t in table_infos],
                                {query_table_name: expected}
                            ),
                            timeout=timeout_seconds
                        )
                    else:
                        # OptimizedDataLakesWorkflow åªæ¥å—2ä¸ªå‚æ•°
                        result_state = await asyncio.wait_for(
                            workflow.run_optimized(
                                state,
                                [t.table_name for t in table_infos]
                            ),
                            timeout=timeout_seconds
                        )
                        metrics = None
                else:
                    # ä½¿ç”¨ run æ–¹æ³•
                    result_state = await asyncio.wait_for(
                        workflow.run(state),
                        timeout=timeout_seconds
                    )
                    metrics = None
                
                query_time = time.time() - start_time
                query_times.append(query_time)
                
                # æå–é¢„æµ‹ç»“æœ
                predictions = []
                if result_state:
                    successful_queries += 1
                    if hasattr(result_state, 'final_results') and result_state.final_results:
                        predictions = [r.target_table for r in result_state.final_results[:10]]
                    elif hasattr(result_state, 'table_matches') and result_state.table_matches:
                        predictions = [m.target_table for m in result_state.table_matches[:10]]
                
                # è®¡ç®—æŒ‡æ ‡
                calc_metrics = calculate_metrics(predictions, expected)
                all_metrics.append(calc_metrics)
                
                # ä¿å­˜ç»“æœ
                result_info = {
                    "query_idx": actual_idx + 1,
                    "query_table": query_table_name,
                    "query_column": query_column,
                    "query_type": query_type,
                    "predictions": predictions,
                    "expected": expected,
                    "metrics": calc_metrics,
                    "query_time": query_time
                }
                all_results.append(result_info)
                
            except asyncio.TimeoutError:
                query_time = timeout_seconds
                query_times.append(query_time)
                failed_queries.append(actual_idx + 1)
                print(f"  âš ï¸ æŸ¥è¯¢ {actual_idx+1} è¶…æ—¶ï¼ˆ{timeout_seconds}ç§’ï¼‰")
                all_metrics.append({"precision": 0.0, "recall": 0.0, "f1": 0.0})
                
            except Exception as e:
                failed_queries.append(actual_idx + 1)
                print(f"  âŒ æŸ¥è¯¢ {actual_idx+1} å¤±è´¥: {str(e)[:100]}")
                all_metrics.append({"precision": 0.0, "recall": 0.0, "f1": 0.0})
        
        # æ˜¾ç¤ºè¿›åº¦
        if all_metrics:
            avg_precision = np.mean([m["precision"] for m in all_metrics])
            avg_recall = np.mean([m["recall"] for m in all_metrics])
            avg_f1 = np.mean([m["f1"] for m in all_metrics])
            avg_time = np.mean(query_times) if query_times else 0
            
            print(f"  å½“å‰å¹³å‡: P={avg_precision:.3f} R={avg_recall:.3f} F1={avg_f1:.3f} Time={avg_time:.2f}s")
    
    # 4. è®¡ç®—æ€»ä½“æŒ‡æ ‡
    print("\n" + "="*80)
    print("ğŸ“Š å®éªŒç»“æœæ±‡æ€»")
    print("="*80)
    
    if query_times and all_metrics:
        avg_precision = np.mean([m["precision"] for m in all_metrics])
        avg_recall = np.mean([m["recall"] for m in all_metrics])
        avg_f1 = np.mean([m["f1"] for m in all_metrics])
        avg_query_time = np.mean(query_times)
        total_time = sum(query_times)
        
        # åˆ›å»ºç»“æœè¡¨æ ¼
        results_table = [
            ["æŒ‡æ ‡", "æ•°å€¼"],
            ["æŸ¥è¯¢æ€»æ•°", len(all_queries)],
            ["æˆåŠŸæŸ¥è¯¢", successful_queries],
            ["å¤±è´¥æŸ¥è¯¢", len(failed_queries)],
            ["æˆåŠŸç‡", f"{successful_queries/len(all_queries)*100:.1f}%"],
            ["", ""],
            ["å¹³å‡ç²¾ç¡®ç‡", f"{avg_precision:.4f}"],
            ["å¹³å‡å¬å›ç‡", f"{avg_recall:.4f}"],
            ["å¹³å‡F1åˆ†æ•°", f"{avg_f1:.4f}"],
            ["", ""],
            ["å¹³å‡æŸ¥è¯¢æ—¶é—´", f"{avg_query_time:.3f}ç§’"],
            ["æ€»æ‰§è¡Œæ—¶é—´", f"{total_time:.2f}ç§’"],
            ["ååé‡", f"{len(query_times)/total_time if total_time > 0 else 0:.2f}æŸ¥è¯¢/ç§’"],
            ["", ""],
            ["LLMçŠ¶æ€", "å¯ç”¨" if enable_llm else "ç¦ç”¨"],
            ["ç´¢å¼•åˆå§‹åŒ–æ—¶é—´", f"{init_time:.2f}ç§’"]
        ]
        
        print(tabulate(results_table, headers="firstrow", tablefmt="grid"))
        
        # æ€§èƒ½è¯„ä¼°
        print("\nğŸ† æ€§èƒ½è¯„ä¼°:")
        if avg_query_time <= 3:
            print(f"  âœ… ä¼˜ç§€ï¼å¹³å‡ {avg_query_time:.3f}ç§’ â‰¤ 3ç§’")
        elif avg_query_time <= 8:
            print(f"  âš ï¸ è¾¾æ ‡ã€‚å¹³å‡ {avg_query_time:.3f}ç§’ â‰¤ 8ç§’")
        else:
            print(f"  âŒ éœ€ä¼˜åŒ–ã€‚å¹³å‡ {avg_query_time:.3f}ç§’ > 8ç§’")
        
        # è´¨é‡è¯„ä¼°
        print("\nğŸ¯ è´¨é‡è¯„ä¼°:")
        if avg_precision >= 0.9:
            print(f"  âœ… ç²¾ç¡®ç‡ä¼˜ç§€: {avg_precision:.3f} â‰¥ 0.9")
        elif avg_precision >= 0.7:
            print(f"  âš ï¸ ç²¾ç¡®ç‡è‰¯å¥½: {avg_precision:.3f}")
        else:
            print(f"  âŒ ç²¾ç¡®ç‡éœ€æ”¹è¿›: {avg_precision:.3f}")
        
        if avg_recall >= 0.9:
            print(f"  âœ… å¬å›ç‡ä¼˜ç§€: {avg_recall:.3f} â‰¥ 0.9")
        elif avg_recall >= 0.7:
            print(f"  âš ï¸ å¬å›ç‡è‰¯å¥½: {avg_recall:.3f}")
        else:
            print(f"  âŒ å¬å›ç‡éœ€æ”¹è¿›: {avg_recall:.3f}")
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„æŸ¥è¯¢ï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡")
    
    # 5. ä¿å­˜ç»“æœ
    output_file = f"experiment_results/full_experiment_fixed_{dataset_type}_{len(all_queries)}queries_{'with_llm' if enable_llm else 'no_llm'}_{time.strftime('%Y%m%d_%H%M%S')}.json"
    os.makedirs("experiment_results", exist_ok=True)
    
    output_data = {
        "experiment_info": {
            "dataset_type": dataset_type,
            "total_tables": len(all_tables),
            "total_queries": len(all_queries),
            "llm_enabled": enable_llm,
            "init_time": init_time,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        },
        "overall_metrics": {
            "avg_precision": avg_precision if query_times else 0,
            "avg_recall": avg_recall if query_times else 0,
            "avg_f1": avg_f1 if query_times else 0,
            "avg_query_time": avg_query_time if query_times else 0,
            "total_time": total_time if query_times else 0,
            "throughput": len(query_times)/total_time if query_times and total_time > 0 else 0,
            "success_rate": successful_queries/len(all_queries) if all_queries else 0
        },
        "detailed_results": all_results
    }
    
    with open(output_file, 'w') as f:
        json.dump(output_data, f, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return output_data


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¿è¡Œå®Œæ•´çš„æ•°æ®æ¹–å®éªŒï¼ˆå®Œå…¨ä¿®å¤ç‰ˆï¼‰')
    parser.add_argument(
        '--dataset',
        type=str,
        default='subset',
        choices=['subset', 'complete'],
        help='æ•°æ®é›†ç±»å‹ (é»˜è®¤: subset)'
    )
    parser.add_argument(
        '--max-queries',
        type=int,
        default=None,
        help='æœ€å¤§æŸ¥è¯¢æ•°é‡ (é»˜è®¤: å…¨éƒ¨)'
    )
    parser.add_argument(
        '--no-llm',
        action='store_true',
        help='ç¦ç”¨LLMéªŒè¯ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰'
    )
    
    args = parser.parse_args()
    
    # è¿è¡Œå®éªŒ
    asyncio.run(run_full_experiment(
        args.dataset, 
        args.max_queries,
        enable_llm=not args.no_llm
    ))


if __name__ == "__main__":
    main()