#!/usr/bin/env python
"""
ç»ˆæä¼˜åŒ–ç‰ˆå®éªŒè¿è¡Œè„šæœ¬ - æ‰¹å¤„ç†çº§åˆ«èµ„æºå…±äº«
ä¸»è¦ä¼˜åŒ–ï¼š
1. ä¸€æ¬¡å‘é‡åˆå§‹åŒ–ï¼Œæ‰€æœ‰æŸ¥è¯¢å…±äº«
2. ä¸€æ¬¡æ¡†æ¶åˆå§‹åŒ–ï¼Œå¤ç”¨å·¥ä½œæµå®ä¾‹
3. OptimizerAgentå’ŒPlannerAgentæ¯ä¸ªæ‰¹æ¬¡åªè°ƒç”¨ä¸€æ¬¡
4. ä½¿ç”¨å…±äº«å†…å­˜å’Œè¿›ç¨‹æ± ä¼˜åŒ–
5. æ‰¹å¤„ç†çº§åˆ«çš„é…ç½®ç¼“å­˜
"""
import os
import sys
import json
import time
import hashlib
import logging
import pickle
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ================== å…¨å±€é…ç½® ==================
# é™ä½è¡¨åæƒé‡
os.environ['TABLE_NAME_WEIGHT'] = '0.05'
# ç¡®ä¿LLMå±‚å¯ç”¨
os.environ['FORCE_LLM_VERIFICATION'] = 'true'
os.environ['SKIP_LLM'] = 'false'
# ä½¿ç”¨SMDå¢å¼ºè¿‡æ»¤å™¨
os.environ['USE_SMD_ENHANCED'] = 'true'
# å›ºå®šhashç§å­
os.environ['PYTHONHASHSEED'] = '0'

# ================== å…¨å±€å…±äº«èµ„æºç®¡ç† ==================
# Managerå¿…é¡»åœ¨ä¸»è¿›ç¨‹ä¸­åˆå§‹åŒ–


def initialize_shared_resources(tables: List[Dict], task_type: str, dataset_type: str):
    """
    åˆå§‹åŒ–æ‰€æœ‰å…±äº«èµ„æºï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    è¿”å›å…±äº«é…ç½®å­—å…¸
    """
    logger.info("ğŸš€ åˆå§‹åŒ–æ‰¹å¤„ç†å…±äº«èµ„æº...")
    
    # 1. é¢„è®¡ç®—å‘é‡ï¼ˆä¸€æ¬¡æ€§å®Œæˆï¼‰
    logger.info("ğŸ“Š é¢„è®¡ç®—å‘é‡åµŒå…¥...")
    cache_dir = Path("cache") / dataset_type
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    index_file = cache_dir / f"vector_index_{len(tables)}.pkl"
    embeddings_file = cache_dir / f"table_embeddings_{len(tables)}.pkl"
    
    if not (index_file.exists() and embeddings_file.exists()):
        logger.info("âš™ï¸ ç”Ÿæˆæ–°çš„å‘é‡ç´¢å¼•...")
        from precompute_embeddings import precompute_all_embeddings
        precompute_all_embeddings(tables, dataset_type)
    
    # 2. åˆå§‹åŒ–å·¥ä½œæµï¼ˆä¸€æ¬¡æ€§ï¼‰
    logger.info("ğŸ”§ åˆå§‹åŒ–å·¥ä½œæµå•ä¾‹...")
    from src.core.langgraph_workflow import DataLakeDiscoveryWorkflow
    workflow = DataLakeDiscoveryWorkflow()
    
    # 3. è·å–ä¼˜åŒ–é…ç½®ï¼ˆå¯¹æ•´ä¸ªæ‰¹æ¬¡åªè°ƒç”¨ä¸€æ¬¡OptimizerAgentï¼‰
    logger.info("ğŸ“‹ è·å–æ‰¹å¤„ç†ä¼˜åŒ–é…ç½®...")
    optimization_config = get_batch_optimization_config(workflow, tables, task_type)
    
    # 4. è·å–æ‰§è¡Œç­–ç•¥ï¼ˆå¯¹æ•´ä¸ªæ‰¹æ¬¡åªè°ƒç”¨ä¸€æ¬¡PlannerAgentï¼‰
    logger.info("ğŸ“‹ è·å–æ‰¹å¤„ç†æ‰§è¡Œç­–ç•¥...")
    execution_strategy = get_batch_execution_strategy(workflow, task_type)
    
    # ä¿å­˜åˆ°å…±äº«é…ç½®
    config = {
        'vector_index_path': str(index_file),
        'embeddings_path': str(embeddings_file),
        'optimization_config': optimization_config,
        'execution_strategy': execution_strategy,
        'table_count': len(tables),
        'task_type': task_type,
        'dataset_type': dataset_type
    }
    
    logger.info("âœ… æ‰¹å¤„ç†èµ„æºåˆå§‹åŒ–å®Œæˆ")
    logger.info(f"  - å‘é‡ç´¢å¼•: å·²åŠ è½½")
    logger.info(f"  - å·¥ä½œæµ: å·²åˆå§‹åŒ–")
    logger.info(f"  - ä¼˜åŒ–é…ç½®: {optimization_config}")
    logger.info(f"  - æ‰§è¡Œç­–ç•¥: {execution_strategy}")
    
    return config


def get_batch_optimization_config(workflow, tables: List[Dict], task_type: str) -> Dict:
    """
    è·å–æ•´ä¸ªæ‰¹æ¬¡çš„ä¼˜åŒ–é…ç½®ï¼ˆåªè°ƒç”¨ä¸€æ¬¡OptimizerAgentï¼‰
    """
    # æ„é€ ä¸€ä¸ªä»£è¡¨æ€§çš„æŸ¥è¯¢æ¥è·å–é…ç½®
    cache_key = f"{task_type}_{len(tables)}"
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„é…ç½®
    if hasattr(workflow, '_optimization_cache') and cache_key in workflow._optimization_cache:
        logger.info(f"  ğŸ“¥ ä½¿ç”¨ç¼“å­˜çš„ä¼˜åŒ–é…ç½®: {cache_key}")
        return workflow._optimization_cache[cache_key]
    
    # è°ƒç”¨OptimizerAgent
    from src.agents.optimizer_agent import OptimizerAgent
    optimizer = OptimizerAgent()
    
    config = optimizer.process({
        'task_type': task_type,
        'table_count': len(tables),
        'complexity': 'medium',  # å›ºå®šä¸ºmedium
        'performance_requirement': 'balanced'
    })
    
    # ç¼“å­˜é…ç½®
    if not hasattr(workflow, '_optimization_cache'):
        workflow._optimization_cache = {}
    workflow._optimization_cache[cache_key] = config
    
    logger.info(f"  âœ… OptimizerAgenté…ç½®: workers={config.get('parallel_workers', 8)}, "
               f"cache={config.get('cache_strategy', 'L1')}")
    
    return config


def get_batch_execution_strategy(workflow, task_type: str) -> Dict:
    """
    è·å–æ•´ä¸ªæ‰¹æ¬¡çš„æ‰§è¡Œç­–ç•¥ï¼ˆåªè°ƒç”¨ä¸€æ¬¡PlannerAgentï¼‰
    """
    cache_key = f"{task_type}_strategy"
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„ç­–ç•¥
    if hasattr(workflow, '_strategy_cache') and cache_key in workflow._strategy_cache:
        logger.info(f"  ğŸ“¥ ä½¿ç”¨ç¼“å­˜çš„æ‰§è¡Œç­–ç•¥: {cache_key}")
        return workflow._strategy_cache[cache_key]
    
    # è°ƒç”¨PlannerAgent
    from src.agents.planner_agent import PlannerAgent
    planner = PlannerAgent()
    
    strategy = planner.process({
        'task_type': task_type,
        'table_structure': 'unknown',
        'data_size': 'medium',
        'performance_requirement': 'balanced'
    })
    
    # ç¼“å­˜ç­–ç•¥
    if not hasattr(workflow, '_strategy_cache'):
        workflow._strategy_cache = {}
    workflow._strategy_cache[cache_key] = strategy
    
    logger.info(f"  âœ… PlannerAgentç­–ç•¥: {strategy.get('strategy', 'bottom-up')}, "
               f"top_k={strategy.get('top_k', 100)}")
    
    return strategy


def process_query_with_shared_resources(args: Tuple) -> Dict:
    """
    ä½¿ç”¨å…±äº«èµ„æºå¤„ç†å•ä¸ªæŸ¥è¯¢
    """
    query, tables, shared_config_dict, cache_file_path = args
    query_table = query.get('query_table', '')
    
    # åŠ è½½ç¼“å­˜
    cache_file = Path(cache_file_path)
    cache = {}
    if cache_file.exists():
        try:
            with open(cache_file, 'rb') as f:
                cache = pickle.load(f)
        except:
            pass
    
    # æ£€æŸ¥ç¼“å­˜
    cache_key = hashlib.md5(
        f"{shared_config_dict['task_type']}:{query_table}:{len(tables)}".encode()
    ).hexdigest()
    
    if cache_key in cache:
        logger.info(f"ğŸ’ ç¼“å­˜å‘½ä¸­: {query_table}")
        cached_result = cache[cache_key].copy()
        cached_result['from_cache'] = True
        return cached_result
    
    # åˆ›å»ºæ–°çš„å·¥ä½œæµå®ä¾‹ï¼ˆæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹ï¼‰
    from src.core.langgraph_workflow import DataLakeDiscoveryWorkflow
    workflow = DataLakeDiscoveryWorkflow()
    
    # ç›´æ¥è®¾ç½®å·²ç»è®¡ç®—å¥½çš„é…ç½®ï¼ˆé¿å…é‡å¤è°ƒç”¨OptimizerAgentå’ŒPlannerAgentï¼‰
    workflow._optimization_cache = {
        f"{shared_config_dict['task_type']}_{shared_config_dict['table_count']}": 
        shared_config_dict['optimization_config']
    }
    workflow._strategy_cache = {
        f"{shared_config_dict['task_type']}_strategy": 
        shared_config_dict['execution_strategy']
    }
    
    try:
        start_time = time.time()
        
        # è¿è¡Œå·¥ä½œæµï¼ˆä¼šè‡ªåŠ¨ä½¿ç”¨ç¼“å­˜çš„é…ç½®ï¼‰
        result = workflow.run(
            query=f"find {shared_config_dict['task_type']}able tables for {query_table}",
            tables=tables,
            task_type=shared_config_dict['task_type'],
            query_table_name=query_table
        )
        
        elapsed = time.time() - start_time
        
        # ç»Ÿè®¡LLMè°ƒç”¨ï¼ˆä¸åŒ…æ‹¬OptimizerAgentå’ŒPlannerAgentï¼Œå› ä¸ºå·²ç»ç¼“å­˜ï¼‰
        llm_calls = 0
        if 'metrics' in result:
            # åªç»Ÿè®¡AnalyzerAgentå’ŒMatcherAgentçš„è°ƒç”¨
            agent_times = result['metrics'].get('agent_times', {})
            if 'analyzer' in agent_times:
                llm_calls += 1
            if 'matcher' in agent_times:
                llm_calls += 1
        
        # å¤„ç†ç»“æœ
        if result.get('success'):
            raw_results = result.get('results', [])
            predictions = []
            
            for r in raw_results[:10]:
                if isinstance(r, dict):
                    table_name = r.get('table_name', '')
                    if table_name and table_name != query_table:
                        predictions.append(table_name)
            
            predictions = predictions[:5]
            
            query_result = {
                'query_table': query_table,
                'predictions': predictions,
                'time': elapsed,
                'llm_calls': llm_calls,
                'metrics': result.get('metrics', {}),
                'from_cache': False
            }
        else:
            query_result = {
                'query_table': query_table,
                'predictions': [],
                'time': elapsed,
                'llm_calls': llm_calls,
                'metrics': {},
                'from_cache': False
            }
        
        # æ›´æ–°ç¼“å­˜
        cache[cache_key] = query_result.copy()
        with open(cache_file, 'wb') as f:
            pickle.dump(cache, f)
        
        logger.info(f"âœ… å®Œæˆ: {query_table} | æ—¶é—´: {elapsed:.2f}s | LLM: {llm_calls}")
        
        return query_result
        
    except Exception as e:
        logger.error(f"å¤„ç†æŸ¥è¯¢å¤±è´¥ {query_table}: {e}")
        return {
            'query_table': query_table,
            'predictions': [],
            'time': 0,
            'llm_calls': 0,
            'metrics': {},
            'from_cache': False,
            'error': str(e)
        }


def run_ultimate_optimized_experiment(tables: List[Dict], queries: List[Dict],
                                     ground_truth: List[Dict], task_type: str,
                                     dataset_type: str, max_workers: int = 4) -> Dict:
    """
    è¿è¡Œç»ˆæä¼˜åŒ–çš„å®éªŒï¼ˆæ‰¹å¤„ç†çº§åˆ«èµ„æºå…±äº«ï¼‰
    """
    # 1. åˆå§‹åŒ–æ‰€æœ‰å…±äº«èµ„æºï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
    shared_config_dict = initialize_shared_resources(tables, task_type, dataset_type)
    
    # 2. å‡†å¤‡ç¼“å­˜æ–‡ä»¶è·¯å¾„
    cache_file = Path(f"cache/{dataset_type}_{task_type}_persistent.pkl")
    
    # 3. å‡†å¤‡è¿›ç¨‹æ± å‚æ•°
    query_args = [
        (query, tables, shared_config_dict, str(cache_file))
        for query in queries
    ]
    
    # 4. ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†æŸ¥è¯¢
    results = []
    cache_hits = 0
    total_llm_calls = 0
    
    logger.info(f"ğŸš€ å¼€å§‹å¤„ç† {len(queries)} ä¸ªæŸ¥è¯¢ (è¿›ç¨‹æ•°={max_workers})...")
    logger.info(f"  âš¡ ä½¿ç”¨å…±äº«é…ç½®ï¼Œé¿å…é‡å¤åˆå§‹åŒ–")
    logger.info(f"  ğŸ“Š OptimizerAgentå’ŒPlannerAgentåªè°ƒç”¨1æ¬¡ï¼ˆè€Œä¸æ˜¯{len(queries)}æ¬¡ï¼‰")
    
    start_time = time.time()
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        future_to_query = {
            executor.submit(process_query_with_shared_resources, args): args[0]
            for args in query_args
        }
        
        completed = 0
        for future in as_completed(future_to_query):
            query = future_to_query[future]
            completed += 1
            
            try:
                result = future.result(timeout=60)
                results.append(result)
                
                if result.get('from_cache', False):
                    cache_hits += 1
                
                total_llm_calls += result.get('llm_calls', 0)
                
                logger.info(f"è¿›åº¦: {completed}/{len(queries)} | "
                          f"{result['query_table']} | "
                          f"æ—¶é—´: {result.get('time', 0):.2f}s | "
                          f"ç¼“å­˜: {result.get('from_cache', False)}")
                
            except Exception as e:
                logger.error(f"æŸ¥è¯¢å¤±è´¥: {query.get('query_table', '')}: {e}")
                results.append({
                    'query_table': query.get('query_table', ''),
                    'predictions': [],
                    'time': 0,
                    'llm_calls': 0,
                    'metrics': {},
                    'error': str(e)
                })
    
    total_time = time.time() - start_time
    
    # 5. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    from evaluate_with_metrics import calculate_metrics
    from run_cached_experiments import convert_ground_truth_format
    converted_gt = convert_ground_truth_format(ground_truth)
    metrics = calculate_metrics(results, converted_gt)
    
    # æ·»åŠ OptimizerAgentå’ŒPlannerAgentçš„è°ƒç”¨ï¼ˆæ‰¹å¤„ç†çº§åˆ«å„1æ¬¡ï¼‰
    total_llm_calls += 2  # OptimizerAgent + PlannerAgent
    
    return {
        'results': results,
        'metrics': metrics,
        'total_time': total_time,
        'avg_time': total_time / len(queries) if queries else 0,
        'total_llm_calls': total_llm_calls,
        'cache_hits': cache_hits,
        'cache_hit_rate': cache_hits / len(queries) if queries else 0,
        'optimization_savings': {
            'optimizer_calls_saved': len(queries) - 1,  # åªè°ƒç”¨1æ¬¡è€Œä¸æ˜¯Næ¬¡
            'planner_calls_saved': len(queries) - 1,    # åªè°ƒç”¨1æ¬¡è€Œä¸æ˜¯Næ¬¡
            'estimated_time_saved': (len(queries) - 1) * 4  # æ¯æ¬¡è°ƒç”¨çº¦2ç§’
        }
    }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    parser = argparse.ArgumentParser(description='è¿è¡Œç»ˆæä¼˜åŒ–çš„å®éªŒ')
    parser.add_argument('--task', choices=['join', 'union', 'both'], default='both')
    parser.add_argument('--dataset', choices=['subset', 'complete'], default='subset')
    parser.add_argument('--max-queries', type=int, default=10)
    parser.add_argument('--workers', type=int, default=4, help='å¹¶è¡Œè¿›ç¨‹æ•°')
    args = parser.parse_args()
    
    # åŠ è½½æ•°æ®
    from run_cached_experiments import load_dataset, filter_queries_with_ground_truth
    
    tasks = ['join', 'union'] if args.task == 'both' else [args.task]
    all_results = {}
    
    for task in tasks:
        logger.info("="*80)
        logger.info(f"ğŸ¯ è¿è¡Œ {task.upper()} ä»»åŠ¡ - {args.dataset.upper()} æ•°æ®é›†")
        logger.info("="*80)
        
        # åŠ è½½æ•°æ®
        tables, queries, ground_truth = load_dataset(task, args.dataset)
        filtered_queries = filter_queries_with_ground_truth(
            queries, ground_truth, args.max_queries
        )
        
        logger.info(f"ğŸ“Š æ•°æ®é›†: {len(tables)} è¡¨, {len(filtered_queries)} æŸ¥è¯¢")
        
        # è¿è¡Œç»ˆæä¼˜åŒ–å®éªŒ
        result = run_ultimate_optimized_experiment(
            tables, filtered_queries, ground_truth, 
            task, args.dataset, max_workers=args.workers
        )
        
        all_results[task] = result
        
        # è¾“å‡ºç»“æœ
        logger.info("="*80)
        logger.info(f"ğŸ“ˆ {task.upper()} ä»»åŠ¡ç»“æœ:")
        logger.info(f"  â±ï¸ æ€»æ—¶é—´: {result['total_time']:.2f}ç§’")
        logger.info(f"  âš¡ å¹³å‡æ—¶é—´: {result['avg_time']:.2f}ç§’/æŸ¥è¯¢")
        logger.info(f"  ğŸ’ ç¼“å­˜å‘½ä¸­: {result['cache_hits']}/{len(filtered_queries)} "
                   f"({result['cache_hit_rate']*100:.1f}%)")
        logger.info(f"  ğŸ¤– LLMè°ƒç”¨: {result['total_llm_calls']}æ¬¡")
        
        # æ˜¾ç¤ºä¼˜åŒ–èŠ‚çœ
        savings = result['optimization_savings']
        logger.info(f"  ğŸ’° ä¼˜åŒ–èŠ‚çœ:")
        logger.info(f"    - OptimizerAgentè°ƒç”¨å‡å°‘: {savings['optimizer_calls_saved']}æ¬¡")
        logger.info(f"    - PlannerAgentè°ƒç”¨å‡å°‘: {savings['planner_calls_saved']}æ¬¡")
        logger.info(f"    - é¢„è®¡èŠ‚çœæ—¶é—´: {savings['estimated_time_saved']:.1f}ç§’")
        
        logger.info(f"  ğŸ“Š è¯„ä¼°æŒ‡æ ‡:")
        metrics = result['metrics']
        for k, v in metrics.items():
            if isinstance(v, float):
                logger.info(f"    {k}: {v:.3f}")
            else:
                logger.info(f"    {k}: {v}")
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"experiment_results/ultimate_{args.dataset}_{timestamp}.json"
    Path("experiment_results").mkdir(exist_ok=True)
    
    with open(output_file, 'w') as f:
        json.dump(all_results, f, indent=2)
    
    logger.info(f"\nâœ… å®éªŒå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_file}")
    
    # æ€§èƒ½æ”¹è¿›æ€»ç»“
    logger.info("\n" + "="*80)
    logger.info("ğŸš€ ç»ˆæä¼˜åŒ–æ€»ç»“:")
    logger.info("="*80)
    
    for task, result in all_results.items():
        logger.info(f"\n{task.upper()} ä»»åŠ¡:")
        logger.info(f"  ğŸ¯ Hit@1: {result['metrics'].get('hit@1', 0):.3f}")
        logger.info(f"  ğŸ“ˆ Hit@3: {result['metrics'].get('hit@3', 0):.3f}")
        logger.info(f"  ğŸ“Š Hit@5: {result['metrics'].get('hit@5', 0):.3f}")
        logger.info(f"  âš¡ æŸ¥è¯¢é€Ÿåº¦: {result['avg_time']:.2f}ç§’")
        logger.info(f"  ğŸ’ ç¼“å­˜æ•ˆç‡: {result['cache_hit_rate']*100:.1f}%")
        
        # è®¡ç®—å®é™…åŠ é€Ÿæ¯”
        if result['optimization_savings']['estimated_time_saved'] > 0:
            theoretical_time = result['total_time'] + result['optimization_savings']['estimated_time_saved']
            speedup = theoretical_time / result['total_time']
            logger.info(f"  ğŸš€ ä¼˜åŒ–åŠ é€Ÿæ¯”: {speedup:.2f}x")


if __name__ == "__main__":
    # ä¿®å¤å¤šè¿›ç¨‹å¯åŠ¨é—®é¢˜
    mp.freeze_support()  # Windowsæ”¯æŒ
    main()