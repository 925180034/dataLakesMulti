#!/usr/bin/env python
"""
ç»Ÿä¸€å®éªŒè¿è¡Œå™¨ - æ”¯æŒWebTableã€OpenDataå’ŒNLCTablesä¸‰ä¸ªæ•°æ®é›†
å¯ä»¥ä½¿ç”¨åŒä¸€ä¸ªç³»ç»Ÿè¿è¡Œæ‰€æœ‰æ•°æ®é›†çš„å®éªŒ
æ”¯æŒä¸three_layer_ablation_optimized.pyç›¸åŒçš„æ‰€æœ‰å‚æ•°
"""

import os
import sys
import json
import time
import shutil
import pickle
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime

# ä»three_layer_ablation_optimizedå¯¼å…¥éœ€è¦çš„å‡½æ•°
from three_layer_ablation_optimized import convert_ground_truth_format

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¯é…ç½®çš„æœ€å¤§é¢„æµ‹æ•°é‡ï¼ˆæ”¯æŒ@Kè®¡ç®—ï¼ŒKæœ€å¤§ä¸º10ï¼Œè®¾ç½®ä¸º20ç•™æœ‰ä½™é‡ï¼‰
MAX_PREDICTIONS = int(os.environ.get('MAX_PREDICTIONS', '20'))
logger.info(f"ğŸ“Š MAX_PREDICTIONS set to {MAX_PREDICTIONS} (supports up to @{MAX_PREDICTIONS//2} evaluation)")

# å…¨å±€ç¼“å­˜å­˜å‚¨
global_unified_cache = {}

def clear_experiment_cache(specific_dataset: str = None):
    """æ¸…ç†å®éªŒç¼“å­˜ï¼ˆä½†ä¿ç•™åµŒå…¥å‘é‡ç¼“å­˜ï¼‰
    
    Args:
        specific_dataset: å¦‚æœæŒ‡å®šï¼Œåªæ¸…ç†è¯¥æ•°æ®é›†çš„ç¼“å­˜
    """
    cache_root = Path("cache")
    
    if not cache_root.exists():
        logger.info("ğŸ“¦ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†")
        return 0
    
    cleared_count = 0
    
    if specific_dataset:
        # æ¸…ç†ç‰¹å®šæ•°æ®é›†çš„ä¸´æ—¶ç¼“å­˜ï¼ˆä½†ä¿ç•™åµŒå…¥å‘é‡ï¼‰
        patterns = [
            f"ablation_{specific_dataset}_*",
            f"experiment_cache/{specific_dataset}_*",
            f"experiment_{specific_dataset}_*",
            f"unified_{specific_dataset}_*"
        ]
        logger.info(f"ğŸ§¹ æ¸…ç† {specific_dataset} æ•°æ®é›†çš„ç¼“å­˜...")
    else:
        # æ¸…ç†æ‰€æœ‰ä¸´æ—¶ç¼“å­˜ï¼ˆä½†ä¿ç•™åµŒå…¥å‘é‡ï¼‰
        patterns = ["ablation_*", "experiment_*", "unified_*"]
        logger.info("ğŸ§¹ æ¸…ç†æ‰€æœ‰å®éªŒç¼“å­˜...")
    
    for pattern in patterns:
        for cache_path in cache_root.glob(pattern):
            if cache_path.is_dir():
                try:
                    shutil.rmtree(cache_path)
                    cleared_count += 1
                    logger.debug(f"  âœ… åˆ é™¤ç¼“å­˜ç›®å½•: {cache_path}")
                except Exception as e:
                    logger.warning(f"  âš ï¸ æ— æ³•åˆ é™¤ {cache_path}: {e}")
    
    # é‡è¦ï¼šä¸åˆ é™¤ cache/{dataset}/ ç›®å½•æœ¬èº«ï¼Œå› ä¸ºåµŒå…¥å‘é‡åœ¨é‚£é‡Œï¼
    # åªåˆ é™¤ ablation_*, experiment_*, unified_* ç­‰ä¸´æ—¶ç¼“å­˜
    
    if cleared_count > 0:
        logger.info(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleared_count} ä¸ªä¸´æ—¶ç¼“å­˜ç›®å½•")
        logger.info(f"ğŸ“¦ ä¿ç•™äº†åµŒå…¥å‘é‡ç¼“å­˜åœ¨ cache/{specific_dataset or '*'}/ ç›®å½•")
    else:
        logger.info("ğŸ“¦ æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„ä¸´æ—¶ç¼“å­˜")
    
    return cleared_count


def prepare_unified_cache(tables: List[Dict], dataset_name: str, task_type: str) -> Dict[str, Any]:
    """ä¸ºæ•´ä¸ªå®éªŒå‡†å¤‡ç»Ÿä¸€çš„ç¼“å­˜ï¼ˆå‘é‡ç´¢å¼•å’ŒåµŒå…¥ï¼‰
    
    Args:
        tables: è¡¨åˆ—è¡¨
        dataset_name: æ•°æ®é›†åç§°
        task_type: ä»»åŠ¡ç±»å‹
        
    Returns:
        åŒ…å«é¢„è®¡ç®—æ•°æ®çš„å­—å…¸
    """
    global global_unified_cache
    
    # å¦‚æœå·²ç»æœ‰ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    cache_key = f"{dataset_name}_{task_type}_{len(tables)}"
    if cache_key in global_unified_cache:
        logger.info("ğŸ“¦ ä½¿ç”¨å·²æœ‰çš„ç»Ÿä¸€ç¼“å­˜")
        return global_unified_cache[cache_key]
    
    logger.info("ğŸ“Š å‡†å¤‡ç»Ÿä¸€çš„å®éªŒç¼“å­˜...")
    
    # åˆ›å»ºç¼“å­˜ç›®å½•
    cache_dir = Path("cache") / dataset_name
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    # å‘é‡ç´¢å¼•å’ŒåµŒå…¥æ–‡ä»¶
    index_file = cache_dir / f"vector_index_{len(tables)}.pkl"
    embeddings_file = cache_dir / f"table_embeddings_{len(tables)}.pkl"
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if index_file.exists() and embeddings_file.exists():
        logger.info("  ğŸ“¦ åŠ è½½ç°æœ‰çš„å‘é‡ç´¢å¼•å’ŒåµŒå…¥...")
        try:
            with open(index_file, 'rb') as f:
                vector_index = pickle.load(f)
            with open(embeddings_file, 'rb') as f:
                table_embeddings = pickle.load(f)
        except Exception as e:
            logger.warning(f"  âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}ï¼Œé‡æ–°ç”Ÿæˆ...")
            vector_index = None
            table_embeddings = None
    else:
        vector_index = None
        table_embeddings = None
    
    # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œç”Ÿæˆæ–°çš„
    if vector_index is None or table_embeddings is None:
        logger.info("  âš™ï¸ ç”Ÿæˆæ–°çš„å‘é‡ç´¢å¼•å’ŒåµŒå…¥...")
        from precompute_embeddings import precompute_all_embeddings
        precompute_all_embeddings(tables, dataset_name)
        
        # é‡æ–°åŠ è½½ç”Ÿæˆçš„æ–‡ä»¶
        with open(index_file, 'rb') as f:
            vector_index = pickle.load(f)
        with open(embeddings_file, 'rb') as f:
            table_embeddings = pickle.load(f)
    
    logger.info(f"  âœ… ç»Ÿä¸€ç¼“å­˜å‡†å¤‡å®Œæˆ")
    logger.info(f"  ğŸ“Š å‘é‡ç´¢å¼•å¤§å°: {index_file.stat().st_size / 1024:.2f}KB")
    logger.info(f"  ğŸ“Š è¡¨åµŒå…¥å¤§å°: {embeddings_file.stat().st_size / 1024:.2f}KB")
    
    result = {
        'vector_index': vector_index,
        'table_embeddings': table_embeddings,
        'cache_dir': cache_dir,
        'cache_key': cache_key
    }
    
    # å­˜å‚¨åˆ°å…¨å±€ç¼“å­˜
    global_unified_cache[cache_key] = result
    
    return result

def detect_dataset_type(tables_path: str) -> str:
    """è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†ç±»å‹"""
    path_str = str(tables_path).lower()
    
    if 'nlctables' in path_str:
        return 'nlctables'
    elif 'opendata' in path_str:
        return 'opendata'
    elif 'webtable' in path_str or 'final' in path_str:
        return 'webtable'
    else:
        # é»˜è®¤æ ¹æ®è¡¨åæ ¼å¼åˆ¤æ–­
        with open(tables_path, 'r') as f:
            tables = json.load(f)
            if tables and isinstance(tables[0], dict):
                first_table_name = tables[0].get('name', '')
                if first_table_name.startswith('dl_table_') or first_table_name.startswith('q_table_'):
                    return 'nlctables'
                elif 'opendata' in first_table_name.lower():
                    return 'opendata'
        return 'webtable'

def run_nlctables_experiment(layer: str, tables: List[Dict], queries: List[Dict], 
                            task_type: str, max_queries: int = None, 
                            max_workers: int = 4, challenging: bool = True) -> Tuple[List[Dict], float]:
    """è¿è¡ŒNLCTableså®éªŒ - ä½¿ç”¨ä¸»ç³»ç»Ÿé€šè¿‡é€‚é…å™¨"""
    logger.info(f"ğŸ”¬ Running NLCTables experiment with layer {layer}")
    logger.info(f"  Task type: {task_type}")
    logger.info(f"  Input queries: {len(queries)}")
    
    # ä½¿ç”¨ä¸»ç³»ç»Ÿè¿è¡ŒNLCTables
    from three_layer_ablation_optimized import run_layer_experiment
    
    # å¤„ç†æŸ¥è¯¢æ•°é‡ - åªåœ¨queriesé•¿åº¦å¤§äºmax_queriesæ—¶é™åˆ¶
    if max_queries is not None and len(queries) > max_queries:
        queries = queries[:max_queries]
        logger.info(f"  Limited to {max_queries} queries")
    
    # TODO: å¦‚æœéœ€è¦æŒ‘æˆ˜æ€§æŸ¥è¯¢ï¼Œå¯ä»¥åœ¨è¿™é‡Œå¤„ç†
    # if challenging:
    #     queries = create_challenging_queries(queries, tables)
    
    # è¿è¡Œå®éªŒ
    results_dict, elapsed_time = run_layer_experiment(
        layer=layer,
        tables=tables,
        queries=queries,
        task_type=task_type,
        dataset_type='nlctables',
        max_workers=max_workers
    )
    
    # è½¬æ¢ç»“æœæ ¼å¼ä»å­—å…¸åˆ°åˆ—è¡¨
    results = []
    if isinstance(results_dict, dict):
        for query_table, predictions in results_dict.items():
            results.append({
                'query_table': query_table,
                'predictions': predictions[:MAX_PREDICTIONS] if isinstance(predictions, list) else []
            })
    else:
        results = results_dict
    
    logger.info(f"  Output results: {len(results)} entries")
    if results and len(results) > 0:
        logger.info(f"  First result: {results[0]['query_table']} -> {len(results[0].get('predictions', []))} predictions")
    
    return results, elapsed_time

def run_webtable_opendata_experiment(layer: str, tables: List[Dict], queries: List[Dict],
                                  task_type: str, dataset_type: str, max_queries: int = None,
                                  max_workers: int = 4, challenging: bool = True) -> Tuple[List[Dict], float]:
    """è¿è¡ŒWebTable/OpenDataå®éªŒ - ä½¿ç”¨ä¸»ç³»ç»Ÿ"""
    logger.info(f"ğŸ”¬ Running {dataset_type.upper()} experiment with layer {layer}")
    
    # å¯¼å…¥ä¸»ç³»ç»Ÿ
    from three_layer_ablation_optimized import run_layer_experiment
    
    # å¤„ç†æŸ¥è¯¢æ•°é‡
    if max_queries is not None:
        queries = queries[:max_queries]
    
    # TODO: å¦‚æœéœ€è¦æŒ‘æˆ˜æ€§æŸ¥è¯¢ï¼Œå¯ä»¥åœ¨è¿™é‡Œå¤„ç†
    # if challenging:
    #     queries = create_challenging_queries(queries, tables)
    
    # è¿è¡Œå®éªŒ
    results_dict, elapsed_time = run_layer_experiment(
        layer=layer,
        tables=tables,
        queries=queries,
        task_type=task_type,
        dataset_type=dataset_type,
        max_workers=max_workers
    )
    
    # è½¬æ¢ç»“æœæ ¼å¼ä»å­—å…¸åˆ°åˆ—è¡¨
    results = []
    if isinstance(results_dict, dict):
        for query_table, predictions in results_dict.items():
            results.append({
                'query_table': query_table,
                'predictions': predictions[:MAX_PREDICTIONS] if isinstance(predictions, list) else []
            })
    else:
        results = results_dict
    
    logger.info(f"  Output results: {len(results)} entries")
    if results and len(results) > 0:
        logger.info(f"  First result: {results[0]['query_table']} -> {len(results[0].get('predictions', []))} predictions")
    
    return results, elapsed_time

def evaluate_results(results: List[Dict], ground_truth, k_values: List[int] = [1, 3, 5]) -> Dict:
    """è¯„ä¼°ç»“æœ - ground_truth can be Dict or List"""
    from src.utils.evaluation import calculate_hit_at_k, calculate_precision_recall_f1, calculate_precision_recall_at_k
    
    metrics = {}
    
    # è®¡ç®—Hit@K
    for k in k_values:
        hit_rate = calculate_hit_at_k(results, ground_truth, k)
        metrics[f'hit@{k}'] = hit_rate
    
    # è®¡ç®—Precision@Kå’ŒRecall@K for k=1, 5, 10
    for k in [1, 5, 10]:
        pr_at_k = calculate_precision_recall_at_k(results, ground_truth, k)
        metrics[f'precision@{k}'] = pr_at_k['precision']
        metrics[f'recall@{k}'] = pr_at_k['recall']
    
    # è®¡ç®—å…¨é‡Precision/Recall/F1ï¼ˆä¿ç•™ï¼Œç”¨äºå…¼å®¹æ€§ï¼‰
    pr_metrics = calculate_precision_recall_f1(results, ground_truth)
    metrics.update(pr_metrics)
    
    return metrics

def print_results_table(all_results: Dict, all_metrics: Dict):
    """æ‰“å°ç»“æœç»Ÿè®¡è¡¨æ ¼"""
    # åˆ†ç¦»JOINå’ŒUNIONç»“æœ
    join_results = {}
    union_results = {}
    
    for exp_key, metrics in all_metrics.items():
        exp_data = all_results[exp_key]
        task = exp_data['task']
        layer = exp_data['layer']
        elapsed_time = exp_data['elapsed_time']
        
        result_data = {
            'layer': layer,
            'hit@1': metrics.get('hit@1', 0.0),
            'hit@3': metrics.get('hit@3', 0.0),
            'hit@5': metrics.get('hit@5', 0.0),
            'precision@1': metrics.get('precision@1', 0.0),
            'precision@5': metrics.get('precision@5', 0.0),
            'precision@10': metrics.get('precision@10', 0.0),
            'recall@1': metrics.get('recall@1', 0.0),
            'recall@5': metrics.get('recall@5', 0.0),
            'recall@10': metrics.get('recall@10', 0.0),
            'time': elapsed_time
        }
        
        if task == 'join':
            join_results[layer] = result_data
        elif task == 'union':
            union_results[layer] = result_data
    
    # æ‰“å°JOINç»“æœè¡¨æ ¼
    if join_results:
        print("\nJOIN Task Results:")
        print("-" * 150)
        print(f"{'Layer':<12} {'Hit@1':<8} {'Hit@3':<8} {'Hit@5':<8} "
              f"{'P@1':<8} {'P@5':<8} {'P@10':<8} "
              f"{'R@1':<8} {'R@5':<8} {'R@10':<8} {'Time(s)':<8}")
        print("-" * 150)
        
        # æŒ‰L1, L1+L2, L1+L2+L3é¡ºåºæ’åº
        layer_order = ['L1', 'L1+L2', 'L1+L2+L3']
        for layer in layer_order:
            if layer in join_results:
                data = join_results[layer]
                print(f"{layer:<12} {data['hit@1']:<8.3f} {data['hit@3']:<8.3f} {data['hit@5']:<8.3f} "
                      f"{data['precision@1']:<8.3f} {data['precision@5']:<8.3f} {data['precision@10']:<8.3f} "
                      f"{data['recall@1']:<8.3f} {data['recall@5']:<8.3f} {data['recall@10']:<8.3f} "
                      f"{data['time']:<8.2f}")
    
    # æ‰“å°UNIONç»“æœè¡¨æ ¼
    if union_results:
        print("\nUNION Task Results:")
        print("-" * 150)
        print(f"{'Layer':<12} {'Hit@1':<8} {'Hit@3':<8} {'Hit@5':<8} "
              f"{'P@1':<8} {'P@5':<8} {'P@10':<8} "
              f"{'R@1':<8} {'R@5':<8} {'R@10':<8} {'Time(s)':<8}")
        print("-" * 150)
        
        # æŒ‰L1, L1+L2, L1+L2+L3é¡ºåºæ’åº
        layer_order = ['L1', 'L1+L2', 'L1+L2+L3']
        for layer in layer_order:
            if layer in union_results:
                data = union_results[layer]
                print(f"{layer:<12} {data['hit@1']:<8.3f} {data['hit@3']:<8.3f} {data['hit@5']:<8.3f} "
                      f"{data['precision@1']:<8.3f} {data['precision@5']:<8.3f} {data['precision@10']:<8.3f} "
                      f"{data['recall@1']:<8.3f} {data['recall@5']:<8.3f} {data['recall@10']:<8.3f} "
                      f"{data['time']:<8.2f}")

def main():
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€å®éªŒè¿è¡Œå™¨')
    parser.add_argument('--clear-cache', action='store_true',
                       help='å®éªŒå‰æ¸…ç†ç¼“å­˜ï¼ˆç°åœ¨é»˜è®¤è‡ªåŠ¨æ¸…ç†ï¼‰')
    parser.add_argument('--no-clear-cache', action='store_true',
                       help='ç¦ç”¨è‡ªåŠ¨ç¼“å­˜æ¸…ç†ï¼ˆç”¨äºåŒä¸€ä¼šè¯å†…çš„è¿ç»­å®éªŒï¼‰')
    parser.add_argument('--no-cache', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰ç¼“å­˜')
    parser.add_argument('--dataset', type=str, required=True,
                       help='æ•°æ®é›†è·¯å¾„æˆ–åç§° (webtable/opendata/nlctables)')
    parser.add_argument('--task', type=str, choices=['join', 'union', 'both'], default='join',
                       help='ä»»åŠ¡ç±»å‹ (bothä¼šåŒæ—¶è¿è¡Œjoinå’Œunion)')
    parser.add_argument('--layer', type=str, choices=['L1', 'L1+L2', 'L1+L2+L3', 'all'], 
                       default='L1+L2+L3', help='è¿è¡Œçš„å±‚çº§ (allè¿è¡Œæ‰€æœ‰å±‚çº§)')
    parser.add_argument('--dataset-type', choices=['subset', 'complete', 'true_subset'], default='subset',
                       help='æ•°æ®é›†ç±»å‹: subset(å­é›†), complete(å®Œæ•´), true_subset(WebTableçš„çœŸå­é›†)')
    parser.add_argument('--max-queries', type=str, default='10',
                       help='æœ€å¤§æŸ¥è¯¢æ•° (æ•°å­—æˆ–"all"è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨)')
    parser.add_argument('--workers', type=int, default=4,
                       help='å¹¶è¡Œè¿›ç¨‹æ•°')
    parser.add_argument('--challenging', action='store_true', default=True,
                       help='ä½¿ç”¨æŒ‘æˆ˜æ€§æ··åˆæŸ¥è¯¢ï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    parser.add_argument('--simple', action='store_true',
                       help='ä½¿ç”¨ç®€å•åŸå§‹æŸ¥è¯¢ï¼ˆç¦ç”¨æŒ‘æˆ˜æ€§æŸ¥è¯¢ï¼‰')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    parser.add_argument('--skip-llm', action='store_true',
                       help='è·³è¿‡LLMå±‚ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰')
    
    args = parser.parse_args()
    
    # å¤„ç†max_querieså‚æ•°
    if args.max_queries.lower() in ['all', '-1', 'none']:
        max_queries = None  # Noneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰æŸ¥è¯¢
        if args.verbose:
            print(f"ğŸ“Š ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†çš„æ‰€æœ‰æŸ¥è¯¢")
    else:
        try:
            max_queries = int(args.max_queries)
            if args.verbose:
                print(f"ğŸ“Š é™åˆ¶æœ€å¤§æŸ¥è¯¢æ•°ä¸º: {max_queries}")
        except ValueError:
            print(f"âš ï¸ æ— æ•ˆçš„max-querieså€¼: {args.max_queries}ï¼Œä½¿ç”¨é»˜è®¤å€¼10")
            max_queries = 10
    
    # å¤„ç†simple/challengingå†²çª
    if args.simple:
        args.challenging = False
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœéœ€è¦è·³è¿‡LLMï¼‰
    if args.skip_llm:
        os.environ['SKIP_LLM'] = 'true'
    else:
        os.environ['SKIP_LLM'] = 'false'
    
    # æ€»æ˜¯æ¸…ç†ç¼“å­˜ä»¥ç¡®ä¿å®éªŒç»“æœçš„å¯é‡å¤æ€§
    # é™¤éæ˜ç¡®æŒ‡å®šäº†--no-clear-cache
    if not hasattr(args, 'no_clear_cache') or not args.no_clear_cache:
        if not args.clear_cache:
            logger.info("ğŸ§¹ è‡ªåŠ¨æ¸…ç†ç¼“å­˜ï¼ˆä½¿ç”¨ --no-clear-cache ç¦ç”¨ï¼‰")
        clear_experiment_cache(args.dataset if args.dataset in ['webtable', 'opendata', 'nlctables'] else None)
    
    # å¦‚æœéœ€è¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆç¼“å­˜
    if args.no_cache:
        logger.info("âš ï¸ å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ‰€æœ‰ç¼“å­˜")
        os.environ['FORCE_REBUILD_CACHE'] = 'true'
    
    # ç¡®å®šæ•°æ®é›†è·¯å¾„
    if args.dataset in ['webtable', 'opendata', 'nlctables']:
        # ä½¿ç”¨é¢„å®šä¹‰è·¯å¾„ï¼Œæ ¹æ®dataset-typeé€‰æ‹©
        if args.dataset == 'webtable':
            # WebTableè·¯å¾„æ ¼å¼ï¼šexamples/webtable/{task}_{dataset_type}/ (ä¸OpenDataç›¸åŒ)
            if args.task == 'both':
                # å¯¹äºbothä»»åŠ¡ï¼Œå…ˆä½¿ç”¨joinæ•°æ®é›†ï¼Œåé¢ä¼šæ ¹æ®ä»»åŠ¡åŠ¨æ€åˆ‡æ¢
                tables_path = f'examples/webtable/join_{args.dataset_type}/tables.json'
            else:
                tables_path = f'examples/webtable/{args.task}_{args.dataset_type}/tables.json'
        elif args.dataset == 'opendata':
            # OpenDataè·¯å¾„æ ¼å¼ï¼šexamples/opendata/{task}_{dataset_type}/
            if args.task == 'both':
                # å¯¹äºbothä»»åŠ¡ï¼Œå…ˆä½¿ç”¨joinæ•°æ®é›†ï¼Œåé¢ä¼šæ ¹æ®ä»»åŠ¡åŠ¨æ€åˆ‡æ¢
                tables_path = f'examples/opendata/join_{args.dataset_type}/tables.json'
            else:
                tables_path = f'examples/opendata/{args.task}_{args.dataset_type}/tables.json'
        elif args.dataset == 'nlctables':
            # NLCTablesè·¯å¾„æ ¼å¼ï¼šexamples/nlctables/{task}_{dataset_type}/
            tables_path = f'examples/nlctables/{args.task}_{args.dataset_type}/tables.json'
        
        dataset_type = args.dataset
    else:
        # ä½¿ç”¨æä¾›çš„è·¯å¾„
        tables_path = args.dataset
        dataset_type = detect_dataset_type(tables_path)
    
    logger.info(f"ğŸ“Š Dataset type detected: {dataset_type}")
    
    # åˆå§‹åŒ–ground_truth_pathï¼ˆå³ä½¿å¯¹NLCTablesä¹Ÿéœ€è¦ï¼‰
    base_dir = Path(tables_path).parent
    ground_truth_path = base_dir / 'ground_truth.json'
    
    # åŠ è½½æ•°æ® - å¯¹NLCTablesä½¿ç”¨é€‚é…å™¨
    if dataset_type == 'nlctables':
        # ä½¿ç”¨é€‚é…å™¨åŠ è½½NLCTablesæ•°æ®
        from nlctables_adapter import NLCTablesAdapter
        adapter = NLCTablesAdapter()
        
        # ä½¿ç”¨å‚æ•°ä¸­çš„dataset_type
        subset_type = args.dataset_type
        
        # å¦‚æœtaskæ˜¯bothï¼Œå…ˆåŠ è½½joinçš„æ•°æ®ï¼ˆåé¢ä¼šæ ¹æ®éœ€è¦é‡æ–°åŠ è½½ï¼‰
        initial_task = 'join' if args.task == 'both' else args.task
        
        tables, queries, ground_truth_list = adapter.load_nlctables_dataset(initial_task, subset_type)
        ground_truth = ground_truth_list  # é€‚é…å™¨å·²ç»è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        logger.info(f"Loaded {len(tables)} tables via NLCTables adapter")
        logger.info(f"Loaded {len(queries)} queries")
    else:
        # åŸæœ‰çš„åŠ è½½é€»è¾‘ï¼ˆWebTable/OpenDataï¼‰
        with open(tables_path, 'r') as f:
            tables = json.load(f)
        logger.info(f"Loaded {len(tables)} tables")
        
        # å¯¹äºOpenDataå’ŒWebTableï¼Œç¡®ä¿è¡¨æœ‰nameå­—æ®µï¼ˆå…¼å®¹æ€§ï¼‰
        if dataset_type in ['opendata', 'webtable']:
            for t in tables:
                if 'name' not in t and 'table_name' in t:
                    t['name'] = t['table_name']
        
        # åŠ è½½æŸ¥è¯¢
        queries_path = base_dir / 'queries.json'
        
        if queries_path.exists():
            with open(queries_path, 'r') as f:
                queries = json.load(f)
            logger.info(f"Loaded {len(queries)} queries")
        else:
            # ç”Ÿæˆé»˜è®¤æŸ¥è¯¢
            queries = [{'query_table': t.get('name', t.get('table_name')), 'task_type': args.task} for t in tables[:10]]
            logger.warning("No queries file found, using first 10 tables as queries")
        
        # åŠ è½½ground truth
        ground_truth_path = base_dir / 'ground_truth.json'
        if ground_truth_path.exists():
            with open(ground_truth_path, 'r') as f:
                ground_truth_list = json.load(f)
            logger.info(f"Loaded {len(ground_truth_list)} ground truth entries")
            # è½¬æ¢ground truthæ ¼å¼
            ground_truth = convert_ground_truth_format(ground_truth_list, task_type=args.task)
        else:
            ground_truth_list = []
            ground_truth = {}
            logger.warning("No ground truth file found")
    
    # ç¡®å®šè¦è¿è¡Œçš„ä»»åŠ¡åˆ—è¡¨
    if args.task == 'both':
        tasks_to_run = ['join', 'union']
    else:
        tasks_to_run = [args.task]
    
    # ç¡®å®šè¦è¿è¡Œçš„å±‚çº§åˆ—è¡¨
    if args.layer == 'all':
        layers_to_run = ['L1', 'L1+L2', 'L1+L2+L3']
    else:
        layers_to_run = [args.layer]
    
    # ä¸ºæ•´ä¸ªå®éªŒå‡†å¤‡ç»Ÿä¸€ç¼“å­˜ï¼ˆæ‰€æœ‰å±‚å’Œä»»åŠ¡å…±äº«ï¼‰
    if not args.skip_llm and len(layers_to_run) > 1:  # åªæœ‰å¤šå±‚æ—¶æ‰éœ€è¦ç»Ÿä¸€ç¼“å­˜
        unified_cache = prepare_unified_cache(tables, dataset_type, tasks_to_run[0])
        # å°†ç¼“å­˜ä¿¡æ¯å­˜å‚¨åˆ°ç¯å¢ƒå˜é‡ä¾›å­è¿›ç¨‹ä½¿ç”¨
        os.environ['UNIFIED_CACHE_DIR'] = str(unified_cache['cache_dir'])
        logger.info(f"  ğŸ“¦ æ‰€æœ‰å±‚å°†å…±äº«ç»Ÿä¸€çš„å‘é‡ç´¢å¼•å’ŒåµŒå…¥")
    
    # è¿è¡Œæ‰€æœ‰ç»„åˆçš„å®éªŒ
    all_results = {}
    for task in tasks_to_run:
        for layer in layers_to_run:
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸš€ è¿è¡Œå®éªŒ: ä»»åŠ¡={task}, å±‚çº§={layer}")
            logger.info(f"{'='*60}")
            
            # æ ¹æ®æ•°æ®é›†ç±»å‹è¿è¡Œå®éªŒ
            if dataset_type == 'nlctables':
                # NLCTableséœ€è¦ä¸ºä¸åŒä»»åŠ¡é‡æ–°åŠ è½½æ•°æ®
                if task != initial_task:
                    # é‡æ–°åŠ è½½å¯¹åº”ä»»åŠ¡çš„æ•°æ®
                    from nlctables_adapter import NLCTablesAdapter
                    adapter = NLCTablesAdapter()
                    current_tables, current_queries, current_ground_truth = adapter.load_nlctables_dataset(task, args.dataset_type)
                    logger.info(f"é‡æ–°åŠ è½½ {task} ä»»åŠ¡æ•°æ®: {len(current_tables)} è¡¨, {len(current_queries)} æŸ¥è¯¢")
                else:
                    # ä½¿ç”¨åˆå§‹åŠ è½½çš„æ•°æ®
                    current_tables, current_queries, current_ground_truth = tables, queries, ground_truth
                
                results, elapsed_time = run_nlctables_experiment(
                    layer=layer,
                    tables=current_tables,
                    queries=current_queries,
                    task_type=task,
                    max_queries=max_queries,
                    max_workers=args.workers,
                    challenging=args.challenging
                )
                
                # ä½¿ç”¨å½“å‰ä»»åŠ¡çš„ground truth
                ground_truth = current_ground_truth
            else:
                # OpenDataå’ŒWebTableéœ€è¦ä¸ºä¸åŒä»»åŠ¡é‡æ–°åŠ è½½æ•°æ®
                if dataset_type in ['opendata', 'webtable'] and args.task == 'both':
                    # ä¸ºOpenData/WebTableçš„ä¸åŒä»»åŠ¡åŠ è½½ç›¸åº”çš„æ•°æ®
                    task_tables_path = f'examples/{dataset_type}/{task}_{args.dataset_type}/tables.json'
                    task_queries_path = f'examples/{dataset_type}/{task}_{args.dataset_type}/queries.json'
                    task_ground_truth_path = f'examples/{dataset_type}/{task}_{args.dataset_type}/ground_truth.json'
                    
                    with open(task_tables_path, 'r') as f:
                        current_tables = json.load(f)
                    with open(task_queries_path, 'r') as f:
                        current_queries = json.load(f)
                    with open(task_ground_truth_path, 'r') as f:
                        current_ground_truth = json.load(f)
                    
                    logger.info(f"åŠ è½½{dataset_type.upper()} {task} ä»»åŠ¡æ•°æ®: {len(current_tables)} è¡¨, {len(current_queries)} æŸ¥è¯¢")
                    
                    # ç¡®ä¿è¡¨æœ‰nameå­—æ®µ
                    for t in current_tables:
                        if 'name' not in t and 'table_name' in t:
                            t['name'] = t['table_name']
                    
                    ground_truth = convert_ground_truth_format(current_ground_truth, task_type=task)
                else:
                    # ä½¿ç”¨å·²åŠ è½½çš„æ•°æ®
                    current_tables = tables
                    current_queries = queries
                    current_ground_truth = ground_truth_list
                    
                results, elapsed_time = run_webtable_opendata_experiment(
                    layer=layer,
                    tables=current_tables,
                    queries=current_queries,
                    task_type=task,
                    dataset_type=dataset_type,
                    max_queries=max_queries,
                    max_workers=args.workers,
                    challenging=args.challenging
                )
            
            # ä¿å­˜ç»“æœå’Œå¯¹åº”çš„ground truth
            experiment_key = f"{task}_{layer}"
            all_results[experiment_key] = {
                'results': results,
                'elapsed_time': elapsed_time,
                'task': task,
                'layer': layer,
                'ground_truth': ground_truth if dataset_type in ['nlctables', 'opendata', 'webtable'] else None  # ä¿å­˜å¯¹åº”çš„ground truth
            }
    
    # è¯„ä¼°æ‰€æœ‰ç»“æœ
    all_metrics = {}
    for exp_key, exp_data in all_results.items():
        results = exp_data['results']
        
        # è¯„ä¼°ç»“æœï¼ˆå¦‚æœæœ‰ground truthï¼‰
        if dataset_type in ['nlctables', 'opendata', 'webtable']:
            # ä½¿ç”¨æ¯ä¸ªå®éªŒä¿å­˜çš„å¯¹åº”ground truth
            exp_ground_truth = exp_data.get('ground_truth')
            if exp_ground_truth:
                metrics = evaluate_results(results, exp_ground_truth)
                all_metrics[exp_key] = metrics
        elif ground_truth_path.exists():
            # å…¶ä»–æ•°æ®é›†ä½¿ç”¨æ–‡ä»¶ä¸­çš„ground truth
            with open(ground_truth_path, 'r') as f:
                ground_truth_raw = json.load(f)
            # è½¬æ¢ground truthæ ¼å¼ï¼ˆå¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ï¼‰
            if isinstance(ground_truth_raw, list):
                # ä»exp_keyä¸­æå–ä»»åŠ¡ç±»å‹ï¼ˆexp_keyæ ¼å¼ä¸º"task_layer"ï¼‰
                exp_task = exp_key.split('_')[0]  # æå–taskéƒ¨åˆ†
                ground_truth = convert_ground_truth_format(ground_truth_raw, task_type=exp_task)
            else:
                ground_truth = ground_truth_raw
            metrics = evaluate_results(results, ground_truth)
            all_metrics[exp_key] = metrics
        
        # æ‰“å°è¯„ä¼°æŒ‡æ ‡
        if exp_key in all_metrics:
            logger.info(f"\nğŸ“ˆ è¯„ä¼°æŒ‡æ ‡ [{exp_key}]:")
            for metric, value in all_metrics[exp_key].items():
                logger.info(f"  {metric}: {value:.3f}")
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_path = args.output
    else:
        # è‡ªåŠ¨ä¿å­˜åˆ°experiment_resultsæ–‡ä»¶å¤¹
        experiment_dir = Path('experiment_results')
        experiment_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        layer_str = args.layer.replace('+', '_')
        filename = f'unified_results_{dataset_type}_{args.task}_{layer_str}_{timestamp}.json'
        output_path = experiment_dir / filename
    
    # æ„å»ºè¾“å‡ºæ•°æ®
    output_data = {
        'dataset': dataset_type,
        'dataset_type': args.dataset_type,
        'task': args.task,
        'layer': args.layer,
        'workers': args.workers,
        'max_queries': max_queries,
        'challenging': args.challenging,
        'experiments': all_results,
        'metrics': all_metrics if all_metrics else None
    }
    
    with open(output_path, 'w') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ğŸ’¾ Results saved to {output_path}")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*60)
    print(f"ğŸ¯ ç»Ÿä¸€å®éªŒå®Œæˆ")
    print(f"   æ•°æ®é›†: {dataset_type} ({args.dataset_type})")
    print(f"   ä»»åŠ¡: {args.task}")
    print(f"   å±‚çº§: {args.layer}")
    print(f"   å¹¶è¡Œè¿›ç¨‹: {args.workers}")
    print(f"   æŸ¥è¯¢æ•°: {max_queries if max_queries else 'all'}")
    print(f"   æŒ‘æˆ˜æ€§æŸ¥è¯¢: {'å¯ç”¨' if args.challenging else 'ç¦ç”¨'}")
    
    # æ‰“å°æ¯ä¸ªå®éªŒçš„ç»“æœ
    for exp_key, exp_data in all_results.items():
        print(f"\n   ğŸ“Š {exp_key}:")
        print(f"      ç”¨æ—¶: {exp_data['elapsed_time']:.2f}s")
        if exp_key in all_metrics:
            metrics = all_metrics[exp_key]
            print(f"      Hit@1: {metrics.get('hit@1', 0):.3f}")
            print(f"      Hit@3: {metrics.get('hit@3', 0):.3f}")
            print(f"      P@5: {metrics.get('precision@5', 0):.3f}")
            print(f"      R@5: {metrics.get('recall@5', 0):.3f}")
    
    print("="*60)
    
    # æ‰“å°ç»Ÿè®¡è¡¨æ ¼
    if all_metrics:
        print_results_table(all_results, all_metrics)
        print()

if __name__ == "__main__":
    main()