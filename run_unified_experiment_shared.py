#!/usr/bin/env python
"""
ä¼˜åŒ–ç‰ˆç»Ÿä¸€å®éªŒè„šæœ¬ - çœŸæ­£çš„å…±äº«èµ„æºå®ç°
æ ¸å¿ƒæ”¹è¿›ï¼š
1. å®Œå…¨æŒ‰ç…§ç”¨æˆ·å‚æ•°å†³å®šè¿›ç¨‹æ•°ï¼Œä¸åšä»»ä½•é™åˆ¶
2. ä¸»è¿›ç¨‹åˆå§‹åŒ–ä¸€æ¬¡ï¼Œå­è¿›ç¨‹é€šè¿‡pickleå…±äº«
3. LLMåœ¨ä¸»è¿›ç¨‹æ‰¹é‡å¤„ç†ï¼Œé¿å…å­è¿›ç¨‹åˆå§‹åŒ–
4. æ”¯æŒä»»æ„æ•°é‡çš„workerè¿›ç¨‹ï¼ˆ1-128æˆ–æ›´å¤šï¼‰
"""

import os
import sys
import json
import time
import pickle
import logging
import argparse
import multiprocessing as mp
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Any
from concurrent.futures import ProcessPoolExecutor, as_completed
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['NUMEXPR_MAX_THREADS'] = '16'
os.environ['USE_REAL_EMBEDDINGS'] = 'true'

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡ï¼šå­è¿›ç¨‹èµ„æºç¼“å­˜
_process_cache = None

def load_shared_resources(cache_file: str) -> Dict:
    """å­è¿›ç¨‹åŠ è½½å…±äº«èµ„æºï¼ˆæ¯ä¸ªè¿›ç¨‹åªåŠ è½½ä¸€æ¬¡ï¼‰"""
    global _process_cache
    
    if _process_cache is None:
        logger.debug(f"è¿›ç¨‹ {os.getpid()} åŠ è½½å…±äº«èµ„æº...")
        with open(cache_file, 'rb') as f:
            _process_cache = pickle.load(f)
    
    return _process_cache

def initialize_resources_in_main(tables: List[Dict], dataset_name: str, 
                                task_type: str, layer: str) -> Dict:
    """
    ä¸»è¿›ç¨‹ä¸€æ¬¡æ€§åˆå§‹åŒ–æ‰€æœ‰å…±äº«èµ„æº
    è¿”å›åºåˆ—åŒ–çš„ç¼“å­˜æ–‡ä»¶è·¯å¾„
    """
    logger.info("="*80)
    logger.info("ğŸš€ ä¸»è¿›ç¨‹åˆå§‹åŒ–å…±äº«èµ„æº")
    logger.info("="*80)
    
    start_time = time.time()
    
    # ç¼“å­˜ç›®å½•
    cache_dir = Path("cache") / dataset_name
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    # ç¼“å­˜key
    cache_key = f"{dataset_name}_{task_type}_{len(tables)}_{layer}"
    cache_file = cache_dir / f"shared_{cache_key}.pkl"
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜
    if cache_file.exists():
        logger.info(f"ğŸ“¦ å‘ç°ç°æœ‰ç¼“å­˜: {cache_file}")
        return {'cache_file': str(cache_file)}
    
    shared_data = {
        'dataset_name': dataset_name,
        'task_type': task_type,
        'table_count': len(tables),
        'layer': layer
    }
    
    # Layer 1: å…ƒæ•°æ®è¿‡æ»¤å™¨
    if 'L1' in layer:
        logger.info("ğŸ” åˆå§‹åŒ–å…ƒæ•°æ®è¿‡æ»¤å™¨...")
        from src.tools.smd_enhanced_metadata_filter import SMDEnhancedMetadataFilter
        metadata_filter = SMDEnhancedMetadataFilter()
        metadata_filter.build_index(tables)
        shared_data['metadata_filter'] = metadata_filter
    
    # Layer 2: å‘é‡åµŒå…¥
    if 'L2' in layer:
        logger.info("ğŸ“Š åŠ è½½å‘é‡åµŒå…¥...")
        embeddings_file = cache_dir / f"table_embeddings_{len(tables)}.pkl"
        
        if not embeddings_file.exists():
            logger.info("âš™ï¸ ç”ŸæˆåµŒå…¥å‘é‡...")
            from precompute_embeddings import precompute_all_embeddings
            precompute_all_embeddings(tables, dataset_name)
        
        with open(embeddings_file, 'rb') as f:
            shared_data['table_embeddings'] = pickle.load(f)
        
        # å°è¯•åŠ è½½HNSWç´¢å¼•
        index_file = cache_dir / f"vector_index_{len(tables)}.pkl"
        if index_file.exists():
            with open(index_file, 'rb') as f:
                shared_data['vector_index'] = pickle.load(f)
        else:
            shared_data['vector_index'] = None
    
    # Layer 3: LLMé…ç½®ï¼ˆä¸åœ¨å­è¿›ç¨‹ä¸­åˆå§‹åŒ–ï¼‰
    if 'L3' in layer:
        logger.info("ğŸ¤– é…ç½®LLMå‚æ•°...")
        shared_data['llm_config'] = {
            'enable': True,
            'confidence_threshold': 0.5,
            'max_candidates': 10
        }
    
    # åºåˆ—åŒ–æ‰€æœ‰èµ„æº
    logger.info(f"ğŸ’¾ ä¿å­˜å…±äº«èµ„æºåˆ°: {cache_file}")
    with open(cache_file, 'wb') as f:
        pickle.dump(shared_data, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    elapsed = time.time() - start_time
    logger.info(f"âœ… åˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f} ç§’")
    
    return {'cache_file': str(cache_file)}

def process_query_worker(args: Tuple) -> Dict:
    """
    å­è¿›ç¨‹å·¥ä½œå‡½æ•° - å¤„ç†å•ä¸ªæŸ¥è¯¢
    æ ¹æ®å±‚çº§æ‰§è¡Œä¸åŒçš„å¤„ç†é€»è¾‘
    """
    query, tables, cache_info, layer = args
    
    # åŠ è½½å…±äº«èµ„æº
    resources = load_shared_resources(cache_info['cache_file'])
    
    query_table_name = query.get('query_table', '')
    
    # æ‰¾åˆ°æŸ¥è¯¢è¡¨
    query_table = None
    for t in tables:
        if t.get('name', t.get('table_name')) == query_table_name:
            query_table = t
            break
    
    if not query_table:
        return {
            'query_table': query_table_name,
            'predictions': [],
            'needs_llm': False
        }
    
    predictions = []
    
    # Layer 1: å…ƒæ•°æ®è¿‡æ»¤
    if 'L1' in layer and 'metadata_filter' in resources:
        metadata_filter = resources['metadata_filter']
        candidates = metadata_filter.filter_candidates(
            query_table,
            threshold=0.5,
            max_candidates=100 if 'L2' in layer else 20
        )
        predictions = [(name, score) for name, score in candidates 
                      if name != query_table_name]
    
    # Layer 2: å‘é‡ç›¸ä¼¼åº¦é‡æ’åº
    if 'L2' in layer and predictions and 'table_embeddings' in resources:
        table_embeddings = resources['table_embeddings']
        
        if query_table_name in table_embeddings:
            query_embedding = np.array(table_embeddings[query_table_name])
            
            reranked = []
            for name, l1_score in predictions:
                if name in table_embeddings:
                    cand_embedding = np.array(table_embeddings[name])
                    # ä½™å¼¦ç›¸ä¼¼åº¦
                    sim = np.dot(query_embedding, cand_embedding) / (
                        np.linalg.norm(query_embedding) * np.linalg.norm(cand_embedding) + 1e-8
                    )
                    # ç»„åˆåˆ†æ•°
                    combined_score = 0.3 * l1_score + 0.7 * float(sim)
                    reranked.append((name, combined_score))
            
            reranked.sort(key=lambda x: x[1], reverse=True)
            predictions = reranked[:20]
    
    # å‡†å¤‡ç»“æœ
    result = {
        'query_table': query_table_name,
        'predictions': [name for name, _ in predictions[:20]],
        'needs_llm': 'L3' in layer  # æ ‡è®°æ˜¯å¦éœ€è¦LLMéªŒè¯
    }
    
    # å¦‚æœéœ€è¦L3ï¼Œè¿”å›æ›´å¤šä¿¡æ¯ä¾›ä¸»è¿›ç¨‹å¤„ç†
    if result['needs_llm']:
        result['query_table_obj'] = query_table
        result['candidate_scores'] = predictions[:10]  # ä¿ç•™åˆ†æ•°ä¾›LLMä½¿ç”¨
    
    return result

def batch_llm_verification(results: List[Dict], tables: List[Dict], 
                          task_type: str) -> List[Dict]:
    """
    ä¸»è¿›ç¨‹æ‰¹é‡è¿›è¡ŒLLMéªŒè¯
    é¿å…åœ¨å­è¿›ç¨‹ä¸­åˆå§‹åŒ–LLM
    """
    if not any(r.get('needs_llm', False) for r in results):
        return results
    
    logger.info("ğŸ¤– å¼€å§‹æ‰¹é‡LLMéªŒè¯...")
    
    # å»¶è¿Ÿå¯¼å…¥ï¼Œåªåœ¨éœ€è¦æ—¶åˆå§‹åŒ–
    from src.tools.llm_matcher import LLMMatcherTool
    
    # åˆå§‹åŒ–LLMï¼ˆåªåœ¨ä¸»è¿›ç¨‹ä¸­åˆå§‹åŒ–ä¸€æ¬¡ï¼‰
    llm_matcher = LLMMatcherTool()
    
    # åˆ›å»ºè¡¨ååˆ°è¡¨å¯¹è±¡çš„æ˜ å°„
    table_dict = {t.get('name', t.get('table_name')): t for t in tables}
    
    # æ‰¹é‡å¤„ç†éœ€è¦LLMéªŒè¯çš„ç»“æœ
    for result in results:
        if not result.get('needs_llm', False):
            continue
        
        query_table = result.get('query_table_obj')
        if not query_table:
            continue
        
        candidate_scores = result.get('candidate_scores', [])
        if not candidate_scores:
            continue
        
        # LLMé‡æ–°è¯„åˆ†
        final_predictions = []
        for cand_name, base_score in candidate_scores[:5]:  # åªéªŒè¯å‰5ä¸ª
            if cand_name in table_dict:
                cand_table = table_dict[cand_name]
                try:
                    # ä½¿ç”¨LLMéªŒè¯
                    is_match, confidence = llm_matcher.verify_match(
                        query_table, cand_table, task_type
                    )
                    if is_match and confidence > 0.5:
                        final_predictions.append(cand_name)
                except:
                    # LLMå¤±è´¥æ—¶ä¿ç•™åŸå§‹é¢„æµ‹
                    if base_score > 0.6:
                        final_predictions.append(cand_name)
        
        # å¦‚æœLLMç­›é€‰å¤ªä¸¥æ ¼ï¼Œä¿ç•™ä¸€äº›é«˜åˆ†åŸå§‹é¢„æµ‹
        if len(final_predictions) < 3:
            for cand_name, score in candidate_scores[:5]:
                if cand_name not in final_predictions and score > 0.7:
                    final_predictions.append(cand_name)
        
        # æ›´æ–°ç»“æœ
        result['predictions'] = final_predictions[:20]
        
        # æ¸…ç†ä¸´æ—¶æ•°æ®
        result.pop('query_table_obj', None)
        result.pop('candidate_scores', None)
        result.pop('needs_llm', None)
    
    logger.info("âœ… LLMéªŒè¯å®Œæˆ")
    return results

def run_experiment(dataset_name: str, task_type: str, layer: str,
                   tables: List[Dict], queries: List[Dict],
                   max_queries: Optional[int] = None,
                   workers: int = None) -> Tuple[Dict, float]:
    """
    è¿è¡Œå®éªŒä¸»å‡½æ•°
    
    Args:
        workers: å·¥ä½œè¿›ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨CPUæ ¸å¿ƒæ•°
    """
    # ç¡®å®šå·¥ä½œè¿›ç¨‹æ•°
    if workers is None:
        workers = mp.cpu_count()
    
    # é™åˆ¶æŸ¥è¯¢æ•°
    if max_queries and max_queries < len(queries):
        queries = queries[:max_queries]
        logger.info(f"ğŸ“Š é™åˆ¶æŸ¥è¯¢æ•°åˆ° {max_queries}")
    
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸš€ è¿è¡Œå®éªŒ: {dataset_name} - {task_type} - {layer}")
    logger.info(f"  è¡¨æ•°é‡: {len(tables)}")
    logger.info(f"  æŸ¥è¯¢æ•°: {len(queries)}")
    logger.info(f"  CPUæ ¸å¿ƒ: {mp.cpu_count()}")
    logger.info(f"  ä½¿ç”¨è¿›ç¨‹: {workers}")
    logger.info(f"{'='*80}")
    
    # ä¸»è¿›ç¨‹åˆå§‹åŒ–å…±äº«èµ„æº
    cache_info = initialize_resources_in_main(tables, dataset_name, task_type, layer)
    
    # å‡†å¤‡è¿›ç¨‹æ± å‚æ•°
    process_args = [
        (query, tables, cache_info, layer)
        for query in queries
    ]
    
    # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†
    results = []
    start_time = time.time()
    
    logger.info(f"ğŸ”„ å¯åŠ¨ {workers} ä¸ªè¿›ç¨‹å¤„ç† {len(queries)} ä¸ªæŸ¥è¯¢...")
    
    with ProcessPoolExecutor(max_workers=workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = [executor.submit(process_query_worker, args) 
                  for args in process_args]
        
        # æ”¶é›†ç»“æœ
        completed = 0
        for future in as_completed(futures):
            try:
                result = future.result(timeout=60)
                results.append(result)
                
                completed += 1
                if completed % 50 == 0 or completed == len(queries):
                    elapsed = time.time() - start_time
                    rate = completed / elapsed if elapsed > 0 else 0
                    remaining = len(queries) - completed
                    eta = remaining / rate if rate > 0 else 0
                    logger.info(f"  è¿›åº¦: {completed}/{len(queries)} "
                              f"({100*completed/len(queries):.1f}%) "
                              f"é€Ÿç‡: {rate:.1f} q/s "
                              f"å‰©ä½™: {eta:.1f}s")
            except Exception as e:
                logger.error(f"å¤„ç†æŸ¥è¯¢å¤±è´¥: {e}")
                results.append({
                    'query_table': 'error',
                    'predictions': []
                })
    
    # å¦‚æœéœ€è¦L3å±‚ï¼Œåœ¨ä¸»è¿›ç¨‹æ‰¹é‡å¤„ç†LLMéªŒè¯
    if 'L3' in layer:
        results = batch_llm_verification(results, tables, task_type)
    
    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    final_results = {
        r['query_table']: r['predictions'] 
        for r in results
    }
    
    elapsed_time = time.time() - start_time
    
    logger.info(f"âœ… å®éªŒå®Œæˆ!")
    logger.info(f"  æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    logger.info(f"  å¹³å‡é€Ÿåº¦: {len(queries)/elapsed_time:.2f} æŸ¥è¯¢/ç§’")
    logger.info(f"  ç»“æœæ•°: {len(final_results)}")
    
    return final_results, elapsed_time

def calculate_metrics(predictions: Dict, ground_truth: Dict) -> Dict:
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    hit_at_k = {1: 0, 3: 0, 5: 0, 10: 0, 20: 0}
    total = 0
    
    for query_table, pred_list in predictions.items():
        if query_table not in ground_truth:
            continue
        
        expected = set(ground_truth[query_table])
        total += 1
        
        for k in hit_at_k.keys():
            if any(p in expected for p in pred_list[:k]):
                hit_at_k[k] += 1
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    if total > 0:
        for k in hit_at_k.keys():
            hit_at_k[k] = hit_at_k[k] / total
    
    return {
        f'hit@{k}': v for k, v in hit_at_k.items()
    }

def main():
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–ç‰ˆç»Ÿä¸€å®éªŒè„šæœ¬')
    
    parser.add_argument('--dataset',
                       choices=['nlctables', 'opendata', 'webtable', 'all'],
                       required=True,
                       help='æ•°æ®é›†åç§°')
    
    parser.add_argument('--task',
                       choices=['join', 'union', 'both'],
                       default='join',
                       help='ä»»åŠ¡ç±»å‹')
    
    parser.add_argument('--dataset-type',
                       choices=['subset', 'complete'],
                       default='subset',
                       help='æ•°æ®é›†ç±»å‹')
    
    parser.add_argument('--max-queries',
                       type=int,
                       default=None,
                       help='æœ€å¤§æŸ¥è¯¢æ•°')
    
    parser.add_argument('--workers',
                       type=int,
                       default=None,
                       help='å·¥ä½œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒï¼‰')
    
    parser.add_argument('--layer',
                       choices=['L1', 'L1+L2', 'L1+L2+L3', 'all'],
                       default='all',
                       help='è¿è¡Œå±‚çº§')
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºé…ç½®
    logger.info("="*80)
    logger.info("âš™ï¸ å®éªŒé…ç½®")
    logger.info(f"  æ•°æ®é›†: {args.dataset}")
    logger.info(f"  ä»»åŠ¡: {args.task}")
    logger.info(f"  ç±»å‹: {args.dataset_type}")
    logger.info(f"  å±‚çº§: {args.layer}")
    logger.info(f"  æŸ¥è¯¢æ•°: {args.max_queries if args.max_queries else 'å…¨éƒ¨'}")
    logger.info(f"  è¿›ç¨‹æ•°: {args.workers if args.workers else f'è‡ªåŠ¨ ({mp.cpu_count()})'}")
    logger.info("="*80)
    
    # å¤„ç†æ•°æ®é›†é€‰æ‹©
    if args.dataset == 'all':
        datasets = ['nlctables', 'opendata', 'webtable']
    else:
        datasets = [args.dataset]
    
    # è¿è¡Œå®éªŒ
    all_results = {}
    
    for dataset in datasets:
        logger.info(f"\n{'#'*80}")
        logger.info(f"# æ•°æ®é›†: {dataset}")
        logger.info('#'*80)
        
        if args.task == 'both':
            for task in ['join', 'union']:
                logger.info(f"\nğŸ“‹ ä»»åŠ¡: {task}")
                
                # åŠ è½½æ•°æ®
                data_dir = Path("examples") / dataset / f"{task}_{args.dataset_type}"
                if not data_dir.exists():
                    logger.warning(f"âš ï¸ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
                    continue
                
                with open(data_dir / "tables.json", 'r') as f:
                    tables = json.load(f)
                with open(data_dir / "queries.json", 'r') as f:
                    queries = json.load(f)
                
                # ç¡®ä¿è¡¨æœ‰nameå­—æ®µ
                for t in tables:
                    if 'name' not in t and 'table_name' in t:
                        t['name'] = t['table_name']
                
                # è¿è¡Œå®éªŒ
                if args.layer == 'all':
                    for layer in ['L1', 'L1+L2', 'L1+L2+L3']:
                        logger.info(f"\nğŸ”¸ å±‚çº§: {layer}")
                        results, elapsed = run_experiment(
                            dataset, task, layer, tables, queries,
                            args.max_queries, args.workers
                        )
                        
                        key = f"{dataset}_{task}_{layer}"
                        all_results[key] = {
                            'results': results,
                            'elapsed': elapsed,
                            'queries': len(results)
                        }
                        
                        # è®¡ç®—æŒ‡æ ‡
                        gt_file = data_dir / "ground_truth.json"
                        if gt_file.exists():
                            with open(gt_file, 'r') as f:
                                ground_truth = json.load(f)
                            metrics = calculate_metrics(results, ground_truth)
                            all_results[key]['metrics'] = metrics
                            logger.info(f"  æŒ‡æ ‡: {metrics}")
                else:
                    results, elapsed = run_experiment(
                        dataset, task, args.layer, tables, queries,
                        args.max_queries, args.workers
                    )
                    
                    key = f"{dataset}_{task}_{args.layer}"
                    all_results[key] = {
                        'results': results,
                        'elapsed': elapsed,
                        'queries': len(results)
                    }
        else:
            # å•ä»»åŠ¡
            # åŠ è½½æ•°æ®
            data_dir = Path("examples") / dataset / f"{args.task}_{args.dataset_type}"
            if not data_dir.exists():
                logger.error(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
                continue
            
            with open(data_dir / "tables.json", 'r') as f:
                tables = json.load(f)
            with open(data_dir / "queries.json", 'r') as f:
                queries = json.load(f)
            
            # ç¡®ä¿è¡¨æœ‰nameå­—æ®µ
            for t in tables:
                if 'name' not in t and 'table_name' in t:
                    t['name'] = t['table_name']
            
            # è¿è¡Œå®éªŒ
            if args.layer == 'all':
                for layer in ['L1', 'L1+L2', 'L1+L2+L3']:
                    logger.info(f"\nğŸ”¸ å±‚çº§: {layer}")
                    results, elapsed = run_experiment(
                        dataset, args.task, layer, tables, queries,
                        args.max_queries, args.workers
                    )
                    
                    key = f"{dataset}_{args.task}_{layer}"
                    all_results[key] = {
                        'results': results,
                        'elapsed': elapsed,
                        'queries': len(results)
                    }
                    
                    # è®¡ç®—æŒ‡æ ‡
                    gt_file = data_dir / "ground_truth.json"
                    if gt_file.exists():
                        with open(gt_file, 'r') as f:
                            ground_truth = json.load(f)
                        
                        # è½¬æ¢ground truthæ ¼å¼
                        if isinstance(ground_truth, list):
                            # Unionæ ¼å¼
                            gt_dict = {}
                            for item in ground_truth:
                                if 'query_table' in item and 'candidate_tables' in item:
                                    gt_dict[item['query_table']] = item['candidate_tables']
                            ground_truth = gt_dict
                        
                        metrics = calculate_metrics(results, ground_truth)
                        all_results[key]['metrics'] = metrics
                        logger.info(f"  æŒ‡æ ‡: Hit@1={metrics['hit@1']:.3f}, "
                                  f"Hit@5={metrics['hit@5']:.3f}, "
                                  f"Hit@20={metrics['hit@20']:.3f}")
            else:
                results, elapsed = run_experiment(
                    dataset, args.task, args.layer, tables, queries,
                    args.max_queries, args.workers
                )
                
                key = f"{dataset}_{args.task}_{args.layer}"
                all_results[key] = {
                    'results': results,
                    'elapsed': elapsed,
                    'queries': len(results)
                }
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"experiment_results/shared_{args.dataset}_{args.task}_{args.layer}_{timestamp}.json"
    
    Path("experiment_results").mkdir(exist_ok=True)
    with open(output_file, 'w') as f:
        json.dump(all_results, f, indent=2)
    
    logger.info(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")
    
    # æ‰“å°æ€»ç»“
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š å®éªŒæ€»ç»“")
    logger.info("="*80)
    
    for key, data in all_results.items():
        logger.info(f"\n{key}:")
        logger.info(f"  æŸ¥è¯¢æ•°: {data['queries']}")
        logger.info(f"  è€—æ—¶: {data['elapsed']:.2f} ç§’")
        logger.info(f"  é€Ÿåº¦: {data['queries']/data['elapsed']:.2f} q/s")
        if 'metrics' in data:
            logger.info(f"  Hit@1: {data['metrics']['hit@1']:.3f}")
            logger.info(f"  Hit@5: {data['metrics']['hit@5']:.3f}")

if __name__ == "__main__":
    main()