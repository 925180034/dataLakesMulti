#!/usr/bin/env python3
"""
æ¨¡å‹ä¸‹è½½å’Œç¯å¢ƒæ£€æŸ¥è„šæœ¬
ç”¨äºå®Œæ•´æµ‹è¯•çš„æ¨¡å‹å‡†å¤‡
"""

import os
import sys
import time
import logging
from pathlib import Path
from typing import Dict, List, Tuple

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_internet_connection() -> bool:
    """æ£€æŸ¥ç½‘ç»œè¿æ¥"""
    try:
        import urllib.request
        urllib.request.urlopen('https://www.google.com', timeout=5)
        return True
    except:
        return False

def check_disk_space(required_gb: float = 2.0) -> bool:
    """æ£€æŸ¥ç£ç›˜ç©ºé—´"""
    try:
        import shutil
        free_bytes = shutil.disk_usage('.').free
        free_gb = free_bytes / (1024**3)
        logger.info(f"å¯ç”¨ç£ç›˜ç©ºé—´: {free_gb:.2f} GB")
        return free_gb >= required_gb
    except Exception as e:
        logger.error(f"æ£€æŸ¥ç£ç›˜ç©ºé—´å¤±è´¥: {e}")
        return False

def install_dependencies() -> Dict[str, bool]:
    """å®‰è£…å¿…éœ€çš„ä¾èµ–åŒ…"""
    dependencies = {
        'sentence-transformers': 'sentence-transformers>=2.7.0',
        'transformers': 'transformers>=4.21.0',
        'torch': 'torch>=1.12.0',
        'numpy': 'numpy>=1.21.0',
        'faiss-cpu': 'faiss-cpu>=1.7.4',
        'hnswlib': 'hnswlib>=0.7.0',
        'chromadb': 'chromadb>=0.4.22',
        'whoosh': 'whoosh>=2.7.4'
    }
    
    results = {}
    
    for name, package in dependencies.items():
        try:
            logger.info(f"æ£€æŸ¥ä¾èµ–: {name}")
            __import__(name.replace('-', '_'))
            results[name] = True
            logger.info(f"âœ… {name} å·²å®‰è£…")
        except ImportError:
            logger.info(f"ğŸ“¦ å®‰è£… {name}...")
            try:
                import subprocess
                result = subprocess.run([
                    sys.executable, '-m', 'pip', 'install', package
                ], capture_output=True, text=True, timeout=300)
                
                if result.returncode == 0:
                    results[name] = True
                    logger.info(f"âœ… {name} å®‰è£…æˆåŠŸ")
                else:
                    results[name] = False
                    logger.error(f"âŒ {name} å®‰è£…å¤±è´¥: {result.stderr}")
            except Exception as e:
                results[name] = False
                logger.error(f"âŒ {name} å®‰è£…å¼‚å¸¸: {e}")
    
    return results

def download_sentence_transformer_models() -> Dict[str, bool]:
    """ä¸‹è½½ SentenceTransformer æ¨¡å‹"""
    models = {
        'all-MiniLM-L6-v2': 'ä¸»è¦åµŒå…¥æ¨¡å‹ (384ç»´, ~90MB)',
        'all-mpnet-base-v2': 'é«˜ç²¾åº¦åµŒå…¥æ¨¡å‹ (768ç»´, ~420MB)',
        'paraphrase-MiniLM-L6-v2': 'é‡Šä¹‰ä¸“ç”¨æ¨¡å‹ (384ç»´, ~90MB)'
    }
    
    results = {}
    
    try:
        from sentence_transformers import SentenceTransformer
        
        for model_name, description in models.items():
            try:
                logger.info(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_name} - {description}")
                start_time = time.time()
                
                # ä¸‹è½½å¹¶åˆå§‹åŒ–æ¨¡å‹
                model = SentenceTransformer(model_name)
                
                # æµ‹è¯•æ¨¡å‹æ˜¯å¦å·¥ä½œ
                test_embedding = model.encode("test sentence")
                
                download_time = time.time() - start_time
                results[model_name] = True
                logger.info(f"âœ… {model_name} ä¸‹è½½å®Œæˆ ({download_time:.1f}s, ç»´åº¦: {len(test_embedding)})")
                
            except Exception as e:
                results[model_name] = False
                logger.error(f"âŒ {model_name} ä¸‹è½½å¤±è´¥: {e}")
    
    except ImportError as e:
        logger.error(f"sentence-transformers æœªæ­£ç¡®å®‰è£…: {e}")
        for model_name in models:
            results[model_name] = False
    
    return results

def verify_model_functionality() -> Dict[str, bool]:
    """éªŒè¯æ¨¡å‹åŠŸèƒ½"""
    tests = {}
    
    # æµ‹è¯• SentenceTransformer
    try:
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # æµ‹è¯•å•ä¸ªæ–‡æœ¬ç¼–ç 
        embedding = model.encode("This is a test sentence")
        assert len(embedding) == 384, f"åµŒå…¥ç»´åº¦é”™è¯¯: {len(embedding)}"
        
        # æµ‹è¯•æ‰¹é‡ç¼–ç 
        batch_embeddings = model.encode(["sentence 1", "sentence 2", "sentence 3"])
        assert batch_embeddings.shape == (3, 384), f"æ‰¹é‡åµŒå…¥å½¢çŠ¶é”™è¯¯: {batch_embeddings.shape}"
        
        tests['sentence_transformer'] = True
        logger.info("âœ… SentenceTransformer åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        tests['sentence_transformer'] = False
        logger.error(f"âŒ SentenceTransformer æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯• FAISS
    try:
        import faiss
        import numpy as np
        
        # åˆ›å»ºæµ‹è¯•ç´¢å¼•
        dimension = 384
        index = faiss.IndexFlatL2(dimension)
        
        # æ·»åŠ æµ‹è¯•å‘é‡
        test_vectors = np.random.random((100, dimension)).astype('float32')
        index.add(test_vectors)
        
        # æœç´¢æµ‹è¯•
        query_vector = np.random.random((1, dimension)).astype('float32')
        distances, indices = index.search(query_vector, 5)
        
        assert len(indices[0]) == 5, f"æœç´¢ç»“æœæ•°é‡é”™è¯¯: {len(indices[0])}"
        
        tests['faiss'] = True
        logger.info("âœ… FAISS åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        tests['faiss'] = False
        logger.error(f"âŒ FAISS æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯• HNSWlib
    try:
        import hnswlib
        import numpy as np
        
        # åˆ›å»ºæµ‹è¯•ç´¢å¼•
        dimension = 384
        index = hnswlib.Index(space='cosine', dim=dimension)
        index.init_index(max_elements=1000, ef_construction=100, M=32)
        
        # æ·»åŠ æµ‹è¯•å‘é‡
        test_vectors = np.random.random((100, dimension)).astype('float32')
        labels = np.arange(100)
        index.add_items(test_vectors, labels)
        
        # æœç´¢æµ‹è¯•
        query_vector = np.random.random((1, dimension)).astype('float32')
        indices, distances = index.knn_query(query_vector, k=5)
        
        assert len(indices[0]) == 5, f"HNSWæœç´¢ç»“æœæ•°é‡é”™è¯¯: {len(indices[0])}"
        
        tests['hnswlib'] = True
        logger.info("âœ… HNSWlib åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        tests['hnswlib'] = False
        logger.error(f"âŒ HNSWlib æµ‹è¯•å¤±è´¥: {e}")
    
    return tests

def check_api_keys() -> Dict[str, bool]:
    """æ£€æŸ¥APIå¯†é’¥é…ç½®"""
    api_keys = {
        'GEMINI_API_KEY': 'Google Gemini API',
        'OPENAI_API_KEY': 'OpenAI API',
        'ANTHROPIC_API_KEY': 'Anthropic Claude API'
    }
    
    results = {}
    for key, description in api_keys.items():
        value = os.getenv(key)
        if value and len(value) > 10:
            results[key] = True
            logger.info(f"âœ… {description} å¯†é’¥å·²é…ç½®")
        else:
            results[key] = False
            logger.warning(f"âš ï¸  {description} å¯†é’¥æœªé…ç½®")
    
    return results

def generate_test_readiness_report(
    internet: bool,
    disk_space: bool,
    dependencies: Dict[str, bool],
    models: Dict[str, bool], 
    functionality: Dict[str, bool],
    api_keys: Dict[str, bool]
) -> str:
    """ç”Ÿæˆæµ‹è¯•å°±ç»ªæŠ¥å‘Š"""
    
    report = []
    report.append("=" * 80)
    report.append("ğŸ§ª å®Œæ•´æµ‹è¯•ç¯å¢ƒå°±ç»ªæŠ¥å‘Š")
    report.append("=" * 80)
    report.append("")
    
    # åŸºç¡€ç¯å¢ƒ
    report.append("ğŸŒ åŸºç¡€ç¯å¢ƒæ£€æŸ¥:")
    report.append(f"  ç½‘ç»œè¿æ¥: {'âœ… æ­£å¸¸' if internet else 'âŒ æ— è¿æ¥'}")
    report.append(f"  ç£ç›˜ç©ºé—´: {'âœ… å……è¶³' if disk_space else 'âŒ ä¸è¶³'}")
    report.append("")
    
    # ä¾èµ–åŒ…
    report.append("ğŸ“¦ ä¾èµ–åŒ…å®‰è£…çŠ¶æ€:")
    for name, status in dependencies.items():
        status_icon = "âœ…" if status else "âŒ"
        report.append(f"  {name}: {status_icon}")
    dependency_success_rate = sum(dependencies.values()) / len(dependencies) * 100
    report.append(f"  å®‰è£…æˆåŠŸç‡: {dependency_success_rate:.1f}%")
    report.append("")
    
    # æ¨¡å‹ä¸‹è½½
    report.append("ğŸ¤– æ¨¡å‹ä¸‹è½½çŠ¶æ€:")
    for name, status in models.items():
        status_icon = "âœ…" if status else "âŒ"
        report.append(f"  {name}: {status_icon}")
    model_success_rate = sum(models.values()) / len(models) * 100 if models else 0
    report.append(f"  ä¸‹è½½æˆåŠŸç‡: {model_success_rate:.1f}%")
    report.append("")
    
    # åŠŸèƒ½æµ‹è¯•
    report.append("âš™ï¸  åŠŸèƒ½éªŒè¯çŠ¶æ€:")
    for name, status in functionality.items():
        status_icon = "âœ…" if status else "âŒ"
        report.append(f"  {name}: {status_icon}")
    functionality_success_rate = sum(functionality.values()) / len(functionality) * 100 if functionality else 0
    report.append(f"  åŠŸèƒ½éªŒè¯ç‡: {functionality_success_rate:.1f}%")
    report.append("")
    
    # APIå¯†é’¥
    report.append("ğŸ”‘ APIå¯†é’¥é…ç½®:")
    configured_keys = sum(api_keys.values())
    for name, status in api_keys.items():
        status_icon = "âœ…" if status else "âš ï¸ "
        report.append(f"  {name}: {status_icon}")
    report.append(f"  é…ç½®å¯†é’¥æ•°: {configured_keys}/{len(api_keys)}")
    report.append("")
    
    # æ€»ä½“è¯„ä¼°
    report.append("ğŸ¯ æµ‹è¯•å°±ç»ªè¯„ä¼°:")
    
    # å¿…éœ€æ¡ä»¶æ£€æŸ¥
    essential_ready = (
        internet and 
        disk_space and 
        dependency_success_rate >= 80 and
        functionality_success_rate >= 80 and
        configured_keys >= 1  # è‡³å°‘æœ‰ä¸€ä¸ªAPIå¯†é’¥
    )
    
    if essential_ready:
        report.append("  âœ… ç¯å¢ƒå·²å°±ç»ªï¼Œå¯ä»¥è¿›è¡Œå®Œæ•´æµ‹è¯•")
        report.append("  ğŸ“‹ å»ºè®®æµ‹è¯•æµç¨‹:")
        report.append("    1. python tests/test_webtable_phase2.py")
        report.append("    2. python tests/test_webtable_phase2_optimized.py") 
        report.append("    3. python run_cli.py discover -q 'test' -t examples/webtable_join_tables.json")
    else:
        report.append("  âŒ ç¯å¢ƒæœªå®Œå…¨å°±ç»ªï¼Œå­˜åœ¨ä»¥ä¸‹é—®é¢˜:")
        if not internet:
            report.append("    â€¢ éœ€è¦ç½‘ç»œè¿æ¥ä¸‹è½½æ¨¡å‹")
        if not disk_space:
            report.append("    â€¢ éœ€è¦è‡³å°‘2GBå¯ç”¨ç£ç›˜ç©ºé—´")
        if dependency_success_rate < 80:
            report.append("    â€¢ éƒ¨åˆ†ä¾èµ–åŒ…å®‰è£…å¤±è´¥")
        if functionality_success_rate < 80:
            report.append("    â€¢ åŠŸèƒ½éªŒè¯å­˜åœ¨é—®é¢˜")
        if configured_keys < 1:
            report.append("    â€¢ éœ€è¦é…ç½®è‡³å°‘ä¸€ä¸ªLLM APIå¯†é’¥")
    
    # å¯é€‰ä¼˜åŒ–å»ºè®®
    report.append("")
    report.append("ğŸ’¡ å¯é€‰ä¼˜åŒ–å»ºè®®:")
    if model_success_rate < 100:
        report.append("  â€¢ å»ºè®®ä¸‹è½½æ‰€æœ‰åµŒå…¥æ¨¡å‹ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
    if configured_keys < 3:
        report.append("  â€¢ é…ç½®å¤šä¸ªAPIå¯†é’¥å¯æä¾›æ›´å¥½çš„å¤‡ä»½")
    report.append("  â€¢ è€ƒè™‘ä½¿ç”¨GPUåŠ é€Ÿä»¥æå‡å‘é‡è®¡ç®—æ€§èƒ½")
    report.append("  â€¢ å®šæœŸæ›´æ–°æ¨¡å‹ç‰ˆæœ¬ä»¥è·å¾—æœ€æ–°æ€§èƒ½")
    
    report.append("")
    report.append("=" * 80)
    
    return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹å®Œæ•´æµ‹è¯•ç¯å¢ƒæ£€æŸ¥å’Œå‡†å¤‡")
    
    # 1. åŸºç¡€ç¯å¢ƒæ£€æŸ¥
    logger.info("ğŸ” æ£€æŸ¥åŸºç¡€ç¯å¢ƒ...")
    internet = check_internet_connection()
    disk_space = check_disk_space()
    
    if not internet:
        logger.error("âŒ æ— ç½‘ç»œè¿æ¥ï¼Œæ— æ³•ä¸‹è½½æ¨¡å‹")
        logger.info("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•")
        return False
    
    if not disk_space:
        logger.error("âŒ ç£ç›˜ç©ºé—´ä¸è¶³")
        logger.info("ğŸ’¡ è¯·æ¸…ç†ç£ç›˜ç©ºé—´åé‡è¯•")
        return False
    
    # 2. å®‰è£…ä¾èµ–
    logger.info("ğŸ“¦ æ£€æŸ¥å’Œå®‰è£…ä¾èµ–åŒ…...")
    dependencies = install_dependencies()
    
    # 3. ä¸‹è½½æ¨¡å‹
    logger.info("ğŸ¤– ä¸‹è½½åµŒå…¥æ¨¡å‹...")
    models = download_sentence_transformer_models()
    
    # 4. åŠŸèƒ½éªŒè¯
    logger.info("âš™ï¸  éªŒè¯åŠŸèƒ½...")
    functionality = verify_model_functionality()
    
    # 5. APIå¯†é’¥æ£€æŸ¥
    logger.info("ğŸ”‘ æ£€æŸ¥APIå¯†é’¥...")
    api_keys = check_api_keys()
    
    # 6. ç”ŸæˆæŠ¥å‘Š
    report = generate_test_readiness_report(
        internet, disk_space, dependencies, models, functionality, api_keys
    )
    
    print(report)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = Path(__file__).parent.parent / "test_readiness_report.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    # è¿”å›æ˜¯å¦å°±ç»ª
    essential_ready = (
        internet and 
        disk_space and 
        sum(dependencies.values()) / len(dependencies) >= 0.8 and
        sum(functionality.values()) / len(functionality) >= 0.8 and
        sum(api_keys.values()) >= 1
    )
    
    return essential_ready

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)