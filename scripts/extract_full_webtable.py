#!/usr/bin/env python3
"""
å®Œæ•´WebTableæ•°æ®é›†æå–å’Œç´¢å¼•å·¥å…·
"""
import asyncio
import json
import pandas as pd
import sys
from pathlib import Path
from typing import List, Dict, Any
import logging

# è®¾ç½®é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.tools.data_indexer import DataIndexer
from src.config.settings import settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FullWebTableExtractor:
    """å®Œæ•´WebTableæ•°æ®é›†æå–å™¨"""
    
    def __init__(self):
        self.webtable_dir = Path("Datasets/webtable/join/tables")
        self.output_dir = Path("examples")
        self.batch_size = 100
        
    def extract_table_metadata(self, csv_file_path: Path) -> Dict[str, Any]:
        """ä»CSVæ–‡ä»¶æå–è¡¨å…ƒæ•°æ®"""
        try:
            # è¯»å–CSVæ–‡ä»¶ (åªè¯»å‰å‡ è¡Œä»¥è·å–ç»“æ„)
            df = pd.read_csv(csv_file_path, nrows=10)
            
            columns = []
            for col_name in df.columns:
                # æ¨æ–­æ•°æ®ç±»å‹
                col_data = df[col_name].dropna()
                if col_data.empty:
                    data_type = "string"
                elif col_data.dtype in ['int64', 'float64']:
                    data_type = "numeric"
                else:
                    data_type = "string"
                
                # è·å–æ ·æœ¬å€¼
                sample_values = col_data.head(5).astype(str).tolist()
                
                columns.append({
                    "table_name": csv_file_path.stem,
                    "column_name": col_name,
                    "data_type": data_type,
                    "sample_values": sample_values
                })
            
            return {
                "table_name": csv_file_path.stem,
                "columns": columns,
                "row_count": len(df),
                "column_count": len(df.columns)
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ {csv_file_path} å¤±è´¥: {e}")
            return None
    
    def extract_all_tables(self) -> List[Dict[str, Any]]:
        """æå–æ‰€æœ‰è¡¨çš„å…ƒæ•°æ®"""
        logger.info(f"å¼€å§‹æå–WebTableæ•°æ®é›†: {self.webtable_dir}")
        
        csv_files = list(self.webtable_dir.glob("*.csv"))
        logger.info(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
        
        all_tables = []
        failed_files = []
        
        for i, csv_file in enumerate(csv_files):
            if i % 100 == 0:
                logger.info(f"å¤„ç†è¿›åº¦: {i}/{len(csv_files)} ({i/len(csv_files)*100:.1f}%)")
            
            table_metadata = self.extract_table_metadata(csv_file)
            if table_metadata:
                all_tables.append(table_metadata)
            else:
                failed_files.append(str(csv_file))
        
        logger.info(f"æˆåŠŸæå– {len(all_tables)} ä¸ªè¡¨")
        if failed_files:
            logger.warning(f"å¤±è´¥æ–‡ä»¶æ•°: {len(failed_files)}")
        
        return all_tables, failed_files
    
    def save_extracted_data(self, tables_data: List[Dict[str, Any]], 
                           failed_files: List[str]):
        """ä¿å­˜æå–çš„æ•°æ®"""
        # ä¿å­˜è¡¨æ•°æ®
        tables_file = self.output_dir / "webtable_full_tables.json"
        with open(tables_file, 'w', encoding='utf-8') as f:
            json.dump(tables_data, f, indent=2, ensure_ascii=False)
        logger.info(f"è¡¨æ•°æ®ä¿å­˜åˆ°: {tables_file}")
        
        # ä¿å­˜åˆ—æ•°æ®
        all_columns = []
        for table in tables_data:
            all_columns.extend(table["columns"])
        
        columns_file = self.output_dir / "webtable_full_columns.json"
        with open(columns_file, 'w', encoding='utf-8') as f:
            json.dump(all_columns, f, indent=2, ensure_ascii=False)
        logger.info(f"åˆ—æ•°æ®ä¿å­˜åˆ°: {columns_file}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_tables": len(tables_data),
            "total_columns": len(all_columns),
            "failed_files": len(failed_files),
            "success_rate": len(tables_data) / (len(tables_data) + len(failed_files)) * 100,
            "avg_columns_per_table": len(all_columns) / len(tables_data) if tables_data else 0,
            "failed_files_list": failed_files[:10]  # åªä¿å­˜å‰10ä¸ªå¤±è´¥æ–‡ä»¶
        }
        
        stats_file = self.output_dir / "webtable_extraction_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        logger.info(f"ç»Ÿè®¡ä¿¡æ¯ä¿å­˜åˆ°: {stats_file}")
        
        return stats
    
    async def create_full_index(self, tables_data: List[Dict[str, Any]]):
        """åˆ›å»ºå®Œæ•´çš„å‘é‡ç´¢å¼•"""
        logger.info("å¼€å§‹åˆ›å»ºå®Œæ•´å‘é‡ç´¢å¼•")
        
        indexer = DataIndexer()
        
        # åˆ†æ‰¹å¤„ç†è¡¨æ•°æ®
        for i in range(0, len(tables_data), self.batch_size):
            batch = tables_data[i:i + self.batch_size]
            logger.info(f"ç´¢å¼•æ‰¹æ¬¡ {i//self.batch_size + 1}: {len(batch)} ä¸ªè¡¨")
            
            try:
                await indexer.index_tables(batch)
            except Exception as e:
                logger.error(f"ç´¢å¼•æ‰¹æ¬¡å¤±è´¥: {e}")
                continue
        
        logger.info("å®Œæ•´ç´¢å¼•åˆ›å»ºå®Œæˆ")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ WebTableå®Œæ•´æ•°æ®é›†æå–å·¥å…·")
    print("=" * 50)
    
    extractor = FullWebTableExtractor()
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not extractor.webtable_dir.exists():
        print(f"âŒ WebTableæ•°æ®ç›®å½•ä¸å­˜åœ¨: {extractor.webtable_dir}")
        return
    
    # æå–æ•°æ®
    print("ğŸ“Š ç¬¬ä¸€é˜¶æ®µ: æå–è¡¨å…ƒæ•°æ®")
    tables_data, failed_files = extractor.extract_all_tables()
    
    if not tables_data:
        print("âŒ æœªèƒ½æå–åˆ°ä»»ä½•è¡¨æ•°æ®")
        return
    
    # ä¿å­˜æ•°æ®
    print("ğŸ’¾ ç¬¬äºŒé˜¶æ®µ: ä¿å­˜æå–æ•°æ®")
    stats = extractor.save_extracted_data(tables_data, failed_files)
    
    print(f"\nğŸ“ˆ æå–ç»Ÿè®¡:")
    print(f"   æ€»è¡¨æ•°: {stats['total_tables']}")
    print(f"   æ€»åˆ—æ•°: {stats['total_columns']}")
    print(f"   æˆåŠŸç‡: {stats['success_rate']:.1f}%")
    print(f"   å¹³å‡æ¯è¡¨åˆ—æ•°: {stats['avg_columns_per_table']:.1f}")
    
    # è¯¢é—®æ˜¯å¦åˆ›å»ºç´¢å¼•
    response = input("\nğŸ”§ æ˜¯å¦ç«‹å³åˆ›å»ºå‘é‡ç´¢å¼•? (y/n): ")
    if response.lower() == 'y':
        print("âš¡ ç¬¬ä¸‰é˜¶æ®µ: åˆ›å»ºå‘é‡ç´¢å¼•")
        await extractor.create_full_index(tables_data)
        print("âœ… å®Œæ•´ç´¢å¼•åˆ›å»ºå®Œæˆ")
    else:
        print("â­ï¸  è·³è¿‡ç´¢å¼•åˆ›å»ºã€‚å¯ä»¥ç¨åä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»º:")
        print("   python run_cli.py index-tables examples/webtable_full_tables.json")
    
    print(f"\nğŸ¯ å®Œæˆ! ä½¿ç”¨ä»¥ä¸‹æ–‡ä»¶è¿›è¡Œæµ‹è¯•:")
    print(f"   è¡¨æ•°æ®: examples/webtable_full_tables.json")
    print(f"   åˆ—æ•°æ®: examples/webtable_full_columns.json")


if __name__ == "__main__":
    asyncio.run(main())