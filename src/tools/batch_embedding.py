"""
æ‰¹é‡åµŒå…¥ç”Ÿæˆå™¨ - æ”¯æŒé«˜æ•ˆçš„æ‰¹é‡å‘é‡è®¡ç®—
"""
import logging
import numpy as np
from typing import List, Dict, Any, Optional
import asyncio
from concurrent.futures import ThreadPoolExecutor
import time

logger = logging.getLogger(__name__)


class BatchEmbeddingGenerator:
    """æ‰¹é‡åµŒå…¥ç”Ÿæˆå™¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = None
        self.dimension = 384  # all-MiniLM-L6-v2 çš„ç»´åº¦
        self._initialized = False
        
    def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self._initialized:
            return
        
        # æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶è·³è¿‡æ¨¡å‹ä¸‹è½½
        import os
        if os.getenv('SKIP_EMBEDDING_DOWNLOAD', '').lower() in ['true', '1', 'yes']:
            logger.info("âš ï¸ æ£€æµ‹åˆ° SKIP_EMBEDDING_DOWNLOAD=trueï¼Œè·³è¿‡æ¨¡å‹ä¸‹è½½ï¼Œä½¿ç”¨è™šæ‹ŸåµŒå…¥")
            self.model = None
            self._initialized = True
            return
            
        if os.getenv('USE_DUMMY_EMBEDDINGS', '').lower() in ['true', '1', 'yes']:
            logger.info("âš ï¸ æ£€æµ‹åˆ° USE_DUMMY_EMBEDDINGS=trueï¼Œä½¿ç”¨è™šæ‹ŸåµŒå…¥å‘é‡")
            self.model = None
            self._initialized = True
            return
            
        try:
            # è®¾ç½®ç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡
            import os
            os.environ['HF_HUB_OFFLINE'] = '1'
            os.environ['TRANSFORMERS_OFFLINE'] = '1'
            os.environ['HF_HOME'] = '/root/.cache/huggingface'
            os.environ['HUGGINGFACE_HUB_CACHE'] = '/root/.cache/huggingface/hub'
            os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
            
            from sentence_transformers import SentenceTransformer
            logger.info(f"ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ‰¹é‡åµŒå…¥æ¨¡å‹: {self.model_name}")
            
            # å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½
            local_model_path = f"/root/.cache/huggingface/hub/models--sentence-transformers--{self.model_name}"
            if os.path.exists(local_model_path):
                logger.info(f"   ä½¿ç”¨æœ¬åœ°ç¼“å­˜æ¨¡å‹ (ç¦»çº¿æ¨¡å¼)")
            else:
                logger.info("   é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ (~90MB)...")
            
            start_time = time.time()
            self.model = SentenceTransformer(self.model_name, cache_folder="/root/.cache/huggingface/hub")
            init_time = time.time() - start_time
            
            logger.info(f"âœ… æ‰¹é‡åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {init_time:.2f}ç§’")
            self._initialized = True
            
        except ImportError:
            logger.error("âŒ sentence-transformersåº“æœªå®‰è£…: pip install sentence-transformers")
            logger.info("ğŸ’¡ è‡ªåŠ¨åˆ‡æ¢åˆ°è™šæ‹ŸåµŒå…¥æ¨¡å¼")
            self.model = None
            self._initialized = True
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
            logger.info("ğŸ’¡ è‡ªåŠ¨åˆ‡æ¢åˆ°è™šæ‹ŸåµŒå…¥æ¨¡å¼")
            self.model = None
            self._initialized = True
    
    def generate_batch(self, texts: List[str], batch_size: int = 32) -> List[np.ndarray]:
        """æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡ - æ ¸å¿ƒä¼˜åŒ–æ–¹æ³•"""
        if not self._initialized:
            self.initialize()
        
        if not texts:
            return []
        
        # å¦‚æœæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨è™šæ‹Ÿå‘é‡
        if self.model is None:
            logger.warning(f"ä½¿ç”¨è™šæ‹ŸåµŒå…¥å‘é‡ï¼Œæ–‡æœ¬æ•°é‡: {len(texts)}")
            return [self._generate_dummy_embedding(text) for text in texts]
        
        try:
            start_time = time.time()
            
            # æ‰¹é‡ç¼–ç ï¼Œè¿™æ˜¯æ€§èƒ½å…³é”®
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=len(texts) > 50,
                convert_to_numpy=True,
                normalize_embeddings=True  # å½’ä¸€åŒ–åµŒå…¥
            )
            
            elapsed = time.time() - start_time
            logger.info(f"æ‰¹é‡ç”Ÿæˆ {len(texts)} ä¸ªåµŒå…¥ï¼Œè€—æ—¶: {elapsed:.2f}ç§’ "
                       f"({len(texts)/elapsed:.1f} ä¸ª/ç§’)")
            
            return [emb for emb in embeddings]
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
            # é™çº§åˆ°è™šæ‹ŸåµŒå…¥
            return [self._generate_dummy_embedding(text) for text in texts]
    
    def generate_table_texts(self, tables: List[Dict]) -> List[str]:
        """ä¸ºè¡¨ç”Ÿæˆæ–‡æœ¬è¡¨ç¤º"""
        texts = []
        for table in tables:
            # æå–è¡¨ä¿¡æ¯
            table_name = table.get('table_name', '')
            columns = table.get('columns', [])
            
            # æ„å»ºæ–‡æœ¬è¡¨ç¤º
            text_parts = [f"è¡¨å: {table_name}"]
            
            # æ·»åŠ åˆ—ä¿¡æ¯ï¼ˆé™åˆ¶æ•°é‡é¿å…è¿‡é•¿ï¼‰
            if columns:
                column_names = []
                column_types = []
                sample_values = []
                
                for col in columns[:15]:  # æœ€å¤š15åˆ—
                    col_name = col.get('column_name', col.get('name', ''))
                    col_type = col.get('data_type', col.get('type', ''))
                    samples = col.get('sample_values', [])
                    
                    if col_name:
                        column_names.append(col_name)
                    if col_type:
                        column_types.append(col_type)
                    if samples:
                        sample_values.extend([str(s) for s in samples[:2] if s])
                
                if column_names:
                    text_parts.append(f"åˆ—å: {' '.join(column_names[:10])}")
                if column_types:
                    text_parts.append(f"ç±»å‹: {' '.join(list(set(column_types))[:5])}")
                if sample_values:
                    text_parts.append(f"æ ·æœ¬: {' '.join(sample_values[:10])}")
            
            texts.append(' | '.join(text_parts))
        
        return texts
    
    def _generate_dummy_embedding(self, text: str) -> np.ndarray:
        """ç”Ÿæˆè™šæ‹ŸåµŒå…¥å‘é‡ï¼ˆç¡®å®šæ€§ï¼‰"""
        import hashlib
        
        # åŸºäºæ–‡æœ¬å†…å®¹ç”Ÿæˆç¡®å®šæ€§çš„è™šæ‹Ÿå‘é‡
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        
        # ä½¿ç”¨å“ˆå¸Œå€¼ç”Ÿæˆå›ºå®šç»´åº¦çš„å‘é‡
        np.random.seed(int(text_hash[:8], 16))  # ä½¿ç”¨å“ˆå¸Œå‰8ä½ä½œä¸ºç§å­
        embedding = np.random.normal(0, 0.3, self.dimension).astype(np.float32)
        
        # å½’ä¸€åŒ–
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    
    def precompute_table_embeddings(self, tables: List[Dict]) -> Dict[str, np.ndarray]:
        """é¢„è®¡ç®—æ‰€æœ‰è¡¨çš„åµŒå…¥å‘é‡"""
        logger.info("å¼€å§‹é¢„è®¡ç®—è¡¨åµŒå…¥å‘é‡...")
        
        start_time = time.time()
        
        # ç”Ÿæˆæ–‡æœ¬è¡¨ç¤º
        texts = self.generate_table_texts(tables)
        table_names = [t.get('table_name', f'table_{i}') for i, t in enumerate(tables)]
        
        # æ‰¹é‡ç”ŸæˆåµŒå…¥
        embeddings = self.generate_batch(texts, batch_size=64)  # æ›´å¤§çš„æ‰¹å¤„ç†
        
        # åˆ›å»ºæ˜ å°„
        embedding_map = {}
        for name, embedding in zip(table_names, embeddings):
            embedding_map[name] = embedding
        
        elapsed = time.time() - start_time
        logger.info(f"é¢„è®¡ç®—å®Œæˆ: {len(tables)} ä¸ªè¡¨ï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
        
        return embedding_map


class OptimizedLLMBatcher:
    """ä¼˜åŒ–çš„LLMæ‰¹é‡è°ƒç”¨å™¨"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
        self.response_cache = {}
        
    async def batch_verify_with_smartskip(self,
                                        query_table: Dict,
                                        candidates: List[Dict],
                                        existing_scores: List[float],
                                        batch_size: int = 10,
                                        confidence_threshold: float = 0.95) -> List[Dict]:
        """æ™ºèƒ½æ‰¹é‡éªŒè¯ - è·³è¿‡é«˜ç½®ä¿¡åº¦å€™é€‰"""
        
        # åˆ†ç¦»é«˜ç½®ä¿¡åº¦å’Œä½ç½®ä¿¡åº¦å€™é€‰
        high_confidence = []
        need_llm_verification = []
        
        for i, (candidate, score) in enumerate(zip(candidates, existing_scores)):
            if score > confidence_threshold:
                # é«˜ç½®ä¿¡åº¦ç›´æ¥é€šè¿‡ï¼Œæ— éœ€LLMéªŒè¯
                high_confidence.append({
                    'index': i,
                    'candidate': candidate,
                    'result': {
                        'is_match': True,
                        'confidence': score,
                        'reason': f'High confidence score: {score:.3f}, skipped LLM verification'
                    }
                })
            else:
                need_llm_verification.append({
                    'index': i,
                    'candidate': candidate,
                    'score': score
                })
        
        logger.info(f"æ™ºèƒ½è·³è¿‡: {len(high_confidence)} ä¸ªé«˜ç½®ä¿¡åº¦å€™é€‰ï¼Œ"
                   f"éœ€è¦LLMéªŒè¯: {len(need_llm_verification)} ä¸ª")
        
        # æ‰¹é‡éªŒè¯ä½ç½®ä¿¡åº¦å€™é€‰
        llm_results = []
        if need_llm_verification:
            llm_results = await self._batch_llm_verify(
                query_table,
                [item['candidate'] for item in need_llm_verification],
                [item['score'] for item in need_llm_verification],
                batch_size
            )
        
        # åˆå¹¶ç»“æœ
        final_results = [None] * len(candidates)
        
        # å¡«å…¥é«˜ç½®ä¿¡åº¦ç»“æœ
        for item in high_confidence:
            final_results[item['index']] = item['result']
        
        # å¡«å…¥LLMéªŒè¯ç»“æœ
        for i, llm_result in enumerate(llm_results):
            original_index = need_llm_verification[i]['index']
            final_results[original_index] = llm_result
        
        return final_results
    
    async def _batch_llm_verify(self,
                              query_table: Dict,
                              candidates: List[Dict],
                              existing_scores: List[float],
                              batch_size: int) -> List[Dict]:
        """çœŸæ­£çš„æ‰¹é‡LLMéªŒè¯"""
        
        # åˆ›å»ºæ‰¹é‡è¯·æ±‚
        batches = [candidates[i:i + batch_size] for i in range(0, len(candidates), batch_size)]
        all_results = []
        
        for batch_idx, batch in enumerate(batches):
            batch_start = time.time()
            
            # å¹¶å‘è°ƒç”¨LLM
            tasks = []
            for candidate in batch:
                task = self._single_llm_verify(query_table, candidate)
                tasks.append(task)
            
            try:
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # å¤„ç†å¼‚å¸¸ç»“æœ
                processed_results = []
                for result in batch_results:
                    if isinstance(result, Exception):
                        processed_results.append({
                            'is_match': False,
                            'confidence': 0.0,
                            'reason': f'LLM verification failed: {str(result)}'
                        })
                    else:
                        processed_results.append(result)
                
                all_results.extend(processed_results)
                
                batch_time = time.time() - batch_start
                logger.info(f"LLMæ‰¹æ¬¡ {batch_idx + 1}/{len(batches)} å®Œæˆï¼Œ"
                           f"è€—æ—¶: {batch_time:.2f}ç§’ï¼Œ"
                           f"é€Ÿåº¦: {len(batch)/batch_time:.1f} ä¸ª/ç§’")
                
            except Exception as e:
                logger.error(f"æ‰¹é‡LLMéªŒè¯å¤±è´¥: {e}")
                # è¿”å›å¤±è´¥ç»“æœ
                all_results.extend([{
                    'is_match': False,
                    'confidence': 0.0,
                    'reason': f'Batch verification failed: {str(e)}'
                }] * len(batch))
        
        return all_results
    
    async def _single_llm_verify(self, query_table: Dict, candidate: Dict) -> Dict:
        """å•ä¸ªLLMéªŒè¯ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"{query_table.get('table_name')}_{candidate.get('table_name')}"
        
        if cache_key in self.response_cache:
            return self.response_cache[cache_key]
        
        # æ„å»ºç®€åŒ–çš„promptä»¥å‡å°‘tokenä½¿ç”¨
        prompt = self._build_efficient_prompt(query_table, candidate)
        
        try:
            response = await self.llm_client.generate(prompt)
            result = self._parse_llm_response(response)
            
            # ç¼“å­˜ç»“æœ
            self.response_cache[cache_key] = result
            return result
            
        except Exception as e:
            result = {
                'is_match': False,
                'confidence': 0.0,
                'reason': f'LLM call failed: {str(e)}'
            }
            return result
    
    def _build_efficient_prompt(self, query_table: Dict, candidate: Dict) -> str:
        """æ„å»ºé«˜æ•ˆçš„promptï¼ˆå‡å°‘tokenï¼‰"""
        query_name = query_table.get('table_name', 'unknown')
        candidate_name = candidate.get('table_name', 'unknown')
        
        # åªä½¿ç”¨å‰5åˆ—çš„ä¿¡æ¯ä»¥å‡å°‘token
        query_cols = [col.get('column_name', '') 
                     for col in query_table.get('columns', [])[:5]]
        candidate_cols = [col.get('column_name', '') 
                         for col in candidate.get('columns', [])[:5]]
        
        prompt = f"""Compare these tables for JOIN compatibility:

Query: {query_name}
Columns: {', '.join(query_cols)}

Candidate: {candidate_name}
Columns: {', '.join(candidate_cols)}

Can they join? Return JSON:
{{"is_match": true/false, "confidence": 0.0-1.0, "reason": "brief explanation"}}"""
        
        return prompt
    
    def _parse_llm_response(self, response: str) -> Dict:
        """è§£æLLMå“åº”"""
        try:
            import json
            
            # æŸ¥æ‰¾JSONéƒ¨åˆ†
            start = response.find('{')
            end = response.rfind('}') + 1
            
            if start >= 0 and end > start:
                json_str = response[start:end]
                result = json.loads(json_str)
                
                # ç¡®ä¿æœ‰å¿…éœ€å­—æ®µ
                result.setdefault('is_match', False)
                result.setdefault('confidence', 0.0)
                result.setdefault('reason', 'No reason provided')
                
                return result
        except:
            pass
        
        # è§£æå¤±è´¥ï¼Œè¿”å›ä¿å®ˆç»“æœ
        return {
            'is_match': False,
            'confidence': 0.0,
            'reason': 'Failed to parse LLM response'
        }