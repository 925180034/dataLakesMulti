"""
æ‰¹é‡LLMå¤„ç†å™¨ - ä¼˜åŒ–çš„æ‰¹é‡APIè°ƒç”¨å®ç°
é€šè¿‡æ‰¹é‡å¤„ç†å’Œå¹¶å‘è°ƒç”¨æ˜¾è‘—æå‡æ€§èƒ½
"""

import asyncio
import logging
import time
from typing import List, Dict, Any, Optional, Callable
from collections import defaultdict
import hashlib
import json

logger = logging.getLogger(__name__)


class BatchLLMProcessor:
    """æ‰¹é‡LLMå¤„ç†å™¨
    
    æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. æ™ºèƒ½æ‰¹é‡åˆå¹¶
    2. å¹¶å‘è°ƒç”¨ç®¡ç†
    3. è‡ªåŠ¨é‡è¯•æœºåˆ¶
    4. ç»“æœç¼“å­˜
    """
    
    def __init__(self, llm_client, max_batch_size: int = 10, max_concurrent: int = 5):
        self.llm_client = llm_client
        self.max_batch_size = max_batch_size
        self.max_concurrent = max_concurrent
        
        # ç¼“å­˜ç³»ç»Ÿ
        self.cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
        # æ€§èƒ½ç»Ÿè®¡
        self.total_calls = 0
        self.total_time = 0
        self.batch_count = 0
        
    async def batch_process(
        self,
        items: List[Dict[str, Any]],
        prompt_builder: Callable,
        response_parser: Callable,
        use_cache: bool = True
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡å¤„ç†å¤šä¸ªé¡¹ç›®
        
        Args:
            items: å¾…å¤„ç†çš„é¡¹ç›®åˆ—è¡¨
            prompt_builder: æ„å»ºæç¤ºçš„å‡½æ•°
            response_parser: è§£æå“åº”çš„å‡½æ•°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        start_time = time.time()
        
        # åˆ†ç»„å¤„ç†
        results = []
        pending_items = []
        
        # æ£€æŸ¥ç¼“å­˜
        for item in items:
            if use_cache:
                cache_key = self._get_cache_key(item)
                if cache_key in self.cache:
                    self.cache_hits += 1
                else:
                    pending_items.append(item)
                    self.cache_misses += 1
            else:
                pending_items.append(item)
        
        # æ‰¹é‡å¤„ç†æœªç¼“å­˜çš„é¡¹ç›®
        if pending_items:
            # åˆ›å»ºæ‰¹æ¬¡
            batches = self._create_batches(pending_items)
            
            # å¹¶å‘å¤„ç†æ‰¹æ¬¡
            batch_tasks = []
            semaphore = asyncio.Semaphore(self.max_concurrent)
            
            for batch in batches:
                task = self._process_batch_with_semaphore(
                    batch, prompt_builder, response_parser, semaphore
                )
                batch_tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # åˆå¹¶ç»“æœ
            for batch_result in batch_results:
                if isinstance(batch_result, Exception):
                    logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {batch_result}")
                    # ä¸ºå¤±è´¥çš„æ‰¹æ¬¡è¿”å›ç©ºç»“æœ
                    results.extend([{} for _ in range(self.max_batch_size)])
                else:
                    results.extend(batch_result)
                    
                    # ç¼“å­˜æˆåŠŸçš„ç»“æœ
                    if use_cache:
                        for item, result in zip(batch, batch_result):
                            cache_key = self._get_cache_key(item)
                            self.cache[cache_key] = result
        
        # åˆå¹¶ç¼“å­˜å’Œæ–°ç»“æœ
        final_results = []
        result_idx = 0
        
        for item in items:
            cache_key = self._get_cache_key(item) if use_cache else None
            if use_cache and cache_key in self.cache:
                # ç›´æ¥ä»ç¼“å­˜è·å–ï¼Œè€Œä¸æ˜¯ä»cached_resultsæ•°ç»„
                final_results.append(self.cache[cache_key])
            else:
                if result_idx < len(results):
                    final_results.append(results[result_idx])
                    result_idx += 1
                else:
                    final_results.append({
                        "table": "unknown",
                        "score": 0.5,
                        "match": False,
                        "reason": "No result",
                        "method": "default"
                    })
        
        # æ›´æ–°ç»Ÿè®¡
        self.total_time += time.time() - start_time
        self.total_calls += len(items)
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {len(items)}ä¸ªé¡¹ç›®, "
                   f"ç¼“å­˜å‘½ä¸­: {self.cache_hits}/{self.cache_hits + self.cache_misses}, "
                   f"è€—æ—¶: {time.time() - start_time:.2f}ç§’")
        
        return final_results
    
    async def _process_batch_with_semaphore(
        self,
        batch: List[Dict[str, Any]],
        prompt_builder: Callable,
        response_parser: Callable,
        semaphore: asyncio.Semaphore
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘å¤„ç†æ‰¹æ¬¡"""
        async with semaphore:
            return await self._process_single_batch(batch, prompt_builder, response_parser)
    
    async def _process_single_batch(
        self,
        batch: List[Dict[str, Any]],
        prompt_builder: Callable,
        response_parser: Callable,
        max_retries: int = 2
    ) -> List[Dict[str, Any]]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        self.batch_count += 1
        
        # æ„å»ºæ‰¹é‡æç¤º
        prompt = prompt_builder(batch)
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(max_retries):
            try:
                # è°ƒç”¨LLMçš„generate_jsonæ–¹æ³•ä»¥è·å¾—ç»“æ„åŒ–è¾“å‡º
                print(f"  ğŸ¤– è°ƒç”¨LLM API - æ‰¹æ¬¡å¤§å°: {len(batch)}")
                if hasattr(self.llm_client, 'generate_json'):
                    # ä¼˜å…ˆä½¿ç”¨generate_jsonæ–¹æ³•
                    try:
                        print(f"  ğŸ“¤ å‘é€åˆ°LLM: generate_json")
                        response = await self.llm_client.generate_json(prompt)
                        print(f"  ğŸ“¥ æ”¶åˆ°LLMå“åº”")
                        # å¦‚æœè¿”å›çš„æ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ä¾›è§£æå™¨å¤„ç†
                        if isinstance(response, dict):
                            response = json.dumps(response)
                        elif isinstance(response, list):
                            response = json.dumps(response)
                    except Exception as e:
                        logger.warning(f"generate_jsonå¤±è´¥ï¼Œå›é€€åˆ°generate: {e}")
                        print(f"  ğŸ“¤ å‘é€åˆ°LLM: generate")
                        response = await self.llm_client.generate(prompt)
                        print(f"  ğŸ“¥ æ”¶åˆ°LLMå“åº”")
                else:
                    # ä½¿ç”¨æ™®é€šgenerateæ–¹æ³•
                    print(f"  ğŸ“¤ å‘é€åˆ°LLM: generate")
                    response = await self.llm_client.generate(prompt)
                    print(f"  ğŸ“¥ æ”¶åˆ°LLMå“åº”")
                
                # è§£æå“åº”
                results = response_parser(response, batch)
                
                # ç¡®ä¿è¿”å›æ­£ç¡®æ•°é‡çš„ç»“æœ
                if len(results) != len(batch):
                    logger.warning(f"ç»“æœæ•°é‡ä¸åŒ¹é…: æœŸæœ›{len(batch)}, å®é™…{len(results)}")
                    # å¡«å……ç¼ºå¤±çš„ç»“æœ
                    while len(results) < len(batch):
                        results.append({
                            "table": "unknown",
                            "score": 0.5,
                            "match": False,
                            "reason": "No response",
                            "method": "default"
                        })
                
                return results
                
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    # è¿”å›é»˜è®¤ç»“æœè€Œéç©ºå­—å…¸
                    return [{
                        "table": batch[i].get("candidate_table", {}).get("name", "unknown") if i < len(batch) else "unknown",
                        "score": 0.5,
                        "match": False,
                        "reason": f"Processing error: {str(e)[:50]}",
                        "method": "error"
                    } for i in range(len(batch))]
                    
                # ç­‰å¾…åé‡è¯•
                await asyncio.sleep(2 ** attempt)
        
        return [{
            "table": batch[i].get("candidate_table", {}).get("name", "unknown") if i < len(batch) else "unknown",
            "score": 0.5,
            "match": False,
            "reason": "Max retries exceeded",
            "method": "error"
        } for i in range(len(batch))]
    
    def _create_batches(self, items: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
        """åˆ›å»ºæ‰¹æ¬¡"""
        batches = []
        for i in range(0, len(items), self.max_batch_size):
            batch = items[i:i + self.max_batch_size]
            batches.append(batch)
        return batches
    
    def _get_cache_key(self, item: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # ä½¿ç”¨itemçš„JSONè¡¨ç¤ºç”Ÿæˆhash
        item_str = json.dumps(item, sort_keys=True)
        return hashlib.md5(item_str.encode()).hexdigest()
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return {
            "total_calls": self.total_calls,
            "total_time": self.total_time,
            "avg_time_per_call": self.total_time / self.total_calls if self.total_calls > 0 else 0,
            "batch_count": self.batch_count,
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "cache_hit_rate": self.cache_hits / (self.cache_hits + self.cache_misses) 
                            if (self.cache_hits + self.cache_misses) > 0 else 0
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        logger.info("ç¼“å­˜å·²æ¸…ç©º")


class TableMatchingPromptBuilder:
    """è¡¨åŒ¹é…æç¤ºæ„å»ºå™¨"""
    
    @staticmethod
    def build_batch_prompt(items: List[Dict[str, Any]]) -> str:
        """æ„å»ºæ‰¹é‡è¡¨åŒ¹é…æç¤º"""
        prompt = """You are a data expert. Analyze if the following table pairs are joinable.

For each candidate table, determine if it can be joined with the query table based on:
1. Common columns that could serve as join keys
2. Compatible data types
3. Semantic relationships

Query Table:
{query_info}

Candidate Tables to evaluate:
{candidates_info}

IMPORTANT: Return ONLY a valid JSON array with exactly {num_items} objects (one per candidate).
Each object must have these exact fields:
- "match": true or false (boolean)
- "confidence": 0.0 to 1.0 (number)
- "reason": brief explanation (string)

Example response format:
[
  {{"match": true, "confidence": 0.9, "reason": "Common ID columns"}},
  {{"match": false, "confidence": 0.2, "reason": "No common columns"}}
]

Response (JSON array only):
"""
        
        # æå–æŸ¥è¯¢è¡¨ä¿¡æ¯ï¼ˆå‡è®¾æ‰€æœ‰itemsä½¿ç”¨ç›¸åŒçš„æŸ¥è¯¢è¡¨ï¼‰
        query_info = items[0].get("query_table", {}) if items else {}
        
        # æå–å€™é€‰è¡¨ä¿¡æ¯
        candidates_info = []
        for i, item in enumerate(items):
            candidate = item.get("candidate_table", {})
            candidates_info.append(f"{i+1}. {candidate.get('name', 'Unknown')}: "
                                  f"{', '.join(c['name'] for c in candidate.get('columns', [])[:5])}")
        
        return prompt.format(
            query_info=json.dumps(query_info, indent=2),
            candidates_info='\n'.join(candidates_info),
            num_items=len(items)
        )
    
    @staticmethod
    def parse_batch_response(response: str, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è§£ææ‰¹é‡å“åº”"""
        try:
            # å¦‚æœresponseå·²ç»æ˜¯å­—ç¬¦ä¸²å½¢å¼çš„JSONï¼Œè§£æå®ƒ
            if isinstance(response, str):
                # å°è¯•æå–JSONéƒ¨åˆ†ï¼ˆå¤„ç†å¯èƒ½çš„é¢å¤–æ–‡æœ¬ï¼‰
                json_start = response.find('[')
                json_end = response.rfind(']') + 1
                if json_start >= 0 and json_end > json_start:
                    json_str = response[json_start:json_end]
                    results = json.loads(json_str)
                else:
                    # å°è¯•æ•´ä¸ªå­—ç¬¦ä¸²
                    results = json.loads(response)
            else:
                # å¦‚æœå·²ç»æ˜¯dictæˆ–listï¼Œç›´æ¥ä½¿ç”¨
                results = response
            
            if not isinstance(results, list):
                results = [results]
            
            # ç¡®ä¿æ¯ä¸ªiteméƒ½æœ‰ç»“æœ
            parsed_results = []
            for i, item in enumerate(items):
                if i < len(results):
                    result = results[i]
                    # å…¼å®¹ä¸åŒçš„å“åº”æ ¼å¼
                    if isinstance(result, dict):
                        # å°è¯•ä»å¤šç§å­—æ®µåè·å–åŒ¹é…çŠ¶æ€
                        match_status = result.get("match", 
                                                result.get("is_match", 
                                                result.get("joinable", False)))
                        
                        # å°è¯•ä»å¤šç§å­—æ®µåè·å–ç½®ä¿¡åº¦
                        confidence = result.get("confidence", 
                                              result.get("score", 
                                              result.get("similarity", 0.5)))
                        
                        parsed_results.append({
                            "table": item.get("candidate_table", {}).get("name", "unknown"),
                            "score": float(confidence) if confidence else 0.5,
                            "match": bool(match_status),
                            "reason": result.get("reason", result.get("explanation", "")),
                            "method": "llm_batch"
                        })
                    else:
                        # å¦‚æœç»“æœä¸æ˜¯å­—å…¸ï¼Œåˆ›å»ºé»˜è®¤ç»“æœ
                        parsed_results.append({
                            "table": item.get("candidate_table", {}).get("name", "unknown"),
                            "score": 0.5,
                            "match": False,
                            "reason": "Invalid response format",
                            "method": "default"
                        })
                else:
                    # é»˜è®¤ç»“æœ
                    parsed_results.append({
                        "table": item.get("candidate_table", {}).get("name", "unknown"),
                        "score": 0.5,
                        "match": False,
                        "reason": "No response",
                        "method": "default"
                    })
            
            return parsed_results
            
        except (json.JSONDecodeError, Exception) as e:
            logger.error(f"å“åº”è§£æå¤±è´¥: {e}, åŸå§‹å“åº”: {str(response)[:200]}")
            # è¿”å›é»˜è®¤ç»“æœ
            return [{
                "table": item.get("candidate_table", {}).get("name", "unknown"),
                "score": 0.5,
                "match": False,
                "reason": f"Parse error: {str(e)[:50]}",
                "method": "error"
            } for item in items]