"""
æ€§èƒ½ç›‘æ§å·¥å…· - å®æ—¶ç›‘æ§ä¸‰å±‚æ¶æ„çš„æ€§èƒ½è¡¨ç°
"""

import logging
import time
import asyncio
from typing import Dict, Any, List, Optional
from collections import defaultdict, deque
from datetime import datetime
import json

logger = logging.getLogger(__name__)


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨
    
    ç›‘æ§æŒ‡æ ‡ï¼š
    1. å“åº”æ—¶é—´ï¼ˆå„å±‚è€—æ—¶ï¼‰
    2. ååé‡ï¼ˆQPSï¼‰
    3. èµ„æºä½¿ç”¨ï¼ˆå†…å­˜ã€ç¼“å­˜ï¼‰
    4. æˆåŠŸç‡å’Œé”™è¯¯ç‡
    """
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        
        # å“åº”æ—¶é—´è®°å½•
        self.response_times = defaultdict(lambda: deque(maxlen=window_size))
        
        # å±‚çº§è€—æ—¶è®°å½•
        self.layer_times = {
            "metadata_filter": deque(maxlen=window_size),
            "vector_search": deque(maxlen=window_size),
            "llm_match": deque(maxlen=window_size)
        }
        
        # æŸ¥è¯¢è®¡æ•°
        self.query_count = 0
        self.success_count = 0
        self.error_count = 0
        
        # ç¼“å­˜å‘½ä¸­ç»Ÿè®¡
        self.cache_hits = 0
        self.cache_misses = 0
        
        # LLMè°ƒç”¨ç»Ÿè®¡
        self.llm_calls = 0
        self.llm_tokens_used = 0
        
        # å¼€å§‹æ—¶é—´
        self.start_time = time.time()
        
        # æ…¢æŸ¥è¯¢æ—¥å¿—
        self.slow_queries = deque(maxlen=50)
        self.slow_query_threshold = 10.0  # 10ç§’
        
    def record_query(
        self,
        query_id: str,
        total_time: float,
        layer_times: Dict[str, float],
        success: bool = True,
        error_msg: Optional[str] = None
    ) -> None:
        """è®°å½•æŸ¥è¯¢æ€§èƒ½æ•°æ®"""
        self.query_count += 1
        
        if success:
            self.success_count += 1
        else:
            self.error_count += 1
        
        # è®°å½•æ€»å“åº”æ—¶é—´
        self.response_times["total"].append(total_time)
        
        # è®°å½•å„å±‚è€—æ—¶
        for layer, layer_time in layer_times.items():
            if layer in self.layer_times:
                self.layer_times[layer].append(layer_time)
        
        # æ£€æŸ¥æ…¢æŸ¥è¯¢
        if total_time > self.slow_query_threshold:
            self.slow_queries.append({
                "query_id": query_id,
                "total_time": total_time,
                "layer_times": layer_times,
                "timestamp": datetime.now().isoformat(),
                "error": error_msg
            })
    
    def record_cache_access(self, hit: bool) -> None:
        """è®°å½•ç¼“å­˜è®¿é—®"""
        if hit:
            self.cache_hits += 1
        else:
            self.cache_misses += 1
    
    def record_llm_call(self, tokens: int = 0) -> None:
        """è®°å½•LLMè°ƒç”¨"""
        self.llm_calls += 1
        self.llm_tokens_used += tokens
    
    def get_metrics(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ€§èƒ½æŒ‡æ ‡"""
        # è®¡ç®—è¿è¡Œæ—¶é—´
        runtime = time.time() - self.start_time
        
        # è®¡ç®—QPS
        qps = self.query_count / runtime if runtime > 0 else 0
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = self.success_count / self.query_count if self.query_count > 0 else 0
        
        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        total_cache_access = self.cache_hits + self.cache_misses
        cache_hit_rate = self.cache_hits / total_cache_access if total_cache_access > 0 else 0
        
        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        avg_response_time = (
            sum(self.response_times["total"]) / len(self.response_times["total"])
            if self.response_times["total"] else 0
        )
        
        # è®¡ç®—å„å±‚å¹³å‡è€—æ—¶
        layer_avg_times = {}
        for layer, times in self.layer_times.items():
            if times:
                layer_avg_times[layer] = sum(times) / len(times)
            else:
                layer_avg_times[layer] = 0
        
        # è®¡ç®—P50, P90, P99
        percentiles = self._calculate_percentiles(list(self.response_times["total"]))
        
        return {
            "summary": {
                "total_queries": self.query_count,
                "success_rate": f"{success_rate:.2%}",
                "qps": f"{qps:.2f}",
                "avg_response_time": f"{avg_response_time:.2f}s",
                "cache_hit_rate": f"{cache_hit_rate:.2%}",
                "runtime": f"{runtime:.0f}s"
            },
            "response_times": {
                "average": f"{avg_response_time:.2f}s",
                "p50": f"{percentiles['p50']:.2f}s",
                "p90": f"{percentiles['p90']:.2f}s",
                "p99": f"{percentiles['p99']:.2f}s",
                "max": f"{max(self.response_times['total']) if self.response_times['total'] else 0:.2f}s"
            },
            "layer_performance": {
                layer: f"{avg_time:.2f}s" 
                for layer, avg_time in layer_avg_times.items()
            },
            "llm_stats": {
                "total_calls": self.llm_calls,
                "avg_calls_per_query": f"{self.llm_calls / self.query_count if self.query_count > 0 else 0:.1f}",
                "total_tokens": self.llm_tokens_used
            },
            "slow_queries": len(self.slow_queries),
            "errors": self.error_count
        }
    
    def _calculate_percentiles(self, values: List[float]) -> Dict[str, float]:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not values:
            return {"p50": 0, "p90": 0, "p99": 0}
        
        sorted_values = sorted(values)
        n = len(sorted_values)
        
        return {
            "p50": sorted_values[int(n * 0.5)],
            "p90": sorted_values[int(n * 0.9)],
            "p99": sorted_values[int(n * 0.99)] if n >= 100 else sorted_values[-1]
        }
    
    def get_slow_queries(self) -> List[Dict[str, Any]]:
        """è·å–æ…¢æŸ¥è¯¢åˆ—è¡¨"""
        return list(self.slow_queries)
    
    def export_metrics(self, filepath: str) -> None:
        """å¯¼å‡ºæ€§èƒ½æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        metrics = self.get_metrics()
        metrics["slow_queries_detail"] = self.get_slow_queries()
        metrics["timestamp"] = datetime.now().isoformat()
        
        with open(filepath, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        logger.info(f"æ€§èƒ½æŒ‡æ ‡å·²å¯¼å‡ºåˆ°: {filepath}")
    
    def reset(self) -> None:
        """é‡ç½®æ‰€æœ‰ç»Ÿè®¡"""
        self.__init__(self.window_size)


class RealtimeMonitor:
    """å®æ—¶æ€§èƒ½ç›‘æ§å™¨ï¼ˆæ”¯æŒå¼‚æ­¥æ›´æ–°ï¼‰"""
    
    def __init__(self, update_interval: float = 1.0):
        self.monitor = PerformanceMonitor()
        self.update_interval = update_interval
        self.running = False
        self._monitor_task = None
    
    async def start(self) -> None:
        """å¯åŠ¨å®æ—¶ç›‘æ§"""
        self.running = True
        self._monitor_task = asyncio.create_task(self._monitor_loop())
        logger.info("å®æ—¶æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")
    
    async def stop(self) -> None:
        """åœæ­¢ç›‘æ§"""
        self.running = False
        if self._monitor_task:
            await self._monitor_task
        logger.info("å®æ—¶æ€§èƒ½ç›‘æ§å·²åœæ­¢")
    
    async def _monitor_loop(self) -> None:
        """ç›‘æ§å¾ªç¯"""
        while self.running:
            try:
                # è·å–å¹¶æ˜¾ç¤ºæŒ‡æ ‡
                metrics = self.monitor.get_metrics()
                self._display_metrics(metrics)
                
                # æ£€æŸ¥å‘Šè­¦
                self._check_alerts(metrics)
                
                await asyncio.sleep(self.update_interval)
                
            except Exception as e:
                logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
    
    def _display_metrics(self, metrics: Dict[str, Any]) -> None:
        """æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡ï¼ˆæ§åˆ¶å°è¾“å‡ºï¼‰"""
        # æ¸…å±ï¼ˆå¯é€‰ï¼‰
        # print("\033[H\033[J")
        
        print("\n" + "="*60)
        print("ğŸ“Š å®æ—¶æ€§èƒ½ç›‘æ§")
        print("="*60)
        
        summary = metrics["summary"]
        print(f"æ€»æŸ¥è¯¢æ•°: {summary['total_queries']} | "
              f"æˆåŠŸç‡: {summary['success_rate']} | "
              f"QPS: {summary['qps']}")
        print(f"å¹³å‡å“åº”: {summary['avg_response_time']} | "
              f"ç¼“å­˜å‘½ä¸­ç‡: {summary['cache_hit_rate']}")
        
        print("\nğŸ“ˆ å“åº”æ—¶é—´åˆ†å¸ƒ:")
        rt = metrics["response_times"]
        print(f"å¹³å‡: {rt['average']} | P50: {rt['p50']} | "
              f"P90: {rt['p90']} | P99: {rt['p99']}")
        
        print("\nâš¡ å„å±‚æ€§èƒ½:")
        for layer, time in metrics["layer_performance"].items():
            print(f"  {layer}: {time}")
        
        print("\nğŸ¤– LLMç»Ÿè®¡:")
        llm = metrics["llm_stats"]
        print(f"æ€»è°ƒç”¨: {llm['total_calls']} | "
              f"å¹³å‡æ¯æŸ¥è¯¢: {llm['avg_calls_per_query']}")
        
        if metrics["slow_queries"] > 0:
            print(f"\nâš ï¸  æ…¢æŸ¥è¯¢: {metrics['slow_queries']}")
        
        if metrics["errors"] > 0:
            print(f"\nâŒ é”™è¯¯æ•°: {metrics['errors']}")
    
    def _check_alerts(self, metrics: Dict[str, Any]) -> None:
        """æ£€æŸ¥å¹¶è§¦å‘å‘Šè­¦"""
        # æˆåŠŸç‡å‘Šè­¦
        success_rate = float(metrics["summary"]["success_rate"].rstrip('%')) / 100
        if success_rate < 0.95:
            logger.warning(f"âš ï¸ æˆåŠŸç‡ä½äº95%: {metrics['summary']['success_rate']}")
        
        # å“åº”æ—¶é—´å‘Šè­¦
        avg_time = float(metrics["response_times"]["average"].rstrip('s'))
        if avg_time > 10:
            logger.warning(f"âš ï¸ å¹³å‡å“åº”æ—¶é—´è¶…è¿‡10ç§’: {avg_time}s")
        
        # ç¼“å­˜å‘½ä¸­ç‡å‘Šè­¦
        cache_hit_rate = float(metrics["summary"]["cache_hit_rate"].rstrip('%')) / 100
        if cache_hit_rate < 0.3:
            logger.warning(f"âš ï¸ ç¼“å­˜å‘½ä¸­ç‡ä½äº30%: {metrics['summary']['cache_hit_rate']}")
    
    def record_query(self, *args, **kwargs) -> None:
        """ä»£ç†åˆ°åº•å±‚ç›‘æ§å™¨"""
        self.monitor.record_query(*args, **kwargs)
    
    def record_cache_access(self, *args, **kwargs) -> None:
        """ä»£ç†åˆ°åº•å±‚ç›‘æ§å™¨"""
        self.monitor.record_cache_access(*args, **kwargs)
    
    def record_llm_call(self, *args, **kwargs) -> None:
        """ä»£ç†åˆ°åº•å±‚ç›‘æ§å™¨"""
        self.monitor.record_llm_call(*args, **kwargs)


# å…¨å±€ç›‘æ§å®ä¾‹
_global_monitor = None


def get_monitor() -> PerformanceMonitor:
    """è·å–å…¨å±€ç›‘æ§å®ä¾‹"""
    global _global_monitor
    if _global_monitor is None:
        _global_monitor = PerformanceMonitor()
    return _global_monitor


def reset_monitor() -> None:
    """é‡ç½®å…¨å±€ç›‘æ§å™¨"""
    global _global_monitor
    if _global_monitor:
        _global_monitor.reset()


# ä¾¿æ·è£…é¥°å™¨
def monitor_performance(layer_name: str):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            monitor = get_monitor()
            start_time = time.time()
            
            try:
                result = await func(*args, **kwargs)
                elapsed = time.time() - start_time
                
                # è®°å½•å±‚çº§æ€§èƒ½
                monitor.layer_times[layer_name].append(elapsed)
                
                return result
                
            except Exception as e:
                elapsed = time.time() - start_time
                monitor.record_query(
                    query_id=f"{layer_name}_{int(start_time)}",
                    total_time=elapsed,
                    layer_times={layer_name: elapsed},
                    success=False,
                    error_msg=str(e)
                )
                raise
        
        return wrapper
    return decorator