#!/usr/bin/env python
"""
æµ‹è¯•ä¼˜åŒ–åçš„ç³»ç»Ÿæ€§èƒ½
éªŒè¯æ˜¯å¦è¾¾åˆ°3-8ç§’çš„æŸ¥è¯¢ç›®æ ‡
"""

import asyncio
import json
import time
import logging
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ç¦ç”¨æŸäº›è¯¦ç»†æ—¥å¿—
logging.getLogger("sentence_transformers").setLevel(logging.WARNING)


async def test_optimized_performance():
    """æµ‹è¯•ä¼˜åŒ–åçš„æ€§èƒ½"""
    from src.core.workflow import discover_data
    from src.utils.data_parser import parse_tables_data
    
    print("\n" + "="*60)
    print("ğŸš€ æ•°æ®æ¹–å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ - ä¼˜åŒ–æ€§èƒ½æµ‹è¯•")
    print("="*60)
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
    with open('examples/final_subset_tables.json') as f:
        all_tables_data = json.load(f)
    
    print(f"âœ… å·²åŠ è½½ {len(all_tables_data)} ä¸ªè¡¨")
    
    # å‡†å¤‡æµ‹è¯•æŸ¥è¯¢
    test_queries = all_tables_data[:5]  # æµ‹è¯•å‰5ä¸ªè¡¨
    
    print(f"\nğŸ“ æµ‹è¯•é…ç½®:")
    print(f"  - æŸ¥è¯¢æ•°é‡: {len(test_queries)}")
    print(f"  - æ•°æ®é›†å¤§å°: {len(all_tables_data)} è¡¨")
    print(f"  - ä¼˜åŒ–åŠŸèƒ½: æ‰¹é‡APIè°ƒç”¨, å¹¶è¡Œå¤„ç†, å¤šçº§ç¼“å­˜")
    print(f"  - ç›®æ ‡æ€§èƒ½: 3-8ç§’/æŸ¥è¯¢")
    
    # ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆåŒ…å«åˆå§‹åŒ–ï¼‰
    print("\n" + "-"*60)
    print("ğŸ”„ ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆåŒ…å«åˆå§‹åŒ–å¼€é”€ï¼‰")
    print("-"*60)
    
    first_run_times = []
    for i, query_table in enumerate(test_queries, 1):
        print(f"\næŸ¥è¯¢ {i}/{len(test_queries)}: {query_table['table_name']}")
        
        start_time = time.time()
        
        try:
            result = await discover_data(
                user_query=f"Find joinable tables for {query_table['table_name']}",
                query_tables=[query_table],
                all_tables_data=all_tables_data if i == 1 else None,  # åªç¬¬ä¸€æ¬¡åˆå§‹åŒ–
                use_optimized=True
            )
            
            query_time = time.time() - start_time
            first_run_times.append(query_time)
            
            # æ£€æŸ¥ç»“æœ
            if hasattr(result, 'table_matches') and result.table_matches:
                match_count = len(result.table_matches)
                print(f"  âœ… æ‰¾åˆ° {match_count} ä¸ªåŒ¹é…")
                print(f"  â±ï¸ æŸ¥è¯¢æ—¶é—´: {query_time:.2f}ç§’")
                
                # æ˜¾ç¤ºå‰3ä¸ªåŒ¹é…
                for j, match in enumerate(result.table_matches[:3], 1):
                    print(f"     {j}. {match.target_table} (åˆ†æ•°: {match.score:.2f})")
            else:
                print(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…")
                print(f"  â±ï¸ æŸ¥è¯¢æ—¶é—´: {query_time:.2f}ç§’")
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            first_run_times.append(30.0)  # é»˜è®¤è¶…æ—¶æ—¶é—´
    
    # ç¬¬äºŒæ¬¡è¿è¡Œï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
    print("\n" + "-"*60)
    print("ğŸ”„ ç¬¬äºŒæ¬¡è¿è¡Œï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰")
    print("-"*60)
    
    second_run_times = []
    for i, query_table in enumerate(test_queries, 1):
        print(f"\næŸ¥è¯¢ {i}/{len(test_queries)}: {query_table['table_name']}")
        
        start_time = time.time()
        
        try:
            result = await discover_data(
                user_query=f"Find joinable tables for {query_table['table_name']}",
                query_tables=[query_table],
                use_optimized=True  # ä¸å†åˆå§‹åŒ–ï¼Œä½¿ç”¨ç¼“å­˜
            )
            
            query_time = time.time() - start_time
            second_run_times.append(query_time)
            
            # æ£€æŸ¥ç»“æœ
            if hasattr(result, 'table_matches') and result.table_matches:
                match_count = len(result.table_matches)
                print(f"  âœ… æ‰¾åˆ° {match_count} ä¸ªåŒ¹é…")
                print(f"  â±ï¸ æŸ¥è¯¢æ—¶é—´: {query_time:.2f}ç§’ï¼ˆç¼“å­˜åŠ é€Ÿï¼‰")
            else:
                print(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…")
                print(f"  â±ï¸ æŸ¥è¯¢æ—¶é—´: {query_time:.2f}ç§’")
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            second_run_times.append(30.0)
    
    # æ€§èƒ½åˆ†æ
    print("\n" + "="*60)
    print("ğŸ“Š æ€§èƒ½åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    if first_run_times:
        avg_first = sum(first_run_times) / len(first_run_times)
        min_first = min(first_run_times)
        max_first = max(first_run_times)
        
        print("\nç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆå«åˆå§‹åŒ–ï¼‰:")
        print(f"  - å¹³å‡æ—¶é—´: {avg_first:.2f}ç§’")
        print(f"  - æœ€å¿«æ—¶é—´: {min_first:.2f}ç§’")
        print(f"  - æœ€æ…¢æ—¶é—´: {max_first:.2f}ç§’")
    
    if second_run_times:
        avg_second = sum(second_run_times) / len(second_run_times)
        min_second = min(second_run_times)
        max_second = max(second_run_times)
        
        print("\nç¬¬äºŒæ¬¡è¿è¡Œï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰:")
        print(f"  - å¹³å‡æ—¶é—´: {avg_second:.2f}ç§’")
        print(f"  - æœ€å¿«æ—¶é—´: {min_second:.2f}ç§’")
        print(f"  - æœ€æ…¢æ—¶é—´: {max_second:.2f}ç§’")
        
        if first_run_times:
            speedup = avg_first / avg_second
            print(f"\nç¼“å­˜åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    # æ€§èƒ½è¯„ä¼°
    print("\n" + "-"*60)
    print("ğŸ¯ æ€§èƒ½ç›®æ ‡è¯„ä¼°")
    print("-"*60)
    
    target_met = False
    if second_run_times:
        # ä½¿ç”¨ç¼“å­˜åçš„æ€§èƒ½ä½œä¸ºè¯„ä¼°æ ‡å‡†
        if avg_second <= 8:
            print(f"âœ… æ€§èƒ½è¾¾æ ‡ï¼å¹³å‡æŸ¥è¯¢æ—¶é—´ {avg_second:.2f}ç§’ â‰¤ 8ç§’ç›®æ ‡")
            target_met = True
        else:
            print(f"âš ï¸ æ€§èƒ½æœªè¾¾æ ‡ã€‚å¹³å‡æŸ¥è¯¢æ—¶é—´ {avg_second:.2f}ç§’ > 8ç§’ç›®æ ‡")
            print("\nå»ºè®®ä¼˜åŒ–æ–¹å‘:")
            print("  1. è¿›ä¸€æ­¥å‡å°‘LLMè°ƒç”¨æ¬¡æ•°")
            print("  2. ä¼˜åŒ–å‘é‡æœç´¢ç®—æ³•")
            print("  3. å®ç°æ›´æ¿€è¿›çš„ç¼“å­˜ç­–ç•¥")
            print("  4. ä½¿ç”¨æ›´å¿«çš„LLMæ¨¡å‹")
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    results = {
        "test_config": {
            "query_count": len(test_queries),
            "dataset_size": len(all_tables_data),
            "optimizations": ["batch_api", "parallel_processing", "multi_level_cache"]
        },
        "first_run": {
            "times": first_run_times,
            "avg": avg_first if first_run_times else 0,
            "min": min_first if first_run_times else 0,
            "max": max_first if first_run_times else 0
        },
        "second_run": {
            "times": second_run_times,
            "avg": avg_second if second_run_times else 0,
            "min": min_second if second_run_times else 0,
            "max": max_second if second_run_times else 0
        },
        "performance": {
            "cache_speedup": speedup if first_run_times and second_run_times else 1,
            "target_met": target_met,
            "target_range": "3-8 seconds"
        }
    }
    
    with open('optimized_performance_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("\nâœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° optimized_performance_results.json")
    
    return target_met


if __name__ == "__main__":
    target_met = asyncio.run(test_optimized_performance())
    
    # é€€å‡ºç ï¼š0è¡¨ç¤ºè¾¾æ ‡ï¼Œ1è¡¨ç¤ºæœªè¾¾æ ‡
    exit(0 if target_met else 1)