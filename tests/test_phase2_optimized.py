#!/usr/bin/env python3
"""
é˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯• - éªŒè¯æ€§èƒ½ä¼˜åŒ–ç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import asyncio
import json
import logging
import time
import sys
from pathlib import Path

# è®¾ç½®é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.tools.data_indexer import build_webtable_indices
from src.core.workflow import discover_data
from src.core.models import TableInfo, ColumnInfo
from src.config.settings import settings

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)


async def create_large_test_dataset():
    """åˆ›å»ºå¤§è§„æ¨¡æµ‹è¯•æ•°æ®é›†ä»¥éªŒè¯æ€§èƒ½ä¼˜åŒ–æ•ˆæœ"""
    
    # åˆ›å»ºå¤§è§„æ¨¡è¡¨æ•°æ®ï¼ˆæ¨¡æ‹ŸçœŸå®WebTableç¯å¢ƒï¼‰
    test_tables = []
    
    # ç”¨æˆ·ç›¸å…³è¡¨
    for i in range(20):
        test_tables.append({
            "name": f"users_dataset_{i}",
            "columns": [
                {
                    "name": "user_id",
                    "type": "int",
                    "sample_values": [str(j) for j in range(1000, 1100)]
                },
                {
                    "name": "email",
                    "type": "string",
                    "sample_values": [f"user{j}@example{i}.com" for j in range(10)]
                },
                {
                    "name": "full_name",
                    "type": "string", 
                    "sample_values": [f"User Name {j}" for j in range(10)]
                },
                {
                    "name": "age",
                    "type": "int",
                    "sample_values": [str(j) for j in range(18, 80, 5)]
                },
                {
                    "name": "registration_date",
                    "type": "date",
                    "sample_values": ["2023-01-01", "2023-06-15", "2024-01-01"]
                }
            ],
            "row_count": 50000
        })
    
    # å®¢æˆ·ç›¸å…³è¡¨
    for i in range(15):
        test_tables.append({
            "name": f"customers_db_{i}",
            "columns": [
                {
                    "name": "customer_id",
                    "type": "int",
                    "sample_values": [str(j) for j in range(2000, 2100)]
                },
                {
                    "name": "email_address",
                    "type": "string",
                    "sample_values": [f"customer{j}@company{i}.com" for j in range(10)]
                },
                {
                    "name": "customer_name",
                    "type": "string",
                    "sample_values": [f"Customer {j}" for j in range(10)]
                },
                {
                    "name": "phone",
                    "type": "string", 
                    "sample_values": [f"555-{1000+j}" for j in range(100)]
                },
                {
                    "name": "created_at",
                    "type": "timestamp",
                    "sample_values": ["2023-01-01 10:00:00", "2023-12-31 23:59:59"]
                }
            ],
            "row_count": 30000
        })
    
    # äº§å“ç›¸å…³è¡¨
    for i in range(10):
        test_tables.append({
            "name": f"products_catalog_{i}",
            "columns": [
                {
                    "name": "product_id",
                    "type": "string",
                    "sample_values": [f"P{j:04d}" for j in range(1000)]
                },
                {
                    "name": "product_name",
                    "type": "string",
                    "sample_values": ["Laptop", "Desktop", "Tablet", "Phone", "Monitor"]
                },
                {
                    "name": "price",
                    "type": "decimal",
                    "sample_values": ["999.99", "1299.99", "599.99", "299.99"]
                },
                {
                    "name": "category",
                    "type": "string",
                    "sample_values": ["Electronics", "Computers", "Accessories"]
                }
            ],
            "row_count": 10000
        })
    
    return test_tables


async def test_phase2_optimization():
    """æµ‹è¯•é˜¶æ®µäºŒä¼˜åŒ–ç»„ä»¶"""
    try:
        logger.info("ğŸš€ å¼€å§‹é˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯•")
        
        # 1. åˆ›å»ºå¤§è§„æ¨¡æµ‹è¯•æ•°æ®
        logger.info("\nğŸ“Š æ­¥éª¤1: åˆ›å»ºå¤§è§„æ¨¡æµ‹è¯•æ•°æ®")
        test_tables = await create_large_test_dataset()
        
        # ä¿å­˜æµ‹è¯•æ•°æ®
        test_data_dir = Path(__file__).parent / "test_data"
        test_data_dir.mkdir(exist_ok=True)
        
        large_tables_file = test_data_dir / "large_test_tables.json"
        with open(large_tables_file, 'w', encoding='utf-8') as f:
            json.dump(test_tables, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… åˆ›å»ºäº† {len(test_tables)} ä¸ªå¤§è§„æ¨¡æµ‹è¯•è¡¨")
        total_columns = sum(len(table['columns']) for table in test_tables)
        logger.info(f"   æ€»åˆ—æ•°: {total_columns}")
        
        # 2. æ„å»ºç´¢å¼•ï¼ˆæµ‹è¯•é˜¶æ®µäºŒä¼˜åŒ–çš„ç´¢å¼•æ„å»ºæ€§èƒ½ï¼‰
        logger.info("\nğŸ”§ æ­¥éª¤2: æ„å»ºå¤§è§„æ¨¡ç´¢å¼•ï¼ˆæµ‹è¯•ä¼˜åŒ–æ•ˆæœï¼‰")
        
        start_time = time.time()
        
        index_result = await build_webtable_indices(
            tables_file=str(large_tables_file),
            columns_file=None,
            save_path=None
        )
        
        index_time = time.time() - start_time
        
        if index_result['status'] != 'success':
            raise Exception(f"ç´¢å¼•æ„å»ºå¤±è´¥: {index_result.get('error')}")
        
        logger.info("âœ… å¤§è§„æ¨¡ç´¢å¼•æ„å»ºå®Œæˆ")
        logger.info(f"   ç´¢å¼•æ„å»ºæ—¶é—´: {index_time:.2f}ç§’")
        logger.info(f"   å¤„ç†è¡¨æ•°: {index_result['tables_processed']}")
        logger.info(f"   ç´¢å¼•è¡¨æ•°: {index_result['tables_indexed']}")
        logger.info(f"   å¤„ç†åˆ—æ•°: {index_result['columns_processed']}")
        logger.info(f"   ç´¢å¼•åˆ—æ•°: {index_result['columns_indexed']}")
        
        # 3. åˆ›å»ºå¤æ‚æŸ¥è¯¢æ•°æ®
        logger.info("\nğŸ“ æ­¥éª¤3: åˆ›å»ºå¤æ‚æŸ¥è¯¢æ•°æ®")
        
        query_table = {
            "table_name": "target_user_profiles",
            "columns": [
                {
                    "table_name": "target_user_profiles",
                    "column_name": "id",
                    "data_type": "int",
                    "sample_values": ["1", "2", "3"]
                },
                {
                    "table_name": "target_user_profiles",
                    "column_name": "email",
                    "data_type": "string", 
                    "sample_values": ["john@example.com", "jane@example.com"]
                },
                {
                    "table_name": "target_user_profiles",
                    "column_name": "name",
                    "data_type": "string",
                    "sample_values": ["John Doe", "Jane Smith"]
                }
            ]
        }
        
        # 4. æ‰§è¡Œæ•°æ®å‘ç°ï¼ˆæµ‹è¯•é˜¶æ®µäºŒä¼˜åŒ–çš„æœç´¢æ€§èƒ½ï¼‰
        logger.info("\nğŸ” æ­¥éª¤4: æ‰§è¡Œä¼˜åŒ–çš„æ•°æ®å‘ç°")
        
        start_time = time.time()
        
        result = await discover_data(
            user_query="find tables with user information, email addresses, and personal data for data integration",
            query_tables=[query_table],
            query_columns=None
        )
        
        discovery_time = time.time() - start_time
        
        # 5. åˆ†æç»“æœå’Œæ€§èƒ½
        logger.info("\nğŸ“ˆ æ­¥éª¤5: åˆ†ææ€§èƒ½ä¼˜åŒ–æ•ˆæœ")
        
        logger.info(f"âœ… æ•°æ®å‘ç°å®Œæˆ")
        logger.info(f"   å‘ç°æ—¶é—´: {discovery_time:.2f}ç§’")
        
        # å¤„ç†è¿”å›ç»“æœç±»å‹
        if isinstance(result, dict):
            from src.core.models import AgentState
            result = AgentState.from_dict(result)
        
        logger.info(f"   å·¥ä½œæµçŠ¶æ€: {result.current_step}")
        logger.info(f"   ç­–ç•¥: {result.strategy}")
        
        if result.final_results:
            logger.info(f"ğŸ‰ æ‰¾åˆ° {len(result.final_results)} ä¸ªåŒ¹é…ç»“æœ")
            
            # åˆ†æåŒ¹é…è´¨é‡
            high_quality_matches = [r for r in result.final_results if r.score > 80]
            medium_quality_matches = [r for r in result.final_results if 60 <= r.score <= 80]
            
            logger.info(f"   é«˜è´¨é‡åŒ¹é… (>80åˆ†): {len(high_quality_matches)}")
            logger.info(f"   ä¸­ç­‰è´¨é‡åŒ¹é… (60-80åˆ†): {len(medium_quality_matches)}")
            
            # æ˜¾ç¤ºå‰5ä¸ªæœ€ä½³åŒ¹é…
            logger.info("\nğŸ† å‰5ä¸ªæœ€ä½³åŒ¹é…:")
            for i, match in enumerate(result.final_results[:5], 1):
                logger.info(f"   {i}. {match.target_table} (è¯„åˆ†: {match.score:.1f}, åŒ¹é…åˆ—: {len(match.matched_columns)})")
        
        else:
            logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°åŒ¹é…ç»“æœ")
        
        # 6. æ€§èƒ½æŒ‡æ ‡æ€»ç»“
        logger.info("\nğŸ“Š é˜¶æ®µäºŒä¼˜åŒ–æ€§èƒ½æ€»ç»“")
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        tables_per_second = len(test_tables) / index_time if index_time > 0 else 0
        columns_per_second = total_columns / index_time if index_time > 0 else 0
        search_throughput = len(test_tables) / discovery_time if discovery_time > 0 else 0
        
        logger.info(f"ğŸš€ ç´¢å¼•æ„å»ºæ€§èƒ½:")
        logger.info(f"   è¡¨å¤„ç†é€Ÿåº¦: {tables_per_second:.1f} è¡¨/ç§’")
        logger.info(f"   åˆ—å¤„ç†é€Ÿåº¦: {columns_per_second:.1f} åˆ—/ç§’")
        
        logger.info(f"ğŸ” æœç´¢æ€§èƒ½:")
        logger.info(f"   æœç´¢ååé‡: {search_throughput:.1f} è¡¨/ç§’")
        logger.info(f"   å¹³å‡æœç´¢å»¶è¿Ÿ: {discovery_time*1000:.1f} æ¯«ç§’")
        
        # 7. æ£€æŸ¥ä¼˜åŒ–ç»„ä»¶æ˜¯å¦ç”Ÿæ•ˆ
        logger.info("\nğŸ”§ ä¼˜åŒ–ç»„ä»¶çŠ¶æ€æ£€æŸ¥:")
        
        optimization_status = {
            "åŒˆç‰™åˆ©ç®—æ³•åŒ¹é…å™¨": settings.hungarian_matcher.enabled,
            "LSHé¢„è¿‡æ»¤å™¨": settings.lsh_prefilter.enabled,
            "å‘é‡åŒ–è®¡ç®—ä¼˜åŒ–å™¨": settings.vectorized_optimizer.enabled,
            "å¤šçº§ç¼“å­˜": settings.cache.multi_level_cache.enabled
        }
        
        for component, enabled in optimization_status.items():
            status_icon = "âœ…" if enabled else "âŒ"
            logger.info(f"   {status_icon} {component}: {'å¯ç”¨' if enabled else 'ç¦ç”¨'}")
        
        enabled_optimizations = sum(optimization_status.values())
        logger.info(f"\nğŸ¯ ä¼˜åŒ–ç»„ä»¶å¯ç”¨ç‡: {enabled_optimizations}/{len(optimization_status)} ({enabled_optimizations/len(optimization_status)*100:.0f}%)")
        
        # åˆ¤æ–­æµ‹è¯•æ˜¯å¦æˆåŠŸ
        success_criteria = [
            index_time < 60,  # ç´¢å¼•æ„å»ºæ—¶é—´ < 60ç§’
            discovery_time < 30,  # å‘ç°æ—¶é—´ < 30ç§’
            len(result.final_results) > 0,  # æœ‰åŒ¹é…ç»“æœ
            enabled_optimizations >= 3  # è‡³å°‘3ä¸ªä¼˜åŒ–ç»„ä»¶å¯ç”¨
        ]
        
        success = all(success_criteria)
        
        if success:
            logger.info("\nğŸ‰ é˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯•æˆåŠŸï¼")
            logger.info("âœ… æ€§èƒ½ä¼˜åŒ–ç»„ä»¶æ­£å¸¸å·¥ä½œ")
            logger.info("âœ… å¤§è§„æ¨¡æ•°æ®å¤„ç†æ€§èƒ½è‰¯å¥½")
            logger.info("âœ… åŒ¹é…è´¨é‡ä¿æŒç¨³å®š")
            return True
        else:
            logger.warning("\nâš ï¸  é˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯•éƒ¨åˆ†é€šè¿‡")
            logger.warning("   æŸäº›æ€§èƒ½æŒ‡æ ‡å¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
            return False
        
    except Exception as e:
        logger.error(f"âŒ é˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ§ª å¼€å§‹é˜¶æ®µäºŒä¼˜åŒ–æ€§èƒ½æµ‹è¯•")
    
    success = await test_phase2_optimization()
    
    if success:
        logger.info("\nğŸ‰ é˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯•å®Œæˆï¼")
        logger.info("âœ… ç³»ç»Ÿå·²å¯ç”¨æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–ç»„ä»¶")
        logger.info("âœ… å¤§è§„æ¨¡æ•°æ®å¤„ç†æ€§èƒ½éªŒè¯é€šè¿‡")
        logger.info("âœ… å‡†å¤‡åœ¨çœŸå®WebTableæ•°æ®é›†ä¸Šæµ‹è¯•")
        sys.exit(0)
    else:
        logger.error("\nâŒ é˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯•æœªå®Œå…¨é€šè¿‡")
        logger.error("   å»ºè®®æ£€æŸ¥ä¼˜åŒ–ç»„ä»¶é…ç½®å’Œæ€§èƒ½å‚æ•°")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())