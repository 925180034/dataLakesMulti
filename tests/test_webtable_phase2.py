#!/usr/bin/env python3
"""
WebTable Phase 2 ä¼˜åŒ–æ•ˆæœæµ‹è¯•
æµ‹è¯•ä¼˜åŒ–åçš„ç³»ç»Ÿåœ¨çœŸå® WebTable æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°
"""

import os
import sys
import json
import time
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Any, Tuple
import traceback

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.workflow import discover_data
from src.core.models import AgentState, TableInfo, ColumnInfo
from src.tools.performance_profiler import PerformanceProfiler
from src.tools.optimized_pipeline import OptimizedPipeline
from src.tools.intelligent_component_router import IntelligentComponentRouter, QueryContext

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class WebTablePhase2Tester:
    """WebTable Phase 2 ä¼˜åŒ–æ•ˆæœæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.examples_dir = self.project_root / "examples"
        self.profiler = PerformanceProfiler()
        self.results = {}
        self.test_scenarios = [
            {
                "name": "Join Tables - Small Scale",
                "tables_file": "webtable_join_tables.json",
                "queries_file": "webtable_join_queries.json",
                "ground_truth_file": "webtable_join_ground_truth.json",
                "sample_size": 50,
                "description": "å°è§„æ¨¡ Join è¡¨æ ¼æµ‹è¯• (50 tables)"
            },
            {
                "name": "Join Tables - Medium Scale", 
                "tables_file": "webtable_join_tables.json",
                "queries_file": "webtable_join_queries.json",
                "ground_truth_file": "webtable_join_ground_truth.json",
                "sample_size": 200,
                "description": "ä¸­ç­‰è§„æ¨¡ Join è¡¨æ ¼æµ‹è¯• (200 tables)"
            },
            {
                "name": "Union Tables - Full Scale",
                "tables_file": "webtable_union_tables.json", 
                "queries_file": "webtable_union_queries.json",
                "ground_truth_file": "webtable_union_ground_truth.json",
                "sample_size": None,  # å…¨é‡æ•°æ®
                "description": "å…¨é‡ Union è¡¨æ ¼æµ‹è¯•"
            }
        ]

    def load_webtable_data(self, filename: str, sample_size: int = None) -> List[Dict]:
        """åŠ è½½ WebTable æ•°æ®"""
        file_path = self.examples_dir / filename
        if not file_path.exists():
            raise FileNotFoundError(f"WebTable æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if sample_size and len(data) > sample_size:
            data = data[:sample_size]
            
        logger.info(f"åŠ è½½ {filename}: {len(data)} æ¡è®°å½•")
        return data

    def load_queries(self, filename: str, sample_size: int = 5) -> List[Dict]:
        """åŠ è½½æµ‹è¯•æŸ¥è¯¢"""
        file_path = self.examples_dir / filename
        if not file_path.exists():
            logger.warning(f"æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {file_path}ï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢")
            return [
                {"query": "find tables with similar schemas for joining", "intent": "join"},
                {"query": "discover tables with related data for union", "intent": "union"},
                {"query": "identify joinable tables with common columns", "intent": "join"},
                {"query": "find semantically similar tables", "intent": "union"},
                {"query": "locate tables that can be joined on key columns", "intent": "join"}
            ]
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if len(data) > sample_size:
            data = data[:sample_size]
            
        return data

    async def test_workflow_performance(self, tables: List[Dict], queries: List[Dict], scenario_name: str) -> Dict:
        """æµ‹è¯•å·¥ä½œæµæ€§èƒ½"""
        logger.info(f"\nğŸš€ å¼€å§‹æµ‹è¯•åœºæ™¯: {scenario_name}")
        logger.info(f"ğŸ“Š æ•°æ®è§„æ¨¡: {len(tables)} è¡¨æ ¼, {len(queries)} æŸ¥è¯¢")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        total_columns = sum(len(table.get('columns', [])) for table in tables)
        
        results = {
            'scenario': scenario_name,
            'tables_count': len(tables),
            'columns_count': total_columns,
            'queries_count': len(queries),
            'execution_times': [],
            'success_count': 0,
            'error_count': 0,
            'avg_results_per_query': 0,
            'total_time': 0,
            'throughput_qps': 0,
            'memory_usage': 0,
            'performance_breakdown': {}
        }
        
        start_time = time.time()
        
        try:
            # å‡†å¤‡è¡¨æ ¼æ•°æ®ï¼ˆè½¬æ¢ä¸º TableInfo æ ¼å¼ï¼‰
            table_info_list = []
            for table in tables:
                # å¤„ç†åˆ—æ•°æ®æ ¼å¼
                columns = []
                if 'columns' in table:
                    for col in table['columns']:
                        if isinstance(col, dict):
                            # æ ‡å‡†åŒ–åˆ—æ•°æ®æ ¼å¼
                            col_info = ColumnInfo(
                                table_name=col.get('table_name', table.get('table_name', 'unknown')),
                                column_name=col.get('column_name', col.get('name', 'unknown')),
                                data_type=col.get('data_type', col.get('type', 'string')),
                                sample_values=col.get('sample_values', [])
                            )
                            columns.append(col_info)
                
                # åˆ›å»ºè¡¨ä¿¡æ¯
                table_info = TableInfo(
                    table_name=table.get('table_name', table.get('name', 'unknown')),
                    columns=columns
                )
                table_info_list.append(table_info)
            
            # æµ‹è¯•æ¯ä¸ªæŸ¥è¯¢
            all_results = []
            for i, query_data in enumerate(queries):
                query_text = query_data.get('query', f'test query {i+1}')
                logger.info(f"ğŸ“ æ‰§è¡ŒæŸ¥è¯¢ {i+1}/{len(queries)}: {query_text[:50]}...")
                
                # æ€§èƒ½ç›‘æ§
                with self.profiler.profile_component(f"query_{i+1}"):
                    try:
                        # æ‰§è¡ŒæŸ¥è¯¢
                        query_start = time.time()
                        result = await discover_data(
                            user_query=query_text,
                            query_tables=[t.model_dump() for t in table_info_list],
                            query_columns=[]
                        )
                        query_time = time.time() - query_start
                        
                        results['execution_times'].append(query_time)
                        results['success_count'] += 1
                        
                        # ç»Ÿè®¡ç»“æœæ•°é‡
                        if isinstance(result, dict):
                            result = AgentState(**result)
                        
                        if hasattr(result, 'final_results') and result.final_results:
                            all_results.extend(result.final_results)
                        
                        logger.info(f"âœ… æŸ¥è¯¢ {i+1} å®Œæˆ: {query_time:.3f}s, ç»“æœæ•°: {len(result.final_results) if hasattr(result, 'final_results') else 0}")
                        
                    except Exception as e:
                        logger.error(f"âŒ æŸ¥è¯¢ {i+1} å¤±è´¥: {str(e)}")
                        results['error_count'] += 1
                        results['execution_times'].append(0)
                        
                        # å¦‚æœæ˜¯æµ‹è¯•ç¯å¢ƒçš„è¿æ¥é—®é¢˜ï¼Œç»§ç»­æµ‹è¯•å…¶ä»–æŸ¥è¯¢
                        if "connection" in str(e).lower() or "network" in str(e).lower():
                            logger.warning("ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œç»§ç»­æµ‹è¯•...")
                            continue
                        else:
                            # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
                            traceback.print_exc()
            
            # è®¡ç®—æ€»ä½“ç»Ÿè®¡
            total_time = time.time() - start_time
            results['total_time'] = total_time
            results['avg_results_per_query'] = len(all_results) / len(queries) if queries else 0
            results['throughput_qps'] = results['success_count'] / total_time if total_time > 0 else 0
            
            # è·å–æ€§èƒ½åˆ†æç»“æœ
            performance_report = self.profiler.get_performance_report()
            results['performance_breakdown'] = performance_report.get('component_stats', {})
            
            # å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆæ¨¡æ‹Ÿï¼‰
            results['memory_usage'] = len(tables) * 0.1 + total_columns * 0.01  # MB
            
            logger.info(f"ğŸ¯ åœºæ™¯å®Œæˆ: {results['success_count']}/{len(queries)} æˆåŠŸ")
            logger.info(f"âš¡ å¹³å‡å“åº”æ—¶é—´: {sum(results['execution_times'])/len(results['execution_times']):.3f}s")
            logger.info(f"ğŸ”¥ ååé‡: {results['throughput_qps']:.2f} QPS")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ æµ‹è¯•åœºæ™¯å¤±è´¥: {str(e)}")
            traceback.print_exc()
            results['error_count'] = len(queries)
            
        return results

    async def run_comprehensive_test(self) -> Dict:
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        logger.info("ğŸ§ª å¼€å§‹ WebTable Phase 2 ä¼˜åŒ–æ•ˆæœç»¼åˆæµ‹è¯•")
        logger.info("=" * 70)
        
        comprehensive_results = {
            'test_summary': {
                'total_scenarios': len(self.test_scenarios),
                'total_success': 0,
                'total_errors': 0,
                'total_tables_tested': 0,
                'total_queries_executed': 0,
                'overall_start_time': time.time()
            },
            'scenario_results': [],
            'performance_comparison': {},
            'optimization_metrics': {}
        }
        
        for scenario in self.test_scenarios:
            try:
                logger.info(f"\nğŸ“‹ å‡†å¤‡åœºæ™¯: {scenario['name']}")
                logger.info(f"ğŸ“„ æè¿°: {scenario['description']}")
                
                # åŠ è½½æ•°æ®
                tables = self.load_webtable_data(
                    scenario['tables_file'], 
                    scenario['sample_size']
                )
                queries = self.load_queries(scenario['queries_file'])
                
                # æ‰§è¡Œæµ‹è¯•
                scenario_result = await self.test_workflow_performance(
                    tables, queries, scenario['name']
                )
                
                comprehensive_results['scenario_results'].append(scenario_result)
                comprehensive_results['test_summary']['total_success'] += scenario_result['success_count']
                comprehensive_results['test_summary']['total_errors'] += scenario_result['error_count']
                comprehensive_results['test_summary']['total_tables_tested'] += scenario_result['tables_count']
                comprehensive_results['test_summary']['total_queries_executed'] += scenario_result['queries_count']
                
            except Exception as e:
                logger.error(f"âŒ åœºæ™¯ {scenario['name']} æ‰§è¡Œå¤±è´¥: {str(e)}")
                comprehensive_results['test_summary']['total_errors'] += 1
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_time = time.time() - comprehensive_results['test_summary']['overall_start_time']
        comprehensive_results['test_summary']['total_time'] = total_time
        comprehensive_results['test_summary']['overall_throughput'] = (
            comprehensive_results['test_summary']['total_success'] / total_time 
            if total_time > 0 else 0
        )
        
        return comprehensive_results

    def generate_performance_report(self, results: Dict) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("ğŸ¯ WebTable Phase 2 ä¼˜åŒ–æ•ˆæœæµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 80)
        report.append("")
        
        # æµ‹è¯•æ€»ç»“
        summary = results['test_summary']
        report.append("ğŸ“Š æµ‹è¯•æ€»ç»“:")
        report.append(f"  æ€»åœºæ™¯æ•°: {summary['total_scenarios']}")
        report.append(f"  æ€»è¡¨æ ¼æ•°: {summary['total_tables_tested']}")
        report.append(f"  æ€»æŸ¥è¯¢æ•°: {summary['total_queries_executed']}")
        report.append(f"  æˆåŠŸæŸ¥è¯¢: {summary['total_success']}")
        report.append(f"  å¤±è´¥æŸ¥è¯¢: {summary['total_errors']}")
        report.append(f"  æˆåŠŸç‡: {summary['total_success']/(summary['total_success']+summary['total_errors'])*100:.1f}%")
        report.append(f"  æ€»æ‰§è¡Œæ—¶é—´: {summary['total_time']:.2f}s")  
        report.append(f"  æ•´ä½“ååé‡: {summary['overall_throughput']:.2f} QPS")
        report.append("")
        
        # åˆ†åœºæ™¯ç»“æœ
        report.append("ğŸ” åˆ†åœºæ™¯æ€§èƒ½åˆ†æ:")
        for scenario_result in results['scenario_results']:
            report.append(f"\nğŸ“‹ {scenario_result['scenario']}:")
            report.append(f"  æ•°æ®è§„æ¨¡: {scenario_result['tables_count']} è¡¨æ ¼, {scenario_result['columns_count']} åˆ—")
            report.append(f"  æŸ¥è¯¢æ•°é‡: {scenario_result['queries_count']}")
            report.append(f"  æˆåŠŸ/å¤±è´¥: {scenario_result['success_count']}/{scenario_result['error_count']}")
            
            if scenario_result['execution_times']:
                avg_time = sum(scenario_result['execution_times']) / len(scenario_result['execution_times'])
                min_time = min(scenario_result['execution_times'])
                max_time = max(scenario_result['execution_times'])
                report.append(f"  å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}s")
                report.append(f"  å“åº”æ—¶é—´èŒƒå›´: {min_time:.3f}s - {max_time:.3f}s")
            
            report.append(f"  ååé‡: {scenario_result['throughput_qps']:.2f} QPS")
            report.append(f"  å†…å­˜ä½¿ç”¨: {scenario_result['memory_usage']:.1f} MB")
            report.append(f"  å¹³å‡ç»“æœæ•°: {scenario_result['avg_results_per_query']:.1f}")
        
        # ä¼˜åŒ–æ•ˆæœè¯„ä¼°
        report.append("")
        report.append("ğŸš€ Phase 2 ä¼˜åŒ–æ•ˆæœè¯„ä¼°:")
        
        # åŸºäºæµ‹è¯•ç»“æœè¯„ä¼°ä¼˜åŒ–æ•ˆæœ
        overall_qps = summary['overall_throughput']
        if overall_qps > 10:
            report.append(f"  âœ… å“è¶Šæ€§èƒ½: {overall_qps:.2f} QPS (è¶…è¿‡é¢„æœŸ)")
        elif overall_qps > 5:
            report.append(f"  âœ… è‰¯å¥½æ€§èƒ½: {overall_qps:.2f} QPS (è¾¾åˆ°é¢„æœŸ)")
        elif overall_qps > 1:
            report.append(f"  âš ï¸  åŸºç¡€æ€§èƒ½: {overall_qps:.2f} QPS (éœ€è¦ä¼˜åŒ–)")
        else:
            report.append(f"  âŒ æ€§èƒ½ä¸è¶³: {overall_qps:.2f} QPS (éœ€è¦é‡æ–°ä¼˜åŒ–)")
        
        success_rate = summary['total_success']/(summary['total_success']+summary['total_errors'])*100
        if success_rate >= 95:
            report.append(f"  âœ… é«˜ç¨³å®šæ€§: {success_rate:.1f}% æˆåŠŸç‡")
        elif success_rate >= 80:
            report.append(f"  âš ï¸  ä¸­ç­‰ç¨³å®šæ€§: {success_rate:.1f}% æˆåŠŸç‡")
        else:
            report.append(f"  âŒ ç¨³å®šæ€§ä¸è¶³: {success_rate:.1f}% æˆåŠŸç‡")
        
        report.append("")
        report.append("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        if overall_qps < 5:
            report.append("  â€¢ è€ƒè™‘å¯ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼")
            report.append("  â€¢ ä¼˜åŒ–å‘é‡è®¡ç®—ç®—æ³•")
        if success_rate < 90:
            report.append("  â€¢ åŠ å¼ºé”™è¯¯å¤„ç†æœºåˆ¶")
            report.append("  â€¢ æå‡ç³»ç»Ÿç¨³å®šæ€§")
        
        report.append("  â€¢ ç›‘æ§ç”Ÿäº§ç¯å¢ƒè¡¨ç°")
        report.append("  â€¢ æ”¶é›†æ›´å¤šçœŸå®åœºæ™¯æ•°æ®")
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    tester = WebTablePhase2Tester()
    
    try:
        # è¿è¡Œç»¼åˆæµ‹è¯•
        results = await tester.run_comprehensive_test()
        
        # ç”Ÿæˆå¹¶è¾“å‡ºæŠ¥å‘Š
        report = tester.generate_performance_report(results)
        print(report)
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        results_file = Path(__file__).parent.parent / "webtable_phase2_test_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
    except Exception as e:
        logger.error(f"ğŸ’¥ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    asyncio.run(main())