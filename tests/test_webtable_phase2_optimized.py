#!/usr/bin/env python3
"""
WebTableé˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯• - åœ¨çœŸå®å¤§è§„æ¨¡WebTableæ•°æ®é›†ä¸ŠéªŒè¯æ€§èƒ½ä¼˜åŒ–æ•ˆæœ
"""
import asyncio
import json
import logging
import time
import sys
from pathlib import Path

# è®¾ç½®é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.tools.data_indexer import build_webtable_indices, verify_indices
from src.core.workflow import discover_data
from src.core.models import TableInfo, ColumnInfo
from src.config.settings import settings

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)


async def test_webtable_phase2_optimization():
    """åœ¨çœŸå®WebTableæ•°æ®é›†ä¸Šæµ‹è¯•é˜¶æ®µäºŒä¼˜åŒ–"""
    try:
        logger.info("ğŸš€ å¼€å§‹WebTableé˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯•")
        logger.info("ğŸ“Š ä½¿ç”¨çœŸå®çš„å¤§è§„æ¨¡WebTableæ•°æ®é›†")
        
        # 1. æ£€æŸ¥WebTableæ•°æ®é›†
        logger.info("\nğŸ“‚ æ­¥éª¤1: æ£€æŸ¥WebTableæ•°æ®é›†")
        
        tables_file = Path("examples/webtable_join_tables.json")
        columns_file = Path("examples/webtable_join_columns.json") 
        
        if not tables_file.exists():
            raise Exception(f"WebTableè¡¨æ•°æ®ä¸å­˜åœ¨: {tables_file}")
        
        if not columns_file.exists():
            logger.warning(f"WebTableåˆ—æ•°æ®ä¸å­˜åœ¨: {columns_file}ï¼Œå°†ä»…ä½¿ç”¨è¡¨æ•°æ®")
            columns_file = None
        
        # è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        with open(tables_file) as f:
            tables_data = json.load(f)
        
        logger.info(f"âœ… WebTableæ•°æ®é›†ç»Ÿè®¡:")
        logger.info(f"   è¡¨æ•°é‡: {len(tables_data)}")
        
        # è®¡ç®—æ€»åˆ—æ•°ï¼ˆä»è¡¨ä¸­ï¼‰
        total_columns_in_tables = sum(len(table.get('columns', [])) for table in tables_data)
        logger.info(f"   è¡¨ä¸­æ€»åˆ—æ•°: {total_columns_in_tables}")
        
        # 2. æ„å»ºå¤§è§„æ¨¡ç´¢å¼•ï¼ˆæµ‹è¯•é˜¶æ®µäºŒä¼˜åŒ–çš„ç´¢å¼•æ„å»ºæ€§èƒ½ï¼‰
        logger.info("\nğŸ”§ æ­¥éª¤2: æ„å»ºWebTableå¤§è§„æ¨¡ç´¢å¼•")
        
        start_time = time.time()
        
        index_result = await build_webtable_indices(
            tables_file=str(tables_file),
            columns_file=str(columns_file) if columns_file else None,
            save_path=None  # ä½¿ç”¨é»˜è®¤è·¯å¾„
        )
        
        index_time = time.time() - start_time
        
        if index_result['status'] != 'success':
            raise Exception(f"WebTableç´¢å¼•æ„å»ºå¤±è´¥: {index_result.get('error')}")
        
        logger.info("âœ… WebTableå¤§è§„æ¨¡ç´¢å¼•æ„å»ºå®Œæˆ")
        logger.info(f"   ç´¢å¼•æ„å»ºæ—¶é—´: {index_time:.2f}ç§’")
        logger.info(f"   å¤„ç†è¡¨æ•°: {index_result['tables_processed']}")
        logger.info(f"   ç´¢å¼•è¡¨æ•°: {index_result['tables_indexed']}")
        logger.info(f"   å¤„ç†åˆ—æ•°: {index_result['columns_processed']}")
        logger.info(f"   ç´¢å¼•åˆ—æ•°: {index_result['columns_indexed']}")
        
        # 3. éªŒè¯ç´¢å¼•
        logger.info("\nğŸ” æ­¥éª¤3: éªŒè¯WebTableç´¢å¼•")
        
        verify_result = await verify_indices()
        if verify_result['status'] != 'success':
            raise Exception(f"ç´¢å¼•éªŒè¯å¤±è´¥: {verify_result.get('error')}")
        
        vector_stats = verify_result.get('vector_search', {})
        value_stats = verify_result.get('value_search', {})
        
        logger.info("âœ… WebTableç´¢å¼•éªŒè¯æˆåŠŸ")
        logger.info(f"   å‘é‡æœç´¢åˆ—æ•°: {vector_stats.get('column_count', 0)}")
        logger.info(f"   å‘é‡æœç´¢è¡¨æ•°: {vector_stats.get('table_count', 0)}")
        logger.info(f"   å€¼æœç´¢ç´¢å¼•åˆ—æ•°: {value_stats.get('indexed_columns', 0)}")
        
        # 4. æ‰§è¡ŒçœŸå®æŸ¥è¯¢æµ‹è¯•
        logger.info("\nğŸ” æ­¥éª¤4: æ‰§è¡ŒWebTableçœŸå®æŸ¥è¯¢æµ‹è¯•")
        
        # åˆ›å»ºæµ‹è¯•æŸ¥è¯¢
        test_queries = [
            {
                "query": "find tables with user demographic and personal information",
                "table": {
                    "table_name": "user_demographics",
                    "columns": [
                        {
                            "table_name": "user_demographics",
                            "column_name": "user_id",
                            "data_type": "int",
                            "sample_values": ["1", "2", "3"]
                        },
                        {
                            "table_name": "user_demographics", 
                            "column_name": "age",
                            "data_type": "int",
                            "sample_values": ["25", "30", "35"]
                        }
                    ]
                }
            },
            {
                "query": "find tables with financial and economic data",
                "table": {
                    "table_name": "financial_data",
                    "columns": [
                        {
                            "table_name": "financial_data",
                            "column_name": "amount",
                            "data_type": "decimal",
                            "sample_values": ["100.50", "200.75"]
                        }
                    ]
                }
            }
        ]
        
        total_discovery_time = 0
        successful_discoveries = 0
        total_matches = 0
        
        for i, test_query in enumerate(test_queries, 1):
            logger.info(f"\n--- æŸ¥è¯¢ {i}/{len(test_queries)} ---")
            logger.info(f"æŸ¥è¯¢æ–‡æœ¬: {test_query['query']}")
            
            start_time = time.time()
            
            try:
                result = await discover_data(
                    user_query=test_query['query'],
                    query_tables=[test_query['table']],
                    query_columns=None
                )
                
                discovery_time = time.time() - start_time
                total_discovery_time += discovery_time
                
                # å¤„ç†è¿”å›ç»“æœç±»å‹
                if isinstance(result, dict):
                    from src.core.models import AgentState
                    result = AgentState.from_dict(result)
                
                logger.info(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œè€—æ—¶: {discovery_time:.2f}ç§’")
                
                if result.final_results:
                    matches_count = len(result.final_results)
                    total_matches += matches_count
                    logger.info(f"   æ‰¾åˆ°åŒ¹é…: {matches_count}ä¸ª")
                    
                    # æ˜¾ç¤ºå‰3ä¸ªæœ€ä½³åŒ¹é…
                    for j, match in enumerate(result.final_results[:3], 1):
                        logger.info(f"     {j}. {match.target_table} (è¯„åˆ†: {match.score:.1f})")
                else:
                    logger.info("   æœªæ‰¾åˆ°åŒ¹é…ç»“æœ")
                
                successful_discoveries += 1
                
            except Exception as e:
                logger.error(f"âŒ æŸ¥è¯¢ {i} å¤±è´¥: {e}")
                continue
        
        # 5. æ€§èƒ½åˆ†æ
        logger.info("\nğŸ“Š æ­¥éª¤5: WebTableé˜¶æ®µäºŒä¼˜åŒ–æ€§èƒ½åˆ†æ")
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        avg_discovery_time = total_discovery_time / successful_discoveries if successful_discoveries > 0 else 0
        tables_per_second_index = len(tables_data) / index_time if index_time > 0 else 0
        columns_per_second_index = index_result['columns_processed'] / index_time if index_time > 0 else 0
        avg_matches_per_query = total_matches / successful_discoveries if successful_discoveries > 0 else 0
        
        logger.info(f"ğŸš€ WebTableç´¢å¼•æ„å»ºæ€§èƒ½:")
        logger.info(f"   æ•°æ®è§„æ¨¡: {len(tables_data)}è¡¨, {index_result['columns_processed']}åˆ—")
        logger.info(f"   æ„å»ºæ—¶é—´: {index_time:.2f}ç§’")
        logger.info(f"   è¡¨å¤„ç†é€Ÿåº¦: {tables_per_second_index:.1f} è¡¨/ç§’")
        logger.info(f"   åˆ—å¤„ç†é€Ÿåº¦: {columns_per_second_index:.1f} åˆ—/ç§’")
        
        logger.info(f"ğŸ” WebTableæœç´¢æ€§èƒ½:")
        logger.info(f"   æˆåŠŸæŸ¥è¯¢: {successful_discoveries}/{len(test_queries)}")
        logger.info(f"   æ€»æœç´¢æ—¶é—´: {total_discovery_time:.2f}ç§’")
        logger.info(f"   å¹³å‡æŸ¥è¯¢æ—¶é—´: {avg_discovery_time:.2f}ç§’")
        logger.info(f"   å¹³å‡åŒ¹é…æ•°: {avg_matches_per_query:.1f}ä¸ª/æŸ¥è¯¢")
        
        # 6. é˜¶æ®µäºŒä¼˜åŒ–ç»„ä»¶çŠ¶æ€
        logger.info("\nğŸ”§ é˜¶æ®µäºŒä¼˜åŒ–ç»„ä»¶çŠ¶æ€:")
        
        optimization_status = {
            "åŒˆç‰™åˆ©ç®—æ³•åŒ¹é…å™¨": settings.hungarian_matcher.enabled,
            "LSHé¢„è¿‡æ»¤å™¨": settings.lsh_prefilter.enabled, 
            "å‘é‡åŒ–è®¡ç®—ä¼˜åŒ–å™¨": settings.vectorized_optimizer.enabled,
            "å¤šçº§ç¼“å­˜": settings.cache.multi_level_cache.get('enabled', True),
            "GPUåŠ é€ŸåµŒå…¥": True  # ä»æ—¥å¿—ä¸­å¯ä»¥çœ‹åˆ°ä½¿ç”¨äº†CUDA
        }
        
        for component, enabled in optimization_status.items():
            status_icon = "âœ…" if enabled else "âŒ"
            logger.info(f"   {status_icon} {component}: {'å¯ç”¨' if enabled else 'ç¦ç”¨'}")
        
        enabled_optimizations = sum(optimization_status.values())
        logger.info(f"\nğŸ¯ ä¼˜åŒ–ç»„ä»¶å¯ç”¨ç‡: {enabled_optimizations}/{len(optimization_status)} ({enabled_optimizations/len(optimization_status)*100:.0f}%)")
        
        # æˆåŠŸæ ‡å‡†
        success_criteria = [
            index_time < 120,  # ç´¢å¼•æ„å»ºæ—¶é—´ < 2åˆ†é’Ÿ
            avg_discovery_time < 10,  # å¹³å‡æŸ¥è¯¢æ—¶é—´ < 10ç§’
            successful_discoveries >= 1,  # è‡³å°‘1ä¸ªæŸ¥è¯¢æˆåŠŸ
            enabled_optimizations >= 4  # è‡³å°‘4ä¸ªä¼˜åŒ–ç»„ä»¶å¯ç”¨
        ]
        
        success = all(success_criteria)
        
        if success:
            logger.info("\nğŸ‰ WebTableé˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯•æˆåŠŸï¼")
            logger.info("âœ… å¤§è§„æ¨¡çœŸå®æ•°æ®å¤„ç†æ€§èƒ½ä¼˜å¼‚")
            logger.info("âœ… é˜¶æ®µäºŒä¼˜åŒ–ç»„ä»¶å…¨é¢å¯ç”¨")
            logger.info("âœ… æœç´¢æ€§èƒ½å’ŒåŒ¹é…è´¨é‡è‰¯å¥½")
            return True
        else:
            logger.warning("\nâš ï¸  WebTableé˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯•éƒ¨åˆ†é€šè¿‡")
            return success_criteria
        
    except Exception as e:
        logger.error(f"âŒ WebTableé˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ§ª å¼€å§‹WebTableå¤§è§„æ¨¡é˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯•")
    
    success = await test_webtable_phase2_optimization()
    
    if success is True:
        logger.info("\nğŸ‰ WebTableé˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯•å®Œå…¨æˆåŠŸï¼")
        logger.info("âœ… ç³»ç»Ÿå·²åœ¨çœŸå®å¤§è§„æ¨¡æ•°æ®ä¸ŠéªŒè¯é˜¶æ®µäºŒä¼˜åŒ–æ•ˆæœ")
        sys.exit(0)
    else:
        logger.error("\nâŒ WebTableé˜¶æ®µäºŒä¼˜åŒ–æµ‹è¯•æœªå®Œå…¨é€šè¿‡")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())