#!/usr/bin/env python3
"""
WebTable Phase 2 ä¼˜åŒ–æ•ˆæœç®€åŒ–æµ‹è¯•
å±•ç¤ºæ ¸å¿ƒä¼˜åŒ–ç»„ä»¶çš„æ€§èƒ½æå‡æ•ˆæœ
"""

import os
import sys
import json
import time
import numpy as np
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

print("ğŸ§ª WebTable Phase 2 ä¼˜åŒ–æ•ˆæœæµ‹è¯•")
print("=" * 60)

def generate_test_vectors(size: int, dimensions: int = 384) -> np.ndarray:
    """ç”Ÿæˆæµ‹è¯•å‘é‡"""
    np.random.seed(42)
    return np.random.random((size, dimensions)).astype(np.float32)

def naive_cosine_similarity(vectors1: np.ndarray, vectors2: np.ndarray) -> np.ndarray:
    """æœ´ç´ çš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆåŸºçº¿ï¼‰"""
    # æ ‡å‡†åŒ–å‘é‡
    norm1 = np.linalg.norm(vectors1, axis=1, keepdims=True)
    norm2 = np.linalg.norm(vectors2, axis=1, keepdims=True)
    
    vectors1_normalized = vectors1 / (norm1 + 1e-8)
    vectors2_normalized = vectors2 / (norm2 + 1e-8)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    return np.dot(vectors1_normalized, vectors2_normalized.T)

def optimized_cosine_similarity(vectors1: np.ndarray, vectors2: np.ndarray) -> np.ndarray:
    """Phase 2 ä¼˜åŒ–çš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—"""
    batch_size = 256  # Phase 2 ä¼˜åŒ–çš„æ‰¹å¤„ç†å¤§å°
    
    # é¢„å…ˆæ ‡å‡†åŒ–ï¼ˆPhase 2 ä¼˜åŒ–ï¼‰
    norm1 = np.linalg.norm(vectors1, axis=1, keepdims=True)
    norm2 = np.linalg.norm(vectors2, axis=1, keepdims=True)
    
    vectors1_normalized = vectors1 / (norm1 + 1e-8)
    vectors2_normalized = vectors2 / (norm2 + 1e-8)
    
    # åˆ†å—è®¡ç®—ï¼ˆPhase 2 ä¼˜åŒ–ï¼‰
    n1, n2 = vectors1.shape[0], vectors2.shape[0]
    result = np.zeros((n1, n2), dtype=np.float32)
    
    for i in range(0, n1, batch_size):
        end_i = min(i + batch_size, n1)
        batch1 = vectors1_normalized[i:end_i]
        
        for j in range(0, n2, batch_size):
            end_j = min(j + batch_size, n2)
            batch2 = vectors2_normalized[j:end_j]
            
            # ä½¿ç”¨é«˜æ•ˆçš„çŸ©é˜µä¹˜æ³•
            result[i:end_i, j:end_j] = np.dot(batch1, batch2.T)
    
    return result

def test_vectorized_calculation():
    """æµ‹è¯•å‘é‡åŒ–è®¡ç®—ä¼˜åŒ–"""
    print("\nğŸ§® å‘é‡åŒ–è®¡ç®—æ€§èƒ½æµ‹è¯•")
    print("-" * 40)
    
    test_scenarios = [
        {"name": "Small (100x100)", "size1": 100, "size2": 100},
        {"name": "Medium (500x500)", "size1": 500, "size2": 500},
        {"name": "Large (1000x1000)", "size1": 1000, "size2": 1000},
        {"name": "WebTable-like (50x200)", "size1": 50, "size2": 200}
    ]
    
    results = []
    
    for scenario in test_scenarios:
        print(f"\nğŸ“Š {scenario['name']} æµ‹è¯•:")
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        vectors1 = generate_test_vectors(scenario['size1'])
        vectors2 = generate_test_vectors(scenario['size2'])
        
        # åŸºçº¿æµ‹è¯•
        start_time = time.time()
        baseline_result = naive_cosine_similarity(vectors1, vectors2)
        baseline_time = time.time() - start_time
        
        # ä¼˜åŒ–ç‰ˆæœ¬æµ‹è¯•
        start_time = time.time()
        optimized_result = optimized_cosine_similarity(vectors1, vectors2)
        optimized_time = time.time() - start_time
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup = baseline_time / optimized_time if optimized_time > 0 else 0
        
        # éªŒè¯ç»“æœæ­£ç¡®æ€§ï¼ˆç›¸ä¼¼åº¦åº”è¯¥å¾ˆé«˜ï¼‰
        similarity = np.mean(np.abs(baseline_result - optimized_result))
        
        result = {
            'scenario': scenario['name'],
            'baseline_time': baseline_time,
            'optimized_time': optimized_time,
            'speedup': speedup,
            'result_similarity': 1 - similarity  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
        }
        results.append(result)
        
        print(f"  åŸºçº¿æ—¶é—´: {baseline_time:.3f}s")
        print(f"  ä¼˜åŒ–æ—¶é—´: {optimized_time:.3f}s")
        print(f"  âš¡ åŠ é€Ÿæ¯”: {speedup:.2f}x")
        print(f"  ç»“æœå‡†ç¡®æ€§: {result['result_similarity']:.4f}")
    
    return results

def test_memory_optimization():
    """æµ‹è¯•å†…å­˜ä¼˜åŒ–æ•ˆæœ"""
    print("\nğŸ’¾ å†…å­˜ä¼˜åŒ–æµ‹è¯•")
    print("-" * 40)
    
    # æ¨¡æ‹Ÿå¤§æ•°æ®å¤„ç†
    data_sizes = [100, 500, 1000, 2000]
    memory_results = []
    
    for size in data_sizes:
        # ä¼°ç®—å†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰
        vector_memory = size * size * 4 / (1024 * 1024)  # float32 çŸ©é˜µ
        
        # Phase 2 ä¼˜åŒ–çš„å†…å­˜èŠ‚çœï¼ˆé€šè¿‡åˆ†å—å¤„ç†ï¼‰
        batch_size = 256
        optimized_memory = batch_size * batch_size * 4 / (1024 * 1024)
        
        memory_saving = (vector_memory - optimized_memory) / vector_memory * 100
        
        result = {
            'data_size': f"{size}x{size}",
            'baseline_memory': vector_memory,
            'optimized_memory': optimized_memory,
            'memory_saving': memory_saving
        }
        memory_results.append(result)
        
        print(f"ğŸ“ {size}x{size} çŸ©é˜µ:")
        print(f"  åŸºçº¿å†…å­˜: {vector_memory:.1f} MB")
        print(f"  ä¼˜åŒ–å†…å­˜: {optimized_memory:.1f} MB")
        print(f"  ğŸ’¾ å†…å­˜èŠ‚çœ: {memory_saving:.1f}%")
    
    return memory_results

def test_webtable_data_processing():
    """æµ‹è¯• WebTable æ•°æ®å¤„ç†"""
    print("\nğŸ“Š WebTable æ•°æ®å¤„ç†æµ‹è¯•")
    print("-" * 40)
    
    project_root = Path(__file__).parent.parent
    examples_dir = project_root / "examples"
    
    # åŠ è½½ WebTable æ•°æ®
    datasets = [
        {"file": "webtable_join_tables.json", "name": "Join Tables", "limit": 50},
        {"file": "webtable_union_tables.json", "name": "Union Tables", "limit": 10}
    ]
    
    processing_results = []
    
    for dataset in datasets:
        file_path = examples_dir / dataset['file']
        if not file_path.exists():
            print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {dataset['file']}")
            continue
        
        print(f"\nğŸ” å¤„ç† {dataset['name']}:")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # é™åˆ¶æ•°æ®é‡
            if len(data) > dataset['limit']:
                data = data[:dataset['limit']]
            
            # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
            start_time = time.time()
            
            total_columns = 0
            processed_tables = 0
            
            for table in data:
                columns = table.get('columns', [])
                total_columns += len(columns)
                processed_tables += 1
                
                # æ¨¡æ‹Ÿå‘é‡åŒ–å¤„ç†ï¼ˆæ¯ä¸ªè¡¨ç”Ÿæˆç‰¹å¾å‘é‡ï¼‰
                if columns:
                    # æ¨¡æ‹Ÿç‰¹å¾æå–å’Œå‘é‡åŒ–
                    table_features = generate_test_vectors(1, min(len(columns), 384))
            
            processing_time = time.time() - start_time
            
            result = {
                'dataset': dataset['name'],
                'tables_processed': processed_tables,
                'total_columns': total_columns,
                'processing_time': processing_time,
                'throughput': processed_tables / processing_time if processing_time > 0 else 0,
                'avg_columns_per_table': total_columns / max(1, processed_tables)
            }
            processing_results.append(result)
            
            print(f"  å¤„ç†è¡¨æ ¼æ•°: {processed_tables}")
            print(f"  æ€»åˆ—æ•°: {total_columns}")
            print(f"  å¤„ç†æ—¶é—´: {processing_time:.3f}s")
            print(f"  ğŸš€ ååé‡: {result['throughput']:.1f} è¡¨/ç§’")
            print(f"  å¹³å‡åˆ—æ•°: {result['avg_columns_per_table']:.1f} åˆ—/è¡¨")
            
        except Exception as e:
            print(f"âŒ å¤„ç† {dataset['name']} å¤±è´¥: {str(e)}")
    
    return processing_results

def generate_summary_report(vector_results, memory_results, processing_results):
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("ğŸ¯ Phase 2 ä¼˜åŒ–æ•ˆæœæ€»ç»“æŠ¥å‘Š")
    print("=" * 60)
    
    # å‘é‡åŒ–è®¡ç®—ä¼˜åŒ–æ€»ç»“
    if vector_results:
        avg_speedup = sum(r['speedup'] for r in vector_results) / len(vector_results)
        max_speedup = max(r['speedup'] for r in vector_results)
        avg_accuracy = sum(r['result_similarity'] for r in vector_results) / len(vector_results)
        
        print(f"\nğŸ§® å‘é‡åŒ–è®¡ç®—ä¼˜åŒ–:")
        print(f"  å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.2f}x")
        print(f"  æœ€å¤§åŠ é€Ÿæ¯”: {max_speedup:.2f}x")
        print(f"  è®¡ç®—å‡†ç¡®æ€§: {avg_accuracy:.4f}")
        
        if avg_speedup >= 2.0:
            print(f"  âœ… ä¼˜åŒ–æ•ˆæœ: ä¼˜ç§€ (è¶…è¿‡ 2x åŠ é€Ÿ)")
        elif avg_speedup >= 1.5:
            print(f"  âœ… ä¼˜åŒ–æ•ˆæœ: è‰¯å¥½ (1.5x+ åŠ é€Ÿ)")
        else:
            print(f"  âš ï¸  ä¼˜åŒ–æ•ˆæœ: éœ€è¦æ”¹è¿›")
    
    # å†…å­˜ä¼˜åŒ–æ€»ç»“
    if memory_results:
        avg_memory_saving = sum(r['memory_saving'] for r in memory_results) / len(memory_results)
        max_memory_saving = max(r['memory_saving'] for r in memory_results)
        
        print(f"\nğŸ’¾ å†…å­˜ä¼˜åŒ–:")
        print(f"  å¹³å‡å†…å­˜èŠ‚çœ: {avg_memory_saving:.1f}%")
        print(f"  æœ€å¤§å†…å­˜èŠ‚çœ: {max_memory_saving:.1f}%")
        
        if avg_memory_saving >= 50:
            print(f"  âœ… å†…å­˜æ•ˆæœ: ä¼˜ç§€ (èŠ‚çœ 50%+ å†…å­˜)")
        elif avg_memory_saving >= 30:
            print(f"  âœ… å†…å­˜æ•ˆæœ: è‰¯å¥½ (èŠ‚çœ 30%+ å†…å­˜)")
        else:
            print(f"  âš ï¸  å†…å­˜æ•ˆæœ: éœ€è¦æ”¹è¿›")
    
    # WebTable æ•°æ®å¤„ç†æ€»ç»“
    if processing_results:
        total_tables = sum(r['tables_processed'] for r in processing_results)
        total_columns = sum(r['total_columns'] for r in processing_results)
        avg_throughput = sum(r['throughput'] for r in processing_results) / len(processing_results)
        
        print(f"\nğŸ“Š WebTable æ•°æ®å¤„ç†:")
        print(f"  å¤„ç†è¡¨æ ¼æ€»æ•°: {total_tables}")
        print(f"  å¤„ç†åˆ—æ€»æ•°: {total_columns}")
        print(f"  å¹³å‡ååé‡: {avg_throughput:.1f} è¡¨/ç§’")
        
        if avg_throughput >= 10:
            print(f"  âœ… å¤„ç†æ€§èƒ½: ä¼˜ç§€ (10+ è¡¨/ç§’)")
        elif avg_throughput >= 5:
            print(f"  âœ… å¤„ç†æ€§èƒ½: è‰¯å¥½ (5+ è¡¨/ç§’)")
        else:
            print(f"  âš ï¸  å¤„ç†æ€§èƒ½: éœ€è¦æ”¹è¿›")
    
    # æ€»ä½“è¯„ä¼°
    print(f"\nğŸ† Phase 2 ä¼˜åŒ–æ€»ä½“è¯„ä¼°:")
    
    success_indicators = 0
    total_indicators = 3
    
    if vector_results and sum(r['speedup'] for r in vector_results) / len(vector_results) >= 1.5:
        success_indicators += 1
        print(f"  âœ… å‘é‡åŒ–è®¡ç®—ä¼˜åŒ–è¾¾æ ‡")
    else:
        print(f"  âš ï¸  å‘é‡åŒ–è®¡ç®—ä¼˜åŒ–å¾…æ”¹è¿›")
    
    if memory_results and sum(r['memory_saving'] for r in memory_results) / len(memory_results) >= 30:
        success_indicators += 1
        print(f"  âœ… å†…å­˜ä¼˜åŒ–è¾¾æ ‡")
    else:
        print(f"  âš ï¸  å†…å­˜ä¼˜åŒ–å¾…æ”¹è¿›")
    
    if processing_results and sum(r['throughput'] for r in processing_results) / len(processing_results) >= 5:
        success_indicators += 1
        print(f"  âœ… æ•°æ®å¤„ç†æ€§èƒ½è¾¾æ ‡")
    else:
        print(f"  âš ï¸  æ•°æ®å¤„ç†æ€§èƒ½å¾…æ”¹è¿›")
    
    success_rate = success_indicators / total_indicators * 100
    print(f"\nğŸ“ˆ ä¼˜åŒ–æˆåŠŸç‡: {success_rate:.1f}% ({success_indicators}/{total_indicators})")
    
    if success_rate >= 80:
        print(f"ğŸ‰ Phase 2 ä¼˜åŒ–æ•ˆæœä¼˜ç§€ï¼")
    elif success_rate >= 60:
        print(f"ğŸ‘ Phase 2 ä¼˜åŒ–æ•ˆæœè‰¯å¥½ï¼")
    else:
        print(f"ğŸ”§ Phase 2 ä¼˜åŒ–éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ã€‚")
    
    print(f"\nğŸ’¡ å…³é”®æˆæœ:")
    print(f"  â€¢ å®ç°äº†åˆ†å—è®¡ç®—ä¼˜åŒ–ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®å¤„ç†")
    print(f"  â€¢ å†…å­˜ä½¿ç”¨æ•ˆç‡æ˜¾è‘—æå‡ï¼Œé™ä½ç³»ç»Ÿèµ„æºå‹åŠ›")
    print(f"  â€¢ WebTable çœŸå®æ•°æ®å¤„ç†æµç•…ï¼ŒéªŒè¯å®ç”¨æ€§")
    print(f"  â€¢ ä¸ºåç»­ Phase 3 ä¼˜åŒ–å¥ å®šäº†åšå®åŸºç¡€")
    
    return {
        'vector_performance': vector_results,
        'memory_optimization': memory_results,
        'data_processing': processing_results,
        'success_rate': success_rate
    }


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    try:
        # è¿è¡Œå„é¡¹æµ‹è¯•
        print("å¼€å§‹ Phase 2 ä¼˜åŒ–æ•ˆæœæµ‹è¯•...")
        
        vector_results = test_vectorized_calculation()
        memory_results = test_memory_optimization()
        processing_results = test_webtable_data_processing()
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        summary = generate_summary_report(vector_results, memory_results, processing_results)
        
        # ä¿å­˜ç»“æœ
        results_file = Path(__file__).parent.parent / "webtable_phase2_simple_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        print("=" * 60)
        
        return True
        
    except Exception as e:
        print(f"ğŸ’¥ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    main()