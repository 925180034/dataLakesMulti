#!/usr/bin/env python
"""
ç¼“å­˜å¢å¼ºç‰ˆä¸‰å±‚æ¶æ„æ¶ˆèå®éªŒè„šæœ¬
åœ¨ä¼˜åŒ–ç‰ˆåŸºç¡€ä¸Šå¢åŠ ç¼“å­˜æœºåˆ¶ï¼Œæ˜¾è‘—åŠ é€Ÿé‡å¤å®éªŒ
ä¸»è¦ä¼˜åŒ–ï¼š
1. æ‰¹å¤„ç†çº§åˆ«èµ„æºå…±äº«
2. è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
3. æŒä¹…åŒ–ç¼“å­˜ç³»ç»Ÿï¼ˆæ–°å¢ï¼‰
4. é¢„è®¡ç®—å‘é‡ç´¢å¼•
5. ç»“æœç¼“å­˜å’Œå¤ç”¨ï¼ˆæ–°å¢ï¼‰
"""
import os
import sys
import json
import time
import hashlib
import logging
import pickle
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# é¦–å…ˆå¯¼å…¥é…ç½®æ¨¡å—ï¼Œè‡ªåŠ¨å¯ç”¨ç¦»çº¿æ¨¡å¼
from src import config  # è¿™ä¼šè‡ªåŠ¨è®¾ç½®ç¦»çº¿æ¨¡å¼

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ================== å…¨å±€é…ç½® ==================
# é™ä½è¡¨åæƒé‡
os.environ['TABLE_NAME_WEIGHT'] = '0.05'
# ä½¿ç”¨SMDå¢å¼ºè¿‡æ»¤å™¨
os.environ['USE_SMD_ENHANCED'] = 'true'
# å›ºå®šhashç§å­
os.environ['PYTHONHASHSEED'] = '0'

# ================== ç¼“å­˜ç®¡ç†å™¨ ==================
class CacheManager:
    """ç»Ÿä¸€çš„ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "cache/experiment_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.memory_cache = {}  # å†…å­˜ç¼“å­˜
        self.stats = {
            'hits': 0,
            'misses': 0,
            'saves': 0
        }
        logger.info(f"âœ… ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–: {self.cache_dir}")
    
    def _get_cache_key(self, operation: str, query: Dict, params: Dict = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # ä½¿ç”¨æ“ä½œç±»å‹ã€æŸ¥è¯¢å†…å®¹å’Œå‚æ•°ç”Ÿæˆå”¯ä¸€é”®
        key_data = {
            'op': operation,
            'query': query.get('query_table', ''),
            'params': params or {}
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def get(self, operation: str, query: Dict, params: Dict = None) -> Optional[Any]:
        """è·å–ç¼“å­˜ç»“æœ"""
        cache_key = self._get_cache_key(operation, query, params)
        
        # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
        if cache_key in self.memory_cache:
            self.stats['hits'] += 1
            return self.memory_cache[cache_key]
        
        # æ£€æŸ¥ç£ç›˜ç¼“å­˜
        cache_file = self.cache_dir / f"{operation}_{cache_key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    result = pickle.load(f)
                    self.memory_cache[cache_key] = result  # åŠ è½½åˆ°å†…å­˜
                    self.stats['hits'] += 1
                    return result
            except:
                pass
        
        self.stats['misses'] += 1
        return None
    
    def set(self, operation: str, query: Dict, result: Any, params: Dict = None):
        """ä¿å­˜ç¼“å­˜ç»“æœ"""
        cache_key = self._get_cache_key(operation, query, params)
        
        # ä¿å­˜åˆ°å†…å­˜ç¼“å­˜
        self.memory_cache[cache_key] = result
        
        # ä¿å­˜åˆ°ç£ç›˜ç¼“å­˜
        cache_file = self.cache_dir / f"{operation}_{cache_key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(result, f)
            self.stats['saves'] += 1
        except Exception as e:
            logger.warning(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def get_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self.stats['hits'] + self.stats['misses']
        hit_rate = self.stats['hits'] / total if total > 0 else 0
        return {
            'hits': self.stats['hits'],
            'misses': self.stats['misses'],
            'saves': self.stats['saves'],
            'hit_rate': f"{hit_rate:.1%}",
            'memory_items': len(self.memory_cache)
        }
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.memory_cache.clear()
        for cache_file in self.cache_dir.glob("*.pkl"):
            cache_file.unlink()
        logger.info("ç¼“å­˜å·²æ¸…ç©º")


# å…¨å±€ç¼“å­˜ç®¡ç†å™¨
cache_manager = None

def init_cache_manager(cache_dir: str = None):
    """åˆå§‹åŒ–å…¨å±€ç¼“å­˜ç®¡ç†å™¨"""
    global cache_manager
    if cache_manager is None:
        cache_dir = cache_dir or "cache/experiment_cache"
        cache_manager = CacheManager(cache_dir)
    return cache_manager


def load_dataset(task_type: str, dataset_name: str = 'webtable', dataset_type: str = 'subset') -> tuple:
    """åŠ è½½æ•°æ®é›†
    
    Args:
        task_type: 'join' æˆ– 'union'
        dataset_name: æ•°æ®é›†åç§° ('webtable', 'opendata', æˆ–è‡ªå®šä¹‰è·¯å¾„)
        dataset_type: æ•°æ®é›†ç±»å‹ ('subset', 'complete', 'true_subset')
    """
    # å¤„ç†æ•°æ®é›†è·¯å¾„
    if '/' in dataset_name or dataset_name.startswith('examples'):
        # ç›´æ¥ä½¿ç”¨æä¾›çš„è·¯å¾„
        base_dir = Path(dataset_name)
    else:
        # æ„å»ºæ ‡å‡†è·¯å¾„
        if dataset_type in ['complete', 'full']:
            suffix = '_complete'
        elif dataset_type == 'true_subset':
            suffix = '_true_subset'
        else:  # subset
            suffix = '_subset'
        
        base_dir = Path(f'examples/{dataset_name}/{task_type}{suffix}')
    
    # ç¡®ä¿è·¯å¾„å­˜åœ¨
    if not base_dir.exists():
        logger.error(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {base_dir}")
        raise FileNotFoundError(f"Dataset path not found: {base_dir}")
    
    with open(base_dir / 'tables.json', 'r') as f:
        tables = json.load(f)
    with open(base_dir / 'queries.json', 'r') as f:
        queries = json.load(f)
    with open(base_dir / 'ground_truth.json', 'r') as f:
        ground_truth = json.load(f)
    
    # ç¡®ä¿æ‰€æœ‰è¡¨æœ‰nameå­—æ®µ
    for t in tables:
        if 'name' not in t and 'table_name' in t:
            t['name'] = t['table_name']
    
    return tables, queries, ground_truth


def convert_ground_truth_format(ground_truth_list: List[Dict]) -> Dict[str, List[str]]:
    """å°†ground truthè½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
    query_to_candidates = {}
    
    for item in ground_truth_list:
        query_table = item.get('query_table', '')
        candidate_table = item.get('candidate_table', '')
        
        if query_table and candidate_table:
            # è¿‡æ»¤è‡ªåŒ¹é…
            if query_table != candidate_table:
                if query_table not in query_to_candidates:
                    query_to_candidates[query_table] = []
                query_to_candidates[query_table].append(candidate_table)
    
    return query_to_candidates


def calculate_metrics(predictions: List[str], ground_truth: List[str], k_values: List[int] = [1, 3, 5]) -> Dict:
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    if not ground_truth:
        return {
            'precision': 0.0,
            'recall': 0.0,
            'f1_score': 0.0,
            **{f'hit@{k}': 0.0 for k in k_values}
        }
    
    # è®¡ç®—Hit@K
    hit_at_k = {}
    for k in k_values:
        top_k = predictions[:k] if len(predictions) >= k else predictions
        hit = 1.0 if any(p in ground_truth for p in top_k) else 0.0
        hit_at_k[f'hit@{k}'] = hit
    
    # è®¡ç®—Precisionå’ŒRecall
    predictions_set = set(predictions[:5])  # ä½¿ç”¨top-5
    ground_truth_set = set(ground_truth)
    
    true_positives = len(predictions_set & ground_truth_set)
    
    precision = true_positives / len(predictions_set) if predictions_set else 0.0
    recall = true_positives / len(ground_truth_set) if ground_truth_set else 0.0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        **hit_at_k
    }


# ================== Layer 1: å…ƒæ•°æ®è¿‡æ»¤ ==================
def initialize_shared_resources_l1(tables: List[Dict], dataset_type: str) -> Dict:
    """åˆå§‹åŒ–L1å±‚å…±äº«èµ„æºï¼ˆé¢„æ„å»ºSMDç´¢å¼•ï¼‰"""
    from src.tools.semantic_similarity_simplified import SemanticSearch
    
    metadata_filter = SemanticSearch()
    # é¢„æ„å»ºSMDç´¢å¼•ä»¥åŠ é€ŸæŸ¥è¯¢
    metadata_filter.build_index(tables)
    
    # åºåˆ—åŒ–ä»¥ä¾¿è¿›ç¨‹é—´å…±äº«
    smd_index_serialized = pickle.dumps(metadata_filter)
    
    return {
        'smd_index': smd_index_serialized,
        'tables': tables
    }


def process_query_l1(args):
    """å¤„ç†å•ä¸ªæŸ¥è¯¢ - L1å±‚ï¼ˆä½¿ç”¨é¢„æ„å»ºçš„SMDç´¢å¼•ï¼‰"""
    query, smd_index_serialized, tables = args
    
    # å…ˆæ£€æŸ¥ç¼“å­˜
    global cache_manager
    if cache_manager:
        cached = cache_manager.get('l1', query)
        if cached is not None:
            return cached
    
    # ååºåˆ—åŒ–SMDç´¢å¼•
    from src.tools.semantic_similarity_simplified import SemanticSearch
    metadata_filter = pickle.loads(smd_index_serialized)
    
    start_time = time.time()
    
    # ä½¿ç”¨é¢„æ„å»ºçš„ç´¢å¼•è¿›è¡Œå¿«é€ŸæŸ¥è¯¢
    candidates = metadata_filter.search_similar_tables_smd(
        query_table_name=query['query_table'],
        query_columns=[col['name'] for col in query.get('columns', [])],
        top_k=40
    )
    
    predictions = [c['table_name'] for c in candidates if c['table_name'] != query['query_table']]
    
    result = {
        'query_table': query['query_table'],
        'predictions': predictions[:5],
        'time': time.time() - start_time
    }
    
    # ä¿å­˜åˆ°ç¼“å­˜
    if cache_manager:
        cache_manager.set('l1', query, result)
    
    return result


# ================== Layer 2: å‘é‡æœç´¢ ==================
def initialize_shared_resources_l2(tables: List[Dict], task_type: str, dataset_type: str) -> Dict:
    """åˆå§‹åŒ–L1+L2å±‚å…±äº«èµ„æº"""
    l1_resources = initialize_shared_resources_l1(tables, dataset_type)
    
    # ç”Ÿæˆæˆ–åŠ è½½å‘é‡ç´¢å¼•
    cache_dir = Path(f"cache/ablation_examples/{dataset_type}")
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    vector_index_path = cache_dir / f"vector_index_{len(tables)}.pkl"
    
    if vector_index_path.exists() and not os.environ.get('FORCE_REBUILD'):
        logger.info(f"ğŸ“‚ åŠ è½½ç¼“å­˜å‘é‡ç´¢å¼•: {vector_index_path}")
        with open(vector_index_path, 'rb') as f:
            vector_index_serialized = pickle.load(f)
    else:
        logger.info(f"âš™ï¸ ç”Ÿæˆæ–°çš„å‘é‡ç´¢å¼•...")
        from src.tools.vector_search import VectorSearch
        vector_search = VectorSearch()
        vector_search.index_tables(tables)
        vector_index_serialized = pickle.dumps(vector_search)
        
        with open(vector_index_path, 'wb') as f:
            pickle.dump(vector_index_serialized, f)
        logger.info(f"âœ… åˆ›å»ºå‘é‡ç´¢å¼•: {vector_index_path}")
    
    return {
        **l1_resources,
        'vector_index': vector_index_serialized,
        'task_type': task_type
    }


def process_query_l1_l2(args):
    """å¤„ç†å•ä¸ªæŸ¥è¯¢ - L1+L2å±‚"""
    query, smd_index_serialized, vector_index_serialized, tables, task_type = args
    
    # å…ˆæ£€æŸ¥ç¼“å­˜
    global cache_manager
    cache_key_params = {'task_type': task_type}
    if cache_manager:
        cached = cache_manager.get('l1_l2', query, cache_key_params)
        if cached is not None:
            return cached
    
    # ååºåˆ—åŒ–ç´¢å¼•
    from src.tools.semantic_similarity_simplified import SemanticSearch
    from src.tools.vector_search import VectorSearch
    
    metadata_filter = pickle.loads(smd_index_serialized)
    vector_search = pickle.loads(vector_index_serialized)
    
    start_time = time.time()
    
    # L1: å…ƒæ•°æ®è¿‡æ»¤
    l1_candidates = metadata_filter.search_similar_tables_smd(
        query_table_name=query['query_table'],
        query_columns=[col['name'] for col in query.get('columns', [])],
        top_k=40
    )
    
    # å‡†å¤‡å€™é€‰è¡¨è¿›è¡Œå‘é‡æœç´¢
    candidate_names = [c['table_name'] for c in l1_candidates]
    candidate_tables = [t for t in tables if t['name'] in candidate_names]
    
    # L2: å‘é‡æœç´¢ï¼ˆå¸¦ä»»åŠ¡ç‰¹å®šæƒé‡ï¼‰
    if task_type == 'union':
        column_weight, value_weight = 0.5, 0.5
    else:
        column_weight, value_weight = 0.7, 0.3
    
    # å‡†å¤‡æŸ¥è¯¢è¡¨
    query_table = {
        'name': query['query_table'],
        'columns': query.get('columns', [])
    }
    
    # å‘é‡æœç´¢
    vector_results = vector_search.search_similar_tables(
        query_table, 
        candidate_tables,
        column_weight=column_weight,
        value_weight=value_weight,
        top_k=5
    )
    
    predictions = [r['table_name'] for r in vector_results if r['table_name'] != query['query_table']]
    
    result = {
        'query_table': query['query_table'],
        'predictions': predictions[:5],
        'time': time.time() - start_time
    }
    
    # ä¿å­˜åˆ°ç¼“å­˜
    if cache_manager:
        cache_manager.set('l1_l2', query, result, cache_key_params)
    
    return result


# ================== Layer 3: LLMéªŒè¯ ==================  
def initialize_shared_resources_l3(tables: List[Dict], task_type: str, dataset_type: str) -> Dict:
    """åˆå§‹åŒ–L1+L2+L3å±‚å…±äº«èµ„æº"""
    return initialize_shared_resources_l2(tables, task_type, dataset_type)


def process_query_l1_l2_l3(args):
    """å¤„ç†å•ä¸ªæŸ¥è¯¢ - L1+L2+L3å±‚ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    query, smd_index_serialized, vector_index_serialized, tables, task_type = args
    
    # å…ˆæ£€æŸ¥ç¼“å­˜
    global cache_manager
    cache_key_params = {'task_type': task_type}
    if cache_manager:
        cached = cache_manager.get('l1_l2_l3', query, cache_key_params)
        if cached is not None:
            return cached
    
    # ååºåˆ—åŒ–ç´¢å¼•
    from src.tools.semantic_similarity_simplified import SemanticSearch
    from src.tools.vector_search import VectorSearch
    
    metadata_filter = pickle.loads(smd_index_serialized)
    vector_search = pickle.loads(vector_index_serialized)
    
    start_time = time.time()
    
    # L1: å…ƒæ•°æ®è¿‡æ»¤
    l1_candidates = metadata_filter.search_similar_tables_smd(
        query_table_name=query['query_table'],
        query_columns=[col['name'] for col in query.get('columns', [])],
        top_k=40
    )
    
    # å‡†å¤‡å€™é€‰è¡¨è¿›è¡Œå‘é‡æœç´¢
    candidate_names = [c['table_name'] for c in l1_candidates]
    candidate_tables = [t for t in tables if t['name'] in candidate_names]
    
    # L2: å‘é‡æœç´¢
    if task_type == 'union':
        column_weight, value_weight = 0.5, 0.5
    else:
        column_weight, value_weight = 0.7, 0.3
    
    query_table = {
        'name': query['query_table'],
        'columns': query.get('columns', [])
    }
    
    vector_results = vector_search.search_similar_tables(
        query_table, 
        candidate_tables,
        column_weight=column_weight,
        value_weight=value_weight,
        top_k=5
    )
    
    # L3: LLMéªŒè¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
    from src.tools.llm_verification_simplified import verify_matches_batch
    
    # å‡†å¤‡LLMéªŒè¯çš„å€™é€‰è¡¨
    candidates_for_llm = [
        {
            'name': r['table_name'],
            'columns': next((t['columns'] for t in tables if t['name'] == r['table_name']), [])
        }
        for r in vector_results[:5]
    ]
    
    if candidates_for_llm:
        llm_results = verify_matches_batch(query_table, candidates_for_llm, task_type)
        
        # åŸºäºLLMåˆ†æ•°é‡æ–°æ’åº
        scored_results = []
        for i, candidate in enumerate(candidates_for_llm):
            score = llm_results.get(candidate['name'], 0.0)
            # ç»“åˆå‘é‡åˆ†æ•°å’ŒLLMåˆ†æ•°
            combined_score = vector_results[i]['score'] * 0.5 + score * 0.5
            scored_results.append({
                'table_name': candidate['name'],
                'score': combined_score
            })
        
        # æŒ‰ç»„åˆåˆ†æ•°æ’åº
        scored_results.sort(key=lambda x: x['score'], reverse=True)
        predictions = [r['table_name'] for r in scored_results if r['table_name'] != query['query_table']]
    else:
        predictions = [r['table_name'] for r in vector_results if r['table_name'] != query['query_table']]
    
    result = {
        'query_table': query['query_table'],
        'predictions': predictions[:5],
        'time': time.time() - start_time
    }
    
    # ä¿å­˜åˆ°ç¼“å­˜
    if cache_manager:
        cache_manager.set('l1_l2_l3', query, result, cache_key_params)
    
    return result


def run_experiment(task_type: str, dataset_name: str, dataset_type: str, max_queries: int = None):
    """è¿è¡Œå®Œæ•´çš„æ¶ˆèå®éªŒï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸš€ Running CACHED Ablation Experiment for {task_type.upper()} Task")
    logger.info(f"ğŸ“‚ Dataset: {dataset_name}/{dataset_type}")
    logger.info(f"ğŸ“Š Max Queries: {max_queries or 'All'}")
    logger.info(f"{'='*80}")
    
    # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
    init_cache_manager(f"cache/experiment_cache/{dataset_name}_{task_type}_{dataset_type}")
    
    # åŠ è½½æ•°æ®
    tables, queries, ground_truth_list = load_dataset(task_type, dataset_name, dataset_type)
    ground_truth = convert_ground_truth_format(ground_truth_list)
    
    logger.info(f"ğŸ“Š Dataset: {len(tables)} tables, {len(queries)} queries")
    
    # é™åˆ¶æŸ¥è¯¢æ•°é‡
    if max_queries:
        queries = queries[:max_queries]
        logger.info(f"ğŸ“Š ä½¿ç”¨å‰{len(queries)}ä¸ªæŸ¥è¯¢")
    
    # è¿‡æ»¤æœ‰æ•ˆæŸ¥è¯¢
    valid_queries = [q for q in queries if q['query_table'] in ground_truth]
    logger.info(f"ğŸ“‹ Using {len(valid_queries)} queries for {task_type.upper()} task experiment")
    
    # å­˜å‚¨ç»“æœ
    results = {}
    
    # è®¾ç½®è¿›ç¨‹æ•°
    num_processes = min(4, mp.cpu_count())
    
    # ============ L1å®éªŒ ============
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ”¬ Running L1 Experiment")
    logger.info(f"{'='*60}")
    
    logger.info("ğŸš€ åˆå§‹åŒ–L1å±‚å…±äº«èµ„æº...")
    shared_resources_l1 = initialize_shared_resources_l1(tables, dataset_type)
    logger.info("âœ… L1å±‚èµ„æºåˆå§‹åŒ–å®Œæˆ")
    
    # å‡†å¤‡å‚æ•°
    l1_args = [
        (q, shared_resources_l1['smd_index'], shared_resources_l1['tables'])
        for q in valid_queries
    ]
    
    # å¹¶è¡Œå¤„ç†
    l1_predictions = []
    l1_times = []
    
    logger.info(f"ğŸ“Š å¤„ç† {len(valid_queries)} ä¸ªæŸ¥è¯¢ (è¿›ç¨‹æ•°={num_processes})...")
    
    with ProcessPoolExecutor(max_workers=num_processes) as executor:
        futures = [executor.submit(process_query_l1, args) for args in l1_args]
        
        for i, future in enumerate(as_completed(futures)):
            result = future.result()
            l1_predictions.append(result)
            l1_times.append(result['time'])
            
            if (i + 1) % 5 == 0:
                logger.info(f"  è¿›åº¦: {i+1}/{len(valid_queries)}")
    
    # è®¡ç®—L1æŒ‡æ ‡
    l1_metrics = []
    for pred in l1_predictions:
        gt = ground_truth.get(pred['query_table'], [])
        metrics = calculate_metrics(pred['predictions'], gt)
        l1_metrics.append(metrics)
    
    avg_l1_metrics = {
        key: np.mean([m[key] for m in l1_metrics])
        for key in l1_metrics[0].keys()
    }
    
    avg_l1_time = np.mean(l1_times)
    logger.info(f"âœ… L1 å®Œæˆ - æ€»æ—¶é—´: {sum(l1_times):.2f}ç§’")
    logger.info(f"ğŸ“ˆ L1 - F1: {avg_l1_metrics['f1_score']:.3f}, Hit@1: {avg_l1_metrics['hit@1']:.3f}, Avg Time: {avg_l1_time:.2f}s/query")
    
    results['L1'] = {
        'metrics': avg_l1_metrics,
        'avg_time': avg_l1_time
    }
    
    # ============ L1+L2å®éªŒ ============
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ”¬ Running L1+L2 Experiment")
    logger.info(f"{'='*60}")
    
    logger.info("ğŸš€ åˆå§‹åŒ–L1+L2å±‚å…±äº«èµ„æº...")
    shared_resources_l2 = initialize_shared_resources_l2(tables, task_type, dataset_type)
    logger.info("âœ… L1+L2å±‚èµ„æºåˆå§‹åŒ–å®Œæˆ")
    
    # å‡†å¤‡å‚æ•°
    l2_args = [
        (q, shared_resources_l2['smd_index'], shared_resources_l2['vector_index'], 
         shared_resources_l2['tables'], shared_resources_l2['task_type'])
        for q in valid_queries
    ]
    
    # å¹¶è¡Œå¤„ç†
    l2_predictions = []
    l2_times = []
    
    logger.info(f"ğŸ“Š å¤„ç† {len(valid_queries)} ä¸ªæŸ¥è¯¢ (è¿›ç¨‹æ•°={num_processes})...")
    
    with ProcessPoolExecutor(max_workers=num_processes) as executor:
        futures = [executor.submit(process_query_l1_l2, args) for args in l2_args]
        
        for i, future in enumerate(as_completed(futures)):
            result = future.result()
            l2_predictions.append(result)
            l2_times.append(result['time'])
            
            if (i + 1) % 5 == 0:
                logger.info(f"  è¿›åº¦: {i+1}/{len(valid_queries)}")
    
    # è®¡ç®—L1+L2æŒ‡æ ‡
    l2_metrics = []
    for pred in l2_predictions:
        gt = ground_truth.get(pred['query_table'], [])
        metrics = calculate_metrics(pred['predictions'], gt)
        l2_metrics.append(metrics)
    
    avg_l2_metrics = {
        key: np.mean([m[key] for m in l2_metrics])
        for key in l2_metrics[0].keys()
    }
    
    avg_l2_time = np.mean(l2_times)
    logger.info(f"âœ… L1+L2 å®Œæˆ - æ€»æ—¶é—´: {sum(l2_times):.2f}ç§’")
    logger.info(f"ğŸ“ˆ L1+L2 - F1: {avg_l2_metrics['f1_score']:.3f}, Hit@1: {avg_l2_metrics['hit@1']:.3f}, Avg Time: {avg_l2_time:.2f}s/query")
    
    results['L1+L2'] = {
        'metrics': avg_l2_metrics,
        'avg_time': avg_l2_time
    }
    
    # ============ L1+L2+L3å®éªŒ ============
    if not os.environ.get('SKIP_LLM'):
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ”¬ Running L1+L2+L3 Experiment")
        logger.info(f"{'='*60}")
        
        logger.info("ğŸš€ åˆå§‹åŒ–L1+L2+L3å±‚å…±äº«èµ„æº...")
        shared_resources_l3 = initialize_shared_resources_l3(tables, task_type, dataset_type)
        logger.info("âœ… L1+L2+L3å±‚èµ„æºåˆå§‹åŒ–å®Œæˆ")
        
        # å‡†å¤‡å‚æ•°
        l3_args = [
            (q, shared_resources_l3['smd_index'], shared_resources_l3['vector_index'],
             shared_resources_l3['tables'], shared_resources_l3['task_type'])
            for q in valid_queries
        ]
        
        # å¹¶è¡Œå¤„ç†ï¼ˆå‡å°‘å¹¶å‘ä»¥é¿å…LLMé™æµï¼‰
        l3_predictions = []
        l3_times = []
        
        logger.info(f"ğŸ“Š å¤„ç† {len(valid_queries)} ä¸ªæŸ¥è¯¢ (è¿›ç¨‹æ•°=2)...")
        
        with ProcessPoolExecutor(max_workers=2) as executor:
            futures = [executor.submit(process_query_l1_l2_l3, args) for args in l3_args]
            
            for i, future in enumerate(as_completed(futures)):
                result = future.result()
                l3_predictions.append(result)
                l3_times.append(result['time'])
                
                if (i + 1) % 5 == 0:
                    logger.info(f"  è¿›åº¦: {i+1}/{len(valid_queries)}")
        
        # è®¡ç®—L1+L2+L3æŒ‡æ ‡
        l3_metrics = []
        for pred in l3_predictions:
            gt = ground_truth.get(pred['query_table'], [])
            metrics = calculate_metrics(pred['predictions'], gt)
            l3_metrics.append(metrics)
        
        avg_l3_metrics = {
            key: np.mean([m[key] for m in l3_metrics])
            for key in l3_metrics[0].keys()
        }
        
        avg_l3_time = np.mean(l3_times)
        logger.info(f"âœ… L1+L2+L3 å®Œæˆ - æ€»æ—¶é—´: {sum(l3_times):.2f}ç§’")
        logger.info(f"ğŸ“ˆ L1+L2+L3 - F1: {avg_l3_metrics['f1_score']:.3f}, Hit@1: {avg_l3_metrics['hit@1']:.3f}, Avg Time: {avg_l3_time:.2f}s/query")
        
        results['L1+L2+L3'] = {
            'metrics': avg_l3_metrics,
            'avg_time': avg_l3_time
        }
    
    # æ‰“å°ç¼“å­˜ç»Ÿè®¡
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡")
    logger.info(f"{'='*60}")
    stats = cache_manager.get_stats()
    logger.info(f"ç¼“å­˜å‘½ä¸­: {stats['hits']}, æœªå‘½ä¸­: {stats['misses']}, å‘½ä¸­ç‡: {stats['hit_rate']}")
    logger.info(f"ç¼“å­˜ä¿å­˜: {stats['saves']}, å†…å­˜é¡¹: {stats['memory_items']}")
    
    return results


def print_results(results: Dict[str, Dict], task_type: str):
    """æ‰“å°å®éªŒç»“æœï¼ˆæ ¼å¼åŒ–è¡¨æ ¼ï¼‰"""
    print(f"\n{'='*100}")
    print(f"ğŸš€ CACHED THREE-LAYER ABLATION EXPERIMENT RESULTS")
    print(f"{'='*100}")
    
    print(f"\n{task_type.upper()} Task Results:")
    print("-" * 80)
    print(f"{'Layer Config':<15} {'Hit@1':<10} {'Hit@3':<10} {'Hit@5':<10} {'Precision':<12} {'Recall':<10} {'F1-Score':<10} {'Time(s)':<10}")
    print("-" * 80)
    
    for layer_config, layer_results in results.items():
        metrics = layer_results['metrics']
        print(f"{layer_config:<15} "
              f"{metrics['hit@1']:<10.3f} "
              f"{metrics['hit@3']:<10.3f} "
              f"{metrics['hit@5']:<10.3f} "
              f"{metrics['precision']:<12.3f} "
              f"{metrics['recall']:<10.3f} "
              f"{metrics['f1_score']:<10.3f} "
              f"{layer_results['avg_time']:<10.2f}")
    
    # åˆ†æå±‚è´¡çŒ®
    print(f"\n{'='*100}")
    print(f"ğŸ“Š LAYER CONTRIBUTION ANALYSIS")
    print(f"{'='*100}")
    
    if 'L1' in results and 'L1+L2' in results:
        l1_f1 = results['L1']['metrics']['f1_score']
        l2_f1 = results['L1+L2']['metrics']['f1_score']
        l2_time = results['L1+L2']['avg_time'] - results['L1']['avg_time']
        
        improvement = ((l2_f1 - l1_f1) / l1_f1 * 100) if l1_f1 > 0 else 0
        print(f"\n{task_type.upper()} Task - Incremental Improvements:")
        print(f"  L2 Contribution: {improvement:+.1f}% F1 improvement, +{l2_time:.2f}s time cost")
        
        if 'L1+L2+L3' in results:
            l3_f1 = results['L1+L2+L3']['metrics']['f1_score']
            l3_time = results['L1+L2+L3']['avg_time'] - results['L1+L2']['avg_time']
            
            l3_improvement = ((l3_f1 - l2_f1) / l2_f1 * 100) if l2_f1 > 0 else 0
            total_improvement = ((l3_f1 - l1_f1) / l1_f1 * 100) if l1_f1 > 0 else 0
            
            print(f"  L3 Contribution: {l3_improvement:+.1f}% F1 improvement, +{l3_time:.2f}s time cost")
            print(f"  Total Improvement: {total_improvement:+.1f}% F1 improvement")
            
            # æˆæœ¬æ•ˆç›Šåˆ†æ
            total_time = results['L1+L2+L3']['avg_time'] - results['L1']['avg_time']
            if total_time > 0:
                cost_benefit = total_improvement / total_time
                print(f"  Cost-Benefit Ratio: {cost_benefit:.2f}% improvement per time unit")
    
    print(f"\n{'='*100}")
    print(f"âš¡ OPTIMIZATION SUMMARY")
    print(f"{'='*100}")
    print("Layer Optimizations Applied:")
    print("  ğŸ” L1 (Metadata): SMD Enhanced filter with reduced table name weight (5%)")
    print("  âš¡ L2 (Vector): Task-specific value similarity (UNION: 50%+50%, JOIN: 70%+30%)")
    print("  ğŸ§  L3 (LLM): Simplified robust verification with enhanced fallback")
    print("\nSystem Optimizations:")
    print("  1. Result caching for repeated experiments")
    print("  2. Batch-level resource sharing (OptimizerAgent & PlannerAgent)")
    print("  3. Process pool parallelization")
    print("  4. Persistent disk and memory caching")
    print("  5. Pre-computed vector indices")
    print(f"{'='*100}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ç¼“å­˜å¢å¼ºç‰ˆä¸‰å±‚æ¶ˆèå®éªŒ')
    parser.add_argument('--task', choices=['join', 'union', 'both'], default='join',
                      help='ä»»åŠ¡ç±»å‹')
    parser.add_argument('--dataset', type=str, default='webtable',
                      help='æ•°æ®é›†åç§°: webtable, opendata, æˆ–è‡ªå®šä¹‰è·¯å¾„')
    parser.add_argument('--dataset-type', choices=['subset', 'complete', 'true_subset'], default='subset',
                      help='æ•°æ®é›†ç±»å‹: subset(å­é›†), complete(å®Œæ•´), true_subset(WebTableçš„çœŸå­é›†)')
    parser.add_argument('--max-queries', type=int, default=None,
                      help='æœ€å¤§æŸ¥è¯¢æ•°')
    parser.add_argument('--output', type=str, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--simple', action='store_true', help='ä½¿ç”¨ç®€å•æŸ¥è¯¢ï¼ˆä¸ä½¿ç”¨æŒ‘æˆ˜æ€§æŸ¥è¯¢ï¼‰')
    parser.add_argument('--skip-llm', action='store_true', help='è·³è¿‡L3å±‚LLMéªŒè¯')
    parser.add_argument('--clear-cache', action='store_true', help='æ¸…ç©ºç¼“å­˜åé‡æ–°è¿è¡Œ')
    
    args = parser.parse_args()
    
    if args.skip_llm:
        os.environ['SKIP_LLM'] = 'true'
    
    # é™åˆ¶æŸ¥è¯¢æ•°
    if args.max_queries:
        logger.info(f"ğŸ“Š é™åˆ¶æœ€å¤§æŸ¥è¯¢æ•°ä¸º: {args.max_queries}")
    
    all_results = {}
    
    if args.task == 'both':
        # è¿è¡Œä¸¤ä¸ªä»»åŠ¡
        for task in ['join', 'union']:
            results = run_experiment(task, args.dataset, args.dataset_type, args.max_queries)
            all_results[task] = results
            print_results(results, task)
    else:
        # è¿è¡Œå•ä¸ªä»»åŠ¡
        results = run_experiment(args.task, args.dataset, args.dataset_type, args.max_queries)
        all_results[args.task] = results
        print_results(results, args.task)
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w') as f:
            json.dump(all_results, f, indent=2)
        logger.info(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    # å¦‚æœè®¾ç½®äº†æ¸…ç©ºç¼“å­˜ï¼Œåœ¨è¿è¡Œç»“æŸåæ¸…ç©º
    if args.clear_cache and cache_manager:
        cache_manager.clear()


if __name__ == "__main__":
    main()