"""
Three-Layer Complete Dynamic Optimizer
å®Œæ•´ä¸‰å±‚åŠ¨æ€ä¼˜åŒ–å™¨ - åŒæ—¶ä¼˜åŒ–L1ã€L2ã€L3å±‚å‚æ•°
"""
import logging
import time
import numpy as np
from typing import Dict, List, Optional
from dataclasses import dataclass, field
from collections import deque

logger = logging.getLogger(__name__)


@dataclass
class ThreeLayerOptimizationState:
    """ä¸‰å±‚ä¼˜åŒ–çŠ¶æ€ï¼ˆè¦†ç›–æ‰€æœ‰å±‚ï¼‰"""
    # æ€§èƒ½å†å²
    precision_history: deque = field(default_factory=lambda: deque(maxlen=10))
    recall_history: deque = field(default_factory=lambda: deque(maxlen=10))
    f1_history: deque = field(default_factory=lambda: deque(maxlen=10))
    time_history: deque = field(default_factory=lambda: deque(maxlen=10))
    
    # L1å±‚å‚æ•°ï¼ˆå…ƒæ•°æ®è¿‡æ»¤ï¼‰
    metadata_min_column_overlap: float = 0.2
    metadata_name_similarity: float = 0.3
    metadata_max_candidates: int = 500
    
    # L2å±‚å‚æ•°ï¼ˆå‘é‡æœç´¢ï¼‰
    vector_top_k: int = 500
    vector_similarity_threshold: float = 0.4
    vector_batch_size: int = 100
    
    # L3å±‚å‚æ•°ï¼ˆLLMéªŒè¯ï¼‰
    llm_confidence_threshold: float = 0.15
    llm_max_candidates: int = 100
    llm_batch_size: int = 20
    llm_rule_high_threshold: float = 0.7
    llm_rule_medium_threshold: float = 0.5
    llm_rule_low_threshold: float = 0.3
    
    # èšåˆå™¨å‚æ•°
    aggregator_max_results: int = 100
    llm_concurrency: int = 3
    batch_size: int = 10
    
    # ä¼˜åŒ–æ§åˆ¶
    adjustment_count: int = 0
    last_adjustment: float = time.time()
    improvement_trend: float = 0.0
    queries_processed: int = 0


class ThreeLayerOptimizer:
    """å®Œæ•´ä¸‰å±‚åŠ¨æ€ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.states = {
            'join': ThreeLayerOptimizationState(),
            'union': ThreeLayerOptimizationState()
        }
        self.adjustment_interval = 10  # æ¯10ä¸ªæŸ¥è¯¢è°ƒæ•´ä¸€æ¬¡
        self.query_count = 0
        
        logger.info("ğŸ¯ ThreeLayerOptimizer initialized for all three layers")
        
    def initialize_batch(self, task_type: str, data_size: int):
        """åˆå§‹åŒ–æ‰¹æ¬¡å‚æ•°ï¼ˆé’ˆå¯¹ä»»åŠ¡ç±»å‹å’Œæ‰€æœ‰ä¸‰å±‚ï¼‰"""
        state = self.states[task_type]
        
        if task_type == 'union':
            # UNIONä»»åŠ¡ï¼šä¼˜åŒ–å¬å›ç‡
            logger.info("ğŸ“Š UNIONä»»åŠ¡åˆå§‹åŒ– - ä¼˜åŒ–å¬å›ç‡")
            
            # L1å±‚ï¼šæ”¾å®½å…ƒæ•°æ®è¿‡æ»¤
            state.metadata_min_column_overlap = 0.1
            state.metadata_max_candidates = 800
            
            # L2å±‚ï¼šå¢å¤§å‘é‡æœç´¢èŒƒå›´
            state.vector_top_k = 800
            state.vector_similarity_threshold = 0.3
            
            # L3å±‚ï¼šæä½çš„LLMé˜ˆå€¼ä»¥æé«˜å¬å›ç‡
            state.llm_confidence_threshold = 0.01  # æä½ï¼
            state.llm_max_candidates = 150
            state.llm_rule_high_threshold = 0.5
            state.llm_rule_medium_threshold = 0.3
            state.llm_rule_low_threshold = 0.1
            state.aggregator_max_results = 150
            
        else:  # join
            # JOINä»»åŠ¡ï¼šå¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡
            logger.info("ğŸ“Š JOINä»»åŠ¡åˆå§‹åŒ– - å¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡")
            
            # L1å±‚ï¼šæ ‡å‡†å‚æ•°
            state.metadata_min_column_overlap = 0.2
            state.metadata_max_candidates = 500
            
            # L2å±‚ï¼šæ ‡å‡†å‚æ•°
            state.vector_top_k = 500
            state.vector_similarity_threshold = 0.4
            
            # L3å±‚ï¼šå¹³è¡¡çš„é˜ˆå€¼
            state.llm_confidence_threshold = 0.10
            state.llm_max_candidates = 100
            state.llm_rule_high_threshold = 0.7
            state.llm_rule_medium_threshold = 0.5
            state.llm_rule_low_threshold = 0.3
            state.aggregator_max_results = 100
            
        # é‡ç½®ç»Ÿè®¡
        state.queries_processed = 0
        state.adjustment_count = 0
        self.query_count = 0
    
    def get_current_params(self, task_type: str) -> Dict:
        """è·å–å½“å‰æ‰€æœ‰ä¸‰å±‚çš„åŠ¨æ€å‚æ•°"""
        state = self.states[task_type]
        
        return {
            # L1å±‚å‚æ•°
            'metadata_min_column_overlap': state.metadata_min_column_overlap,
            'metadata_name_similarity': state.metadata_name_similarity,
            'metadata_max_candidates': state.metadata_max_candidates,
            
            # L2å±‚å‚æ•°
            'vector_top_k': state.vector_top_k,
            'vector_similarity_threshold': state.vector_similarity_threshold,
            'vector_batch_size': state.vector_batch_size,
            
            # L3å±‚å‚æ•°
            'llm_confidence_threshold': state.llm_confidence_threshold,
            'llm_max_candidates': state.llm_max_candidates,
            'llm_batch_size': state.llm_batch_size,
            'rule_high_threshold': state.llm_rule_high_threshold,
            'rule_medium_threshold': state.llm_rule_medium_threshold,
            'rule_low_threshold': state.llm_rule_low_threshold,
            
            # èšåˆå™¨å‚æ•°
            'aggregator_max_results': state.aggregator_max_results,
            'llm_concurrency': state.llm_concurrency,
            'batch_size': state.batch_size,
            
            # ä»»åŠ¡ç±»å‹
            'task_type': task_type,
            'optimize_for_recall': task_type == 'union'
        }
    
    def update_performance(self, task_type: str, precision: float, recall: float, 
                          f1: float, query_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡å¹¶å†³å®šæ˜¯å¦è°ƒæ•´å‚æ•°"""
        state = self.states[task_type]
        self.query_count += 1
        state.queries_processed += 1
        
        # è®°å½•æ€§èƒ½
        state.precision_history.append(precision)
        state.recall_history.append(recall)
        state.f1_history.append(f1)
        state.time_history.append(query_time)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´
        if self.query_count % self.adjustment_interval == 0 and len(state.f1_history) >= 5:
            self._adjust_parameters(task_type)
    
    def _adjust_parameters(self, task_type: str):
        """è°ƒæ•´æ‰€æœ‰ä¸‰å±‚çš„å‚æ•°"""
        state = self.states[task_type]
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_precision = np.mean(state.precision_history)
        avg_recall = np.mean(state.recall_history)
        avg_f1 = np.mean(state.f1_history)
        
        logger.info(f"\nğŸ”„ åŠ¨æ€è°ƒæ•´å‚æ•° (Query {self.query_count})")
        logger.info(f"  å¹³å‡æ€§èƒ½: P={avg_precision:.3f}, R={avg_recall:.3f}, F1={avg_f1:.3f}")
        
        # è®¡ç®—æ€§èƒ½å·®è·
        precision_gap = 0.5 - avg_precision  # ç›®æ ‡ç²¾ç¡®ç‡50%
        recall_gap = 0.4 - avg_recall        # ç›®æ ‡å¬å›ç‡40%
        
        if task_type == 'union':
            # UNION: ä¸»è¦å…³æ³¨å¬å›ç‡
            if recall_gap > 0.15:  # å¬å›ç‡å¤ªä½
                logger.info("  ğŸ“ˆ UNIONå¬å›ç‡å¤ªä½ï¼Œæ”¾å®½æ‰€æœ‰å±‚çš„é˜ˆå€¼")
                
                # L1å±‚ï¼šæ”¾å®½å…ƒæ•°æ®è¿‡æ»¤
                state.metadata_min_column_overlap = max(0.05, state.metadata_min_column_overlap - 0.05)
                state.metadata_max_candidates = min(1000, state.metadata_max_candidates + 100)
                
                # L2å±‚ï¼šå¢åŠ å‘é‡æœç´¢ç»“æœ
                state.vector_top_k = min(1000, state.vector_top_k + 150)
                state.vector_similarity_threshold = max(0.2, state.vector_similarity_threshold - 0.05)
                
                # L3å±‚ï¼šå¤§å¹…é™ä½LLMé˜ˆå€¼
                state.llm_confidence_threshold = max(0.001, state.llm_confidence_threshold - 0.02)
                state.llm_max_candidates = min(200, state.llm_max_candidates + 20)
                state.llm_rule_high_threshold = max(0.3, state.llm_rule_high_threshold - 0.1)
                state.llm_rule_medium_threshold = max(0.2, state.llm_rule_medium_threshold - 0.1)
                state.llm_rule_low_threshold = max(0.05, state.llm_rule_low_threshold - 0.05)
                
            elif avg_precision < 0.2:  # ç²¾ç¡®ç‡å¤ªä½
                logger.info("  âš ï¸ UNIONç²¾ç¡®ç‡å¤ªä½ï¼Œç¨å¾®æ”¶ç´§é˜ˆå€¼")
                
                # å¾®è°ƒå‚æ•°
                state.llm_confidence_threshold = min(0.1, state.llm_confidence_threshold + 0.01)
                state.vector_similarity_threshold = min(0.5, state.vector_similarity_threshold + 0.02)
                
        else:  # join
            # JOIN: å¹³è¡¡ç²¾ç¡®ç‡å’Œå¬å›ç‡
            if avg_f1 < 0.3:  # F1å¤ªä½
                if recall_gap > precision_gap:
                    logger.info("  ğŸ“ˆ JOINå¬å›ç‡ä¸è¶³ï¼Œæ”¾å®½é˜ˆå€¼")
                    
                    # L1å±‚ï¼šæ”¾å®½
                    state.metadata_min_column_overlap = max(0.1, state.metadata_min_column_overlap - 0.02)
                    state.metadata_max_candidates = min(700, state.metadata_max_candidates + 50)
                    
                    # L2å±‚ï¼šå¢åŠ å€™é€‰
                    state.vector_top_k = min(700, state.vector_top_k + 100)
                    
                    # L3å±‚ï¼šé™ä½é˜ˆå€¼
                    state.llm_confidence_threshold = max(0.05, state.llm_confidence_threshold - 0.02)
                    state.llm_max_candidates = min(150, state.llm_max_candidates + 10)
                    
                else:
                    logger.info("  ğŸ“‰ JOINç²¾ç¡®ç‡ä¸è¶³ï¼Œæ”¶ç´§é˜ˆå€¼")
                    
                    # L1å±‚ï¼šæ”¶ç´§
                    state.metadata_min_column_overlap = min(0.4, state.metadata_min_column_overlap + 0.02)
                    
                    # L2å±‚ï¼šå‡å°‘å€™é€‰
                    state.vector_top_k = max(300, state.vector_top_k - 50)
                    
                    # L3å±‚ï¼šæé«˜é˜ˆå€¼
                    state.llm_confidence_threshold = min(0.3, state.llm_confidence_threshold + 0.02)
        
        state.adjustment_count += 1
        state.last_adjustment = time.time()
        
        # è®°å½•è°ƒæ•´åçš„å‚æ•°
        logger.info(f"  ğŸ“Š è°ƒæ•´åå‚æ•°:")
        logger.info(f"    L1: overlap={state.metadata_min_column_overlap:.2f}, candidates={state.metadata_max_candidates}")
        logger.info(f"    L2: top_k={state.vector_top_k}, threshold={state.vector_similarity_threshold:.2f}")
        logger.info(f"    L3: confidence={state.llm_confidence_threshold:.3f}, candidates={state.llm_max_candidates}")
    
    def get_optimization_summary(self, task_type: str) -> str:
        """è·å–ä¼˜åŒ–æ€»ç»“"""
        state = self.states[task_type]
        
        if len(state.f1_history) == 0:
            return "No optimization data available"
        
        summary = f"\nğŸ“Š åŠ¨æ€ä¼˜åŒ–æ€»ç»“ ({task_type.upper()}):\n"
        summary += f"  è°ƒæ•´æ¬¡æ•°: {state.adjustment_count}\n"
        summary += f"  å¹³å‡æ€§èƒ½: P={np.mean(state.precision_history):.3f}, "
        summary += f"R={np.mean(state.recall_history):.3f}, "
        summary += f"F1={np.mean(state.f1_history):.3f}\n"
        summary += f"  æœ€ç»ˆå‚æ•°:\n"
        summary += f"    L1: overlap={state.metadata_min_column_overlap:.2f}, candidates={state.metadata_max_candidates}\n"
        summary += f"    L2: top_k={state.vector_top_k}, threshold={state.vector_similarity_threshold:.2f}\n"
        summary += f"    L3: confidence={state.llm_confidence_threshold:.3f}, candidates={state.llm_max_candidates}\n"
        
        return summary