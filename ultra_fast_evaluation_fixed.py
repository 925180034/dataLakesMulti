#!/usr/bin/env python
"""
ä¿®å¤ç‰ˆï¼šè¶…å¿«é€Ÿæ•°æ®æ¹–è¯„ä¼°ç³»ç»Ÿ
è§£å†³502æŸ¥è¯¢çš„é—®é¢˜
"""

import json
import time
import asyncio
import numpy as np
from tabulate import tabulate
from typing import List, Dict, Any, Tuple
from collections import defaultdict
import os

# å¼ºåˆ¶è®¾ç½®çŸ­è¶…æ—¶å’Œå°‘é‡è¯•
os.environ['SKIP_LLM'] = 'false'
os.environ['LLM_TIMEOUT'] = '5'  # 5ç§’è¶…æ—¶
os.environ['LLM_MAX_RETRIES'] = '1'  # æœ€å¤š1æ¬¡é‡è¯•


async def evaluate_queries_fixed(
    num_queries: int,
    dataset_type: str = "subset"
):
    """ä¿®å¤çš„è¯„ä¼°å‡½æ•°"""
    
    print("="*80)
    print(f"ğŸš€ è¶…å¿«é€Ÿæ•°æ®æ¹–è¯„ä¼°ç³»ç»Ÿï¼ˆä¿®å¤ç‰ˆï¼‰")
    print(f"ğŸ“Š æµ‹è¯•é…ç½®: {num_queries}ä¸ªæŸ¥è¯¢, {dataset_type}æ•°æ®é›†")
    print("="*80)
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    
    # æ ¹æ®æŸ¥è¯¢æ•°é‡é€‰æ‹©åˆé€‚çš„æ•°æ®é›†
    if dataset_type == "subset" and num_queries > 100:
        print(f"âš ï¸ Subsetåªæœ‰100ä¸ªè¡¨ï¼Œä½†è¯·æ±‚{num_queries}ä¸ªæŸ¥è¯¢")
        print(f"ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°Completeæ•°æ®é›†")
        dataset_type = "complete"
    
    # åŠ è½½å¯¹åº”æ•°æ®é›†
    if dataset_type == "subset":
        tables_file = "examples/final_subset_tables.json"
        ground_truth_file = "examples/final_subset_ground_truth_auto.json"
    else:
        tables_file = "examples/final_complete_tables.json"
        ground_truth_file = "examples/final_complete_ground_truth_auto.json"
    
    with open(tables_file) as f:
        all_tables = json.load(f)
    
    with open(ground_truth_file) as f:
        ground_truth = json.load(f)
    
    print(f"âœ… å·²åŠ è½½ {len(all_tables)} ä¸ªè¡¨")
    print(f"âœ… å·²åŠ è½½çœŸå®æ ‡ç­¾ ({len(ground_truth)}æ¡è®°å½•)")
    
    # 2. åˆå§‹åŒ–ä¼˜åŒ–å·¥ä½œæµï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
    print("\nâš™ï¸ åˆå§‹åŒ–è¶…ä¼˜åŒ–å·¥ä½œæµ...")
    from src.core.models import TableInfo, ColumnInfo, AgentState
    
    # è½¬æ¢è¡¨æ•°æ®
    table_infos = []
    for table_data in all_tables:
        table_info = TableInfo(
            table_name=table_data['table_name'],
            columns=[
                ColumnInfo(
                    table_name=table_data['table_name'],
                    column_name=col.get('column_name', col.get('name', '')),
                    data_type=col.get('data_type', col.get('type', 'unknown'))
                )
                for col in table_data.get('columns', [])[:10]  # é™åˆ¶åˆ—æ•°
            ]
        )
        table_infos.append(table_info)
    
    # åˆ›å»ºä¼˜åŒ–å·¥ä½œæµï¼ˆå¸¦å¿«é€Ÿå¤±è´¥é…ç½®ï¼‰
    from src.core.ultra_optimized_workflow import UltraOptimizedWorkflow
    workflow = UltraOptimizedWorkflow()
    
    # é…ç½®æ›´æ¿€è¿›çš„ä¼˜åŒ–å‚æ•°
    workflow.max_metadata_candidates = 20  # è¿›ä¸€æ­¥å‡å°‘
    workflow.max_vector_candidates = 5     # è¿›ä¸€æ­¥å‡å°‘
    workflow.max_llm_candidates = 2        # åªéªŒè¯å‰2ä¸ª
    workflow.early_stop_threshold = 0.75   # æ›´æ—©åœæ­¢
    
    init_start = time.time()
    await workflow.initialize(table_infos)
    init_time = time.time() - init_start
    print(f"âœ… ç´¢å¼•åˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {init_time:.2f}ç§’)")
    
    # 3. å‡†å¤‡æŸ¥è¯¢ï¼ˆæ­£ç¡®å¤„ç†æ•°é‡ï¼‰
    actual_queries = min(num_queries, len(all_tables))
    if num_queries > len(all_tables):
        print(f"\nâš ï¸ è¯·æ±‚{num_queries}ä¸ªæŸ¥è¯¢ï¼Œä½†åªæœ‰{len(all_tables)}ä¸ªè¡¨")
        print(f"ğŸ“Š å°†å¾ªç¯æŸ¥è¯¢ä»¥è¾¾åˆ°{num_queries}æ¬¡")
        
        # å¾ªç¯ä½¿ç”¨è¡¨ä»¥è¾¾åˆ°è¯·æ±‚çš„æŸ¥è¯¢æ•°
        query_tables = []
        for i in range(num_queries):
            query_tables.append(table_infos[i % len(table_infos)])
    else:
        query_tables = table_infos[:num_queries]
    
    print(f"\nğŸ” å¼€å§‹è¯„ä¼° {len(query_tables)} ä¸ªæŸ¥è¯¢...")
    print("-"*80)
    
    # 4. æ‰§è¡Œæ‰¹é‡æŸ¥è¯¢ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
    all_metrics = []
    query_times = []
    successful_queries = 0
    total_matches = 0
    failed_queries = []
    
    # è¿›åº¦æ˜¾ç¤º
    progress_interval = max(1, len(query_tables) // 10)
    
    for i, query_table in enumerate(query_tables, 1):
        # å‡†å¤‡æŸ¥è¯¢çŠ¶æ€
        state = AgentState()
        state.query_tables = [query_table]
        state.query_columns = query_table.columns
        
        # è·å–è¯¥æŸ¥è¯¢çš„çœŸå®æ ‡ç­¾
        query_gt = {query_table.table_name: ground_truth.get(query_table.table_name, [])}
        
        # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
        start_time = time.time()
        try:
            # è®¾ç½®å•ä¸ªæŸ¥è¯¢çš„æœ€å¤§è¶…æ—¶æ—¶é—´ï¼ˆ10ç§’ï¼‰
            result_state, metrics = await asyncio.wait_for(
                workflow.run_optimized(
                    state,
                    [t.table_name for t in table_infos],
                    query_gt if ground_truth else None
                ),
                timeout=10.0
            )
            
            query_time = time.time() - start_time
            query_times.append(query_time)
            
            # ç»Ÿè®¡ç»“æœ
            if result_state:
                successful_queries += 1
                
                # ç»Ÿè®¡åŒ¹é…æ•°é‡
                if hasattr(result_state, 'final_results') and result_state.final_results:
                    total_matches += len(result_state.final_results)
                elif hasattr(result_state, 'matches') and result_state.matches:
                    total_matches += len(result_state.matches)
                elif hasattr(result_state, 'results') and result_state.results:
                    total_matches += len(result_state.results)
            
            # æ”¶é›†è¯„ä»·æŒ‡æ ‡
            if metrics:
                all_metrics.append(metrics)
                # æ‰“å°ç¬¬ä¸€ä¸ªæŸ¥è¯¢çš„metricsç»“æ„ï¼Œç”¨äºè°ƒè¯•
                if i == 1:
                    print(f"  ğŸ“Š Metricsç¤ºä¾‹: precision={getattr(metrics, 'precision', 'N/A')}, recall={getattr(metrics, 'recall', 'N/A')}")
                
        except asyncio.TimeoutError:
            query_time = 10.0  # è¶…æ—¶è®°ä¸º10ç§’
            query_times.append(query_time)
            failed_queries.append(i)
            print(f"  âš ï¸ æŸ¥è¯¢ {i} è¶…æ—¶ï¼ˆ10ç§’ï¼‰")
            
        except Exception as e:
            query_time = 0.0
            query_times.append(query_time)
            failed_queries.append(i)
            print(f"  âŒ æŸ¥è¯¢ {i} å¤±è´¥: {str(e)[:50]}")
        
        # æ˜¾ç¤ºè¿›åº¦
        if i % progress_interval == 0 or i == len(query_tables):
            valid_times = [t for t in query_times if t > 0]
            if valid_times:
                avg_time = np.mean(valid_times)
                success_rate = successful_queries / i * 100
                print(f"è¿›åº¦: {i}/{len(query_tables)} | "
                      f"å¹³å‡æ—¶é—´: {avg_time:.3f}s | "
                      f"æˆåŠŸç‡: {success_rate:.1f}%")
    
    # 5. è®¡ç®—ç»Ÿè®¡
    print("\n" + "="*80)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»")
    print("="*80)
    
    # æ€§èƒ½ç»Ÿè®¡
    valid_times = [t for t in query_times if t > 0]
    if valid_times:
        performance_data = [
            ["æŸ¥è¯¢æ€»æ•°", len(query_tables)],
            ["æˆåŠŸæŸ¥è¯¢", successful_queries],
            ["å¤±è´¥æŸ¥è¯¢", len(failed_queries)],
            ["æˆåŠŸç‡", f"{successful_queries/len(query_tables)*100:.1f}%"],
            ["æ€»åŒ¹é…æ•°", total_matches],
            ["å¹³å‡æ¯æŸ¥è¯¢åŒ¹é…æ•°", f"{total_matches/successful_queries:.1f}" if successful_queries > 0 else "0"],
            ["", ""],
            ["å¹³å‡æŸ¥è¯¢æ—¶é—´", f"{np.mean(valid_times):.3f}ç§’"],
            ["ä¸­ä½æ•°æ—¶é—´", f"{np.median(valid_times):.3f}ç§’"],
            ["æ ‡å‡†å·®", f"{np.std(valid_times):.3f}ç§’"],
            ["æœ€å°æ—¶é—´", f"{np.min(valid_times):.3f}ç§’"],
            ["æœ€å¤§æ—¶é—´", f"{np.max(valid_times):.3f}ç§’"],
        ]
        
        print("\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
        print(tabulate(performance_data, headers=["æŒ‡æ ‡", "å€¼"], tablefmt="grid"))
    
    # è´¨é‡æŒ‡æ ‡
    if all_metrics:
        # å¤„ç†ä¸åŒç±»å‹çš„metricså¯¹è±¡
        precisions = []
        recalls = []
        f1_scores = []
        mrrs = []
        hit_rates = []
        
        for m in all_metrics:
            if hasattr(m, 'precision'):
                precisions.append(getattr(m, 'precision', 0))
            elif isinstance(m, dict) and 'precision' in m:
                precisions.append(m['precision'])
                
            if hasattr(m, 'recall'):
                recalls.append(getattr(m, 'recall', 0))
            elif isinstance(m, dict) and 'recall' in m:
                recalls.append(m['recall'])
                
            if hasattr(m, 'f1_score'):
                f1_scores.append(getattr(m, 'f1_score', 0))
            elif isinstance(m, dict) and 'f1_score' in m:
                f1_scores.append(m['f1_score'])
                
            if hasattr(m, 'mrr'):
                mrrs.append(getattr(m, 'mrr', 0))
            elif isinstance(m, dict) and 'mrr' in m:
                mrrs.append(m['mrr'])
                
            if hasattr(m, 'hit_rate'):
                hit_rates.append(getattr(m, 'hit_rate', 0))
            elif isinstance(m, dict) and 'hit_rate' in m:
                hit_rates.append(m['hit_rate'])
        
        # è®¡ç®—å¹³å‡å€¼ï¼ˆåªæœ‰æœ‰å€¼æ—¶æ‰è®¡ç®—ï¼‰
        avg_precision = np.mean(precisions) if precisions else 0.0
        avg_recall = np.mean(recalls) if recalls else 0.0
        avg_f1 = np.mean(f1_scores) if f1_scores else 0.0
        avg_mrr = np.mean(mrrs) if mrrs else 0.0
        avg_hit_rate = np.mean(hit_rates) if hit_rates else 0.0
        
        quality_data = [
            ["Precision@10", f"{avg_precision:.3f}"],
            ["Recall@10", f"{avg_recall:.3f}"],
            ["F1-Score", f"{avg_f1:.3f}"],
            ["MRR", f"{avg_mrr:.3f}"],
            ["Hit Rate", f"{avg_hit_rate:.3f}"],
            ["æœ‰æ•ˆè¯„ä»·æ•°", f"{len(precisions)}/{len(all_metrics)}"],
        ]
        
        print("\nğŸ¯ è´¨é‡æŒ‡æ ‡:")
        print(tabulate(quality_data, headers=["æŒ‡æ ‡", "å€¼"], tablefmt="grid"))
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„è¯„ä»·æŒ‡æ ‡ï¼Œè¯´æ˜åŸå› 
        if not precisions:
            print("\nâš ï¸ æ³¨æ„ï¼šæœªèƒ½è®¡ç®—å‡†ç¡®çš„è´¨é‡æŒ‡æ ‡")
            print("  å¯èƒ½åŸå› ï¼š")
            print("  1. Ground truthæ•°æ®ä¸å®Œæ•´")
            print("  2. æŸ¥è¯¢ç»“æœä¸ºç©º")
            print("  3. è¯„ä»·æŒ‡æ ‡è®¡ç®—æ¨¡å—æœªæ­£ç¡®è¿”å›æ•°æ®")
    
    # æ€§èƒ½è¯„ä¼°
    if valid_times:
        avg_time = np.mean(valid_times)
        print("\nğŸ† æ€§èƒ½è¯„ä¼°:")
        if avg_time <= 1:
            print(f"  âœ… å“è¶Šï¼å¹³å‡ {avg_time:.3f}ç§’ â‰¤ 1ç§’ (æ¯«ç§’çº§)")
        elif avg_time <= 3:
            print(f"  âœ… ä¼˜ç§€ï¼å¹³å‡ {avg_time:.3f}ç§’ â‰¤ 3ç§’")
        elif avg_time <= 8:
            print(f"  âš ï¸ è¾¾æ ‡ã€‚å¹³å‡ {avg_time:.3f}ç§’ â‰¤ 8ç§’")
        else:
            print(f"  âŒ éœ€ä¼˜åŒ–ã€‚å¹³å‡ {avg_time:.3f}ç§’ > 8ç§’")
    
    # ä¿å­˜ç»“æœ
    results = {
        "config": {
            "num_queries": len(query_tables),
            "dataset_type": dataset_type,
            "init_time": init_time
        },
        "performance": {
            "avg_time": np.mean(valid_times) if valid_times else 0,
            "median_time": np.median(valid_times) if valid_times else 0,
            "std_time": np.std(valid_times) if valid_times else 0,
            "min_time": np.min(valid_times) if valid_times else 0,
            "max_time": np.max(valid_times) if valid_times else 0,
            "success_rate": successful_queries / len(query_tables) if query_tables else 0,
            "total_matches": total_matches,
            "failed_queries": failed_queries[:10]  # åªä¿å­˜å‰10ä¸ªå¤±è´¥çš„
        },
        "query_times": query_times[:500]  # æœ€å¤šä¿å­˜500ä¸ªæ—¶é—´
    }
    
    output_file = f"ultra_evaluation_{dataset_type}_{len(query_tables)}_fixed.json"
    output_path = f"experiment_results/final/{output_file}"
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° {output_path}")
    print("="*80)


if __name__ == "__main__":
    import sys
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    num_queries = int(sys.argv[1]) if len(sys.argv) > 1 else 50
    dataset_type = sys.argv[2] if len(sys.argv) > 2 else "subset"
    
    # è¿è¡Œè¯„ä¼°
    asyncio.run(evaluate_queries_fixed(num_queries, dataset_type))